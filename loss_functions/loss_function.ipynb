{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1024a9e9",
   "metadata": {},
   "source": [
    "## Loss Functions in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f3002d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "softplus(x)&:= \\frac{1}{\\sum_{i=1}^ne^{x_i}} \n",
    "\\begin{pmatrix}\n",
    "    e^{x_1}\\\\ \n",
    "    e^{x_2}\\\\ \n",
    "    \\vdots \\\\ \n",
    "    e^{x_n}\n",
    "\\end{pmatrix},\\quad x \\in \\mathbb{R^n}\\\\\n",
    "sigmoid(x) &:= \\frac{1}{1+e^{-x}}, \\quad x \\in \\mathbb{R} \n",
    "\\end{align*}\n",
    "$$\n",
    "Notice that, if $x\\in \\mathbb{R}^2$\n",
    "$$\n",
    "softplus(x)=\\frac{1}{e^{x_1}+e^{x_2}} \n",
    "\\begin{pmatrix}\n",
    "    e^{x_1}\\\\ \n",
    "    e^{x_2}\n",
    "\\end{pmatrix}=\\begin{pmatrix}\n",
    "    \\frac{1}{1+e^{x_2-x_1}}\\\\ \n",
    "    \\frac{1}{1+e^{x_1-x_2}}\n",
    "\\end{pmatrix}\n",
    "=\\begin{pmatrix}\n",
    "    sigmoid(x_1-x_2)\\\\ \n",
    "   sigmoid(x_2-x_1)\n",
    "\\end{pmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3d3db96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cbd517",
   "metadata": {},
   "source": [
    "## Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7984c47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(20, 10)\n",
    "y = torch.randn(20, 1)\n",
    "model = nn.Linear(10, 1)\n",
    "pred_y = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529004b9",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "Compute mse loss\n",
    "\n",
    "$$mse(\\hat{y},y)=\\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f65dad81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output = tensor(1.2652, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.MSELoss()\n",
    "output = loss(pred_y, y)\n",
    "print(f'{output = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "048c67e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse_loss(pred_y, y) = tensor(1.2652, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def mse_loss(pred_y, y):\n",
    "    return ((pred_y - y) ** 2).mean()\n",
    "\n",
    "print(f'{mse_loss(pred_y, y) = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4e88d0",
   "metadata": {},
   "source": [
    "The general asumption in machine learning is \n",
    "$$\n",
    "(X_1, Y_1), (X_2, Y_2), \\dots, (X_n, X_n) \\;\\;\\overset{\\text{i.i.d.}}{\\sim}\\; P_{X,Y}\n",
    "$$\n",
    "where the data $\\{(x_i,y_i)\\}_{i=1}^n$ is a realization of $X_i,Y_i$ respectively.\n",
    "\n",
    "Notice that from this assumption we optain\n",
    "$$\n",
    "\\begin{align*}\n",
    "p\\left(Y_{1:n}=y_{1:n}|X_{1:n}=x_{1:n}\\right)&= \\frac{p\\left(Y_{1:n}=y_{1:n},X_{1:n}=x_{1:n}\\right)}{p\\left(X_{1:n}=x_{1:n}\\right)}= \\prod_{i=1}^n\\frac{p\\left(Y_{i}=y_{i},X_{i}=x_{i}\\right)}{p\\left(X_{i}=x_{i}\\right)}=\\prod_{i=1}^np(Y_{i}=y_{i}|X_{i}=x_{i})\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac521204",
   "metadata": {},
   "source": [
    "# Binary Classification\n",
    "\n",
    "Conisider $Y$ the label random variable. We assume that $Y:\\Omega \\to  \\{0,1\\}$ follows a Bernoulli distribution with probability of success given $X=x$ equal to $p(x)$:\n",
    "\n",
    "$$p(Y=y|X=x)=\n",
    "\\begin{cases}\n",
    "p(x),\\quad &\\text{ if } y=1,\\\\\n",
    "1-p(x),&\\text{ if } y=0.\n",
    "\\end{cases}\n",
    "=p(x)^y(1-p(x))^{1-y}\n",
    "$$\n",
    "\n",
    "We tray to find the function $p(x)$ that maximize the likelihood of the data.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p\\left(Y_{1:n}=y_{1:n}|X_{1:n}=x_{1:n}\\right)=\\prod_{i=1}^np(Y_{i}=y_{i}|X_{i}=x_{i})=\\prod_{i=1}^np(x_i)^{y_i}(1-p(x_i))^{1-y_i}\n",
    "\\end{align*}\n",
    "$$\n",
    "This is equivlaent to minimizing the neggative log-likelihood\n",
    "$$\n",
    "\\begin{align*}\n",
    "-\\log(p\\left(Y_{1:n}=y_{1:n}|X_{1:n}=x_{1:n}\\right))&=-\\log\\left(\\prod_{i=1}^np(x_i)^{y_i}(1-p(x_i))^{1-y_i}\\right),\\\\\n",
    "&=-\\sum_{i=1}^n\\log\\left(p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\\right),\\\\\n",
    "&=-\\sum_{i=1}^n\\left[y_i\\log(p(x_i))+(1-y_i)\\log(1-p(x_i))\\right],\\\\\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc2f19a",
   "metadata": {},
   "source": [
    "\n",
    "Compute binary cross entropy loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6eb248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.randint(0, 2, (20,1)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c69b0818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output = 0.7293540239334106\n"
     ]
    }
   ],
   "source": [
    "loss_fn = F.binary_cross_entropy_with_logits\n",
    "output = loss_fn(pred_y, y).item()\n",
    "print(f'{output = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4084b4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output = 0.7293540239334106\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "output = loss_fn(pred_y, y).item()\n",
    "print(f'{output = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d055fefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output = 0.7293540239334106\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.BCELoss()\n",
    "output = loss_fn(F.sigmoid(pred_y), y).item()\n",
    "print(f'{output = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11115beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output = 0.7293540239334106\n"
     ]
    }
   ],
   "source": [
    "def binary_cross_entropy_with_logits(pred_y, y):\n",
    "    p = nn.functional.sigmoid(pred_y) #Logits\n",
    "    return -(y * torch.log(p) + (1 - y) * torch.log(1-p)).mean() #BCE\n",
    "\n",
    "loss_fn = binary_cross_entropy_with_logits\n",
    "output = loss_fn(pred_y, y).item()\n",
    "print(f'{output = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5d7111",
   "metadata": {},
   "source": [
    "# Multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41363e5a",
   "metadata": {},
   "source": [
    "Conisider $Y$ the label random variable. We assume that $Y:\\Omega \\to  \\{1,2,\\dots,K\\}$ follows a Categorical distribution where the probability distribution of class $1\\leq j\\leq K$ given $X=x$ is $p_j(x)$ with $\\sum_{j=1}^Kp_j(x)=1$. Then we have\n",
    "\n",
    "$$p(Y=y|X=x)=p_y(x)=\\prod_{j=1}^K p_j(x)^{1_{\\{y=j\\}}}\n",
    "$$\n",
    "\n",
    "We tray to find the function $p(x)=(p_1(x),\\dots,p_K(x))$ that maximize the likelihood of the data.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p\\left(Y_{1:n}=y_{1:n}|X_{1:n}=x_{1:n}\\right)=\\prod_{i=1}^np(Y_{i}=y_{i}|X_{i}=x_{i})=\\prod_{i=1}^n\\prod_{j=1}^K p_j(x_i)^{1_{\\{y_i=j\\}}}=\\prod_{i=1}^np_{y_i}(x_i)\n",
    "\\end{align*}\n",
    "$$\n",
    "This is equivlaent to minimizing the neggative log-likelihood\n",
    "$$\n",
    "\\begin{align*}\n",
    "-\\log(p\\left(Y_{1:n}=y_{1:n}|X_{1:n}=x_{1:n}\\right))&=-\\log\\left(\\prod_{i=1}^n\\prod_{j=1}^K p_j(x_i)^{1_{\\{y_i=j\\}}}\\right)=-\\sum_{i=1}^n\\sum_{j=1}^K1_{\\{y_i=j\\}}\\log\\left(p_j(x_i)\\right)\\\\\n",
    "&=-\\sum_{i=1}^n\\log\\left(p_{y_i}(x_i)\\right)\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc985bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 3\n",
    "model = nn.Linear(10, n_classes)\n",
    "x = torch.randn(20, 10)\n",
    "y = torch.randint(0, n_classes, (20,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a8af6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output = 1.1571094989776611\n"
     ]
    }
   ],
   "source": [
    "pred_y = model(x)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "output = loss_fn(pred_y, y).item()\n",
    "print(f'{output = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "134e87f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output = 1.1571094989776611\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.functional.cross_entropy\n",
    "output = loss_fn(pred_y, y).item()\n",
    "print(f'{output = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05241ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output = 1.1571094989776611\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy(pred_y, y):\n",
    "    probs = nn.functional.softmax(pred_y, dim=1)\n",
    "    p_yi=probs[torch.arange(y.shape[0]), y]\n",
    "    log_p_yi = torch.log(p_yi)\n",
    "    return -log_p_yi.mean()\n",
    "\n",
    "loss_fn = cross_entropy\n",
    "output = loss_fn(pred_y, y).item()\n",
    "print(f'{output = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3507509e",
   "metadata": {},
   "source": [
    "### Multiple binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d34aecb",
   "metadata": {},
   "source": [
    "Conisider $Y=(Y_1,Y_2,\\dots,Y_K)$ the label random variable. We assume that $Y_j:\\Omega \\to  \\{0,1\\}$ follows a Bernoulli distribution with probability of success given $X=x$ equal to $p_j(x)$: We aalso assume (this is questionable) that $Y_j$ $1\\leq j \\leq K$ are independent\n",
    "\n",
    "$$p(Y_j=y_j|X=x)=\n",
    "\\begin{cases}\n",
    "p_j(x),\\quad &\\text{ if } y_j=1,\\\\\n",
    "1-p_j(x),&\\text{ if } y_j=0.\n",
    "\\end{cases}\n",
    "=p_j(x)^{y_j}(1-p_j(x))^{1-y_j}\n",
    "$$\n",
    "\n",
    "We tray to find the function $p=(p_1,p_2,\\dots,p_K)$ that maximize the likelihood of the data.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p\\left(Y_{1:n}=y_{1:n}|X_{1:n}=x_{1:n}\\right)&=\\prod_{i=1}^np(Y_{i}=y_{i}|X_{i}=x_{i})\n",
    "=\\prod_{i=1}^np\\left(\\prod_{j=1}^K\\{Y_{i,j}=y_{i,j}\\}\\left|X_{i}=x_{i}\\right.\\right),\\\\\n",
    "&=\\prod_{i=1}^n\\prod_{j=1}^Kp\\left(Y_{i,j}=y_{i,j}\\left|X_{i}=x_{i}\\right.\\right),\\\\\n",
    "&=\\prod_{i=1}^n\\prod_{j=1}^Kp_j(x_{i})^{y_{i,j}}(1-p_j(x_{i}))^{1-y_{i,j}}\n",
    "\\end{align*}\n",
    "$$\n",
    "This is equivlaent to minimizing the neggative log-likelihood\n",
    "$$\n",
    "\\begin{align*}\n",
    "-\\log\\left(p\\left(Y_{1:n}=y_{1:n}|X_{1:n}=x_{1:n}\\right)\\right)&=-\\log\\left(\\prod_{i=1}^n\\prod_{j=1}^Kp_j(x_{i})^{y_{i,j}}(1-p_j(x_{i}))^{1-y_{i,j}}\\right),\\\\\n",
    "&=-\\sum_{i=1}^n\\sum_{j=1}^K\\log\\left(p_j(x_{i})^{y_{i,j}}(1-p_j(x_{i}))^{1-y_{i,j}}\\right),\\\\\n",
    "&=-\\sum_{i=1}^n\\sum_{j=1}^K\\left[y_{i,j}\\log(p_j(x_{i}))+(1-y_{i,j})\\log(1-p_j(x_{i}))\\right],\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cd5933",
   "metadata": {},
   "source": [
    "Note: The loss is also Binary Cross Entropy, but here the mean is in two dimensions (sanmples and output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efe2656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 3\n",
    "model = nn.Linear(10, n_classes)\n",
    "x = torch.randn(20, 10)\n",
    "y = torch.randint(0, 2, (20, n_classes)).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7872fe90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output = 0.7459571957588196\n"
     ]
    }
   ],
   "source": [
    "pred_y = model(x)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "output = loss_fn(pred_y, y).item()\n",
    "print(f'{output = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44bc721e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output = 0.7459571957588196\n"
     ]
    }
   ],
   "source": [
    "def binary_cross_entropy_with_logits(pred_y, y):\n",
    "    p = nn.functional.sigmoid(pred_y) #logits\n",
    "    return -(y * torch.log(p) + (1 - y) * torch.log(1-p)).mean() #BCE (here mean is in two dimensions)\n",
    "loss_fn = binary_cross_entropy_with_logits\n",
    "output = loss_fn(pred_y, y).item()\n",
    "print(f'{output = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1d3891",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-notebook",
   "language": "python",
   "name": "ml-notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
