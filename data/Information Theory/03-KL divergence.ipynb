{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b011cd39",
   "metadata": {},
   "source": [
    "# [Kullback–Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4cfd4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c8fd03",
   "metadata": {},
   "source": [
    "## Definition: Kullback–Leibler (KL) Divergence\n",
    "\n",
    "Let $\\mathcal{M}_1(\\mathcal{X})$ denote the set of all pmf of probability measures on the\n",
    "measurable space $(\\mathcal{X}, \\mathcal{P}(\\mathcal{X}))$ with $\\mathcal{X}$ finite.\n",
    "\n",
    "Define\n",
    "$$\n",
    "\\widehat{\\mathcal{M}_1(\\mathcal{X})^2}\n",
    ":= \\big\\{\\, (p,q)\\in \\mathcal{M}_1(\\mathcal{X})^2\n",
    "\\; : \\; p(x) > 0 \\Rightarrow q(x) > 0,\\ \\forall x \\in \\mathcal{X} \\,\\big\\}.\n",
    "$$\n",
    "\n",
    "We define the **Kullback–Leibler (KL) divergence** as the function\n",
    "$$\n",
    "\\begin{aligned}\n",
    "D_{KL} : \\widehat{\\mathcal{M}_1(\\mathcal{X})^2} &\\longrightarrow [0,\\infty) \\\\\n",
    "D_{KL}(p \\Vert q) \n",
    "&:= \\sum_{x \\in \\mathcal{X}} \n",
    "p(x)\\,\\log\\!\\left(\\frac{p(x)}{q(x)}\\right),\n",
    "\\end{aligned}\n",
    "$$\n",
    "with the convention $0 \\log 0 := 0$.\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "The KL divergence $D_{KL}(p \\Vert q)$ measures how *inefficient* or *informationally costly* it would be to use the model $q$ to represent or encode data that is actually generated by $p$.  \n",
    "It is zero if and only if $p=q$, and increases as $q$ becomes a worse approximation of $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f8c50c",
   "metadata": {},
   "source": [
    "## Properties: Kullback–Leibler (KL) Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacd2a82",
   "metadata": {},
   "source": [
    "### Metric properties\n",
    "\n",
    "1. **Non-negativity (Gibbs' inequality)**\n",
    "\n",
    "$$\n",
    "D_{KL}(p \\,\\|\\, q) \\ge 0\n",
    "$$\n",
    "\n",
    "with equality if and only if $p = q$.\n",
    "\n",
    "2. **Not symmetric and no triangle inequality**\n",
    "\n",
    "In general,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "D_{KL}(p \\,\\|\\, q) &\\neq D_{KL}(q \\,\\|\\, p),\\\\\n",
    "D_{KL}(p \\,\\|\\, r) &\\not\\le D_{KL}(p \\,\\|\\, q)+ D_{KL}(q \\,\\|\\, r)\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Therefore, KL divergence is **not a metric** (it does not define a distance in the mathematical sense).\n",
    "\n",
    "3. **Not bounded**\n",
    "\n",
    "KL divergence can take arbitrarily large values and may be $+\\infty$ if\n",
    "there exists $x$ such that\n",
    "\n",
    "$$\n",
    "p(x) > 0 \\quad \\text{and} \\quad q(x) = 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6aa2c3",
   "metadata": {},
   "source": [
    "**Proof (1.):**\n",
    "We use the elementary inequality\n",
    "$$\n",
    "\\log t \\le t - 1 \\quad \\text{for all } t > 0,\n",
    "$$\n",
    "with equality if and only if $t = 1$.\n",
    "\n",
    "This is equivalent to\n",
    "$$\n",
    "-\\log t \\ge 1 - t.\n",
    "$$\n",
    "Then we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "D_{KL}(p \\,\\|\\, q)\n",
    "&= \\sum_{x \\in \\mathcal{X}} p(x)\\,\\log\\!\\left( \\frac{p(x)}{q(x)} \\right),\\\\\n",
    "&= \\sum_{x \\in \\mathcal{X}} p(x)\\left[-\\log\\!\\left( \\frac{p(x)}{p(x)} \\right)\\right],\\\\\n",
    "&\\ge \\sum_{x \\in \\mathcal{X}} p(x)\\left(1- \\frac{q(x)}{p(x)} \\right),\\\\\n",
    "&= \\sum_{x \\in \\mathcal{X}} \\left(p(x)-q(x) \\right),\\\\\n",
    "&= \\sum_{x \\in \\mathcal{X}} p(x)-\\sum_{x \\in \\mathcal{X}}q(x),\\\\\n",
    "&= 1 -1,\\\\\n",
    "&= 0.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6c2e35",
   "metadata": {},
   "source": [
    "For the equality condition we have\n",
    "\n",
    "Equality holds if and only if\n",
    "$$\n",
    "-\\log\\frac{q(x)}{p(x)} = 1 - \\frac{q(x)}{p(x)}\n",
    "$$\n",
    "for all $x$ with $p(x) > 0$,\n",
    "which happens if and only if\n",
    "$$\n",
    "\\frac{q(x)}{p(x)} = 1\n",
    "\\quad \\Longleftrightarrow \\quad\n",
    "p(x) = q(x)\n",
    "$$\n",
    "for all $x$ with $p(x)>0$.\n",
    "\n",
    "Since both are probability distributions, this implies\n",
    "$$\n",
    "p(x) = q(x) \\quad \\text{for all } x \\in \\mathcal{X}.\n",
    "$$\n",
    "\n",
    "Conversely, if $p=q$, then\n",
    "$$\n",
    "D_{KL}(p\\|q) = \\sum_x P(x)\\log 1 = 0.\n",
    "$$\n",
    "\n",
    "Thus\n",
    "$$\n",
    "D_{KL}(p\\|q) \\ge 0,\n",
    "$$\n",
    "with equality if and only if $p = q$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ff006f",
   "metadata": {},
   "source": [
    "### Property: Relation with entropy and cross-entropy\n",
    "\n",
    "A key identity connects cross-entropy, entropy, and KL-divergence:\n",
    "\n",
    "$$\n",
    "H(p \\Vert q) \n",
    "=  D_{KL}(p \\,\\|\\, q)+ H(p).\n",
    "$$\n",
    "\n",
    "This decomposition shows that cross-entropy contains **two distinct effects**:\n",
    "\n",
    "1. An intrinsic part: the entropy of the true distribution $H(p)$\n",
    "2. A mismatch part: the divergence between the model and the truth $D_{KL}(p\\Vert q)$\n",
    "\n",
    "Interpretation\n",
    "\n",
    "* High cross-entropy $H(p \\Vert q)$ means that outcomes generated by the true distribution $p$ are (on average) highly unpredictable — either because  \n",
    "  - $p$ itself is highly random (high entropy) $H(p)$, or  \n",
    "  - the model $q$ is very different from the true distribution $p$, or  \n",
    "  - both.\n",
    "\n",
    "* Low cross-entropy $H(p \\Vert q)$ means that outcomes generated by $p$ are (on average) quite predictable using the model $q$, which happens when  \n",
    "  - $p$ is structured (low entropy) and  \n",
    "  - $q$ is close to $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f2c614",
   "metadata": {},
   "source": [
    "#### **Proof:**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H(p \\Vert q) \n",
    "&:= -\\sum_{x \\in \\mathcal{X}} p(x)\\log q(x),\\\\\n",
    "&= \\sum_{x \\in \\mathcal{X}} p(x)\\log \\left(\\frac{1}{q(x)}\\right)-H(p)+H(p),\\\\\n",
    "&= \\sum_{x \\in \\mathcal{X}} p(x)\\log \\left(\\frac{1}{q(x)}\\right)+\\sum_{x \\in \\mathcal{X}} p(x)\\log \\left(p(x)\\right)+H(p),\\\\\n",
    "&= \\sum_{x \\in \\mathcal{X}} p(x)\\log \\left(\\frac{p(x)}{q(x)}\\right)+H(p),\\\\\n",
    "&=  D_{KL}(p \\,\\|\\, q)+ H(p).\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764c45db",
   "metadata": {},
   "source": [
    "### Property: Relation with mutual information\n",
    "\n",
    "Then mutual information is a special case of KL divergence:\n",
    "\n",
    "$$\n",
    " H(X \\Cap Y)\n",
    "= D_{KL}\\!\\left( p_{(X,Y)} \\, \\| \\, p_X\\,p_Y \\right).\n",
    "$$\n",
    "\n",
    "This shows:\n",
    "\n",
    "- Mutual information measures how far the joint distribution is\n",
    "  from the distribution under independence\n",
    "- Hence it quantifies **statistical dependence**\n",
    "- And explains why\n",
    "$$\n",
    "\n",
    " H(X \\Cap Y) \\ge 0\n",
    "$$\n",
    "with equality if and only if $X$ and $Y$ are independent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4c3ce4",
   "metadata": {},
   "source": [
    "#### **Proof:** \n",
    "From the KL-divergence form property of the mutual informaiton we have \n",
    "$$ H(X \\Cap Y) = \\sum_{x,y} p(x,y)\\log \\frac{p(x,y)}{p(x)p(y)} = D_{KL}\\!\\left( p_{(X,Y)} \\, \\| \\, p_X\\,p_Y \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c40416",
   "metadata": {},
   "source": [
    "## Code: Kullback–Leibler (KL) Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e5b470",
   "metadata": {},
   "source": [
    "Consider the random variable $X,Y$ with probability mass function\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p_X:\\mathcal{X}&\\to[0,1]\\\\\n",
    "p_X(0)&=0.2\\\\\n",
    "p_X(1)&=0.3\\\\\n",
    "p_X(2)&=0.5,\n",
    "\\end{aligned}\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\begin{aligned}\n",
    "p_Y:\\mathcal{Y}&\\to[0,1]\\\\\n",
    "p_Y(0)&=0.1\\\\\n",
    "p_Y(1)&=0.4,\\\\\n",
    "p_Y(2)&=0.5,\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c070cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(P, Q, base=2):\n",
    "\n",
    "    assert P.shape == Q.shape, \"P and Q must have the same shape\"\n",
    "    assert np.isclose(np.sum(P), 1), \"P must sum to 1\"\n",
    "    assert np.isclose(np.sum(Q), 1), \"Q must sum to 1\"\n",
    "    assert np.all(P[Q == 0] == 0), \"Violation: Q==0 implies P must also be 0\"\n",
    "\n",
    "    \n",
    "    # Mask of valid entries\n",
    "    mask = P > 0\n",
    "\n",
    "    # I(X;Y) = sum p(x,y) log( p(x,y) / (p(x)p(y)) )\n",
    "    return (np.sum(P[mask] * np.log((P/Q)[mask]))/np.log(base)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a69ea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(P, Q,base=2):\n",
    "    P_pos = P[P > 0]\n",
    "    Q_pos = Q[P > 0]\n",
    "    return -((P_pos * np.log(Q_pos)).sum()/np.log(base)).item()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9640830a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_X=array([0.2, 0.5, 0.3])\n",
      "P_X_hat=array([0.3, 0.4, 0.3])\n",
      "P_Y=array([0.8, 0.1, 0.1])\n"
     ]
    }
   ],
   "source": [
    "P_X=np.array([0.2, 0.5, 0.3])\n",
    "P_X_hat=np.array([0.3, 0.4, 0.3])\n",
    "P_Y=np.array([0.8, 0.1, 0.1])\n",
    "print(f\"{P_X=}\")\n",
    "print(f\"{P_X_hat=}\")\n",
    "print(f\"{P_Y=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d363419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_KL(P_X,P_Y) = 0.857043770593505\n"
     ]
    }
   ],
   "source": [
    "print(f\"D_KL(P_X,P_Y) = {entropy(P_X,P_Y).item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29a2576d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_KL(P_X,P_Y) = 0.857043770593505\n"
     ]
    }
   ],
   "source": [
    "print(f\"D_KL(P_X,P_Y) = {kl_divergence(P_X,P_Y,base=np.e)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cda622d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_KL(P_X,P_X_hat) = 0.030478754035472025\n"
     ]
    }
   ],
   "source": [
    "print(f\"D_KL(P_X,P_X_hat) = {kl_divergence(P_X,P_X_hat,base=np.e)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd58d15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_KL(P_X,P_X) = 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"D_KL(P_X,P_X) = {kl_divergence(P_X,P_X,base=np.e)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c70a6ae",
   "metadata": {},
   "source": [
    "Property\n",
    "$$\n",
    "H(p \\Vert q) \n",
    "=  D_{KL}(p \\,\\|\\, q)+ H(p).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b083bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(P_X || P_Y) = 1.8866967846580782\n",
      "D_KL(P_X,P_Y) + H(P_X) = 1.8866967846580787\n"
     ]
    }
   ],
   "source": [
    "print(f\"H(P_X || P_Y) = {cross_entropy(P_X, P_Y, base=np.e)}\")\n",
    "print(f\"D_KL(P_X,P_Y) + H(P_X) = {kl_divergence(P_X,P_Y,base=np.e) + entropy(P_X)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb077981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-notebook",
   "language": "python",
   "name": "ml-notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
