{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61f435b4",
   "metadata": {},
   "source": [
    "# Normalization in PyTorch\n",
    "\n",
    "[link](https://ut.philkr.net/deeplearning/residuals_and_normalizations/normalizations_in_pytorch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "25c6130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b84dab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNoBIas(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLPNoBIas, self).__init__()\n",
    "        layers = [nn.Flatten()]\n",
    "        in_dim = input_dim\n",
    "        for h_dim in hidden_dim:\n",
    "            layers.append(nn.Linear(in_dim, h_dim, bias=False))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_dim = h_dim\n",
    "        layers.append(nn.Linear(in_dim, output_dim, bias=False))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b6234d",
   "metadata": {},
   "source": [
    "## Vanashing activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "11b95735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_norm = 12.411030769348145\n",
      "out_norm = 5.221368789672852\n",
      "out_norm = 2.2383782863616943\n",
      "out_norm = 0.8727157711982727\n",
      "out_norm = 0.3667830526828766\n",
      "out_norm = 0.14457768201828003\n",
      "out_norm = 0.0638008713722229\n",
      "out_norm = 0.02657708339393139\n",
      "out_norm = 0.010049083270132542\n",
      "out_norm = 0.00452427938580513\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 28, 28)\n",
    "for i in range(10):\n",
    "    net = MLPNoBIas(input_dim=28*28, hidden_dim=i*[512], output_dim=100)\n",
    "    out_norm = net(x).norm().item()\n",
    "    print(f'{out_norm = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eda7ab3",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "174e2109",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBNPre(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        layers = [nn.Flatten()]\n",
    "        in_dim = input_dim\n",
    "        for h_dim in hidden_dim:\n",
    "            layers.append(nn.BatchNorm1d(in_dim,affine=False)) # bias learned by LinearLayer\n",
    "            layers.append(nn.Linear(in_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_dim = h_dim\n",
    "        layers.append(nn.Linear(in_dim, output_dim, bias=False))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0055bd9e",
   "metadata": {},
   "source": [
    "**Note:** We should always learn the bias before the ReLU. Otherwise, the ReLU will set half of the activations to zero, which limits the expressiveness of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e9e3c62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_norm = 12.024511337280273\n",
      "out_norm = 5.024309158325195\n",
      "out_norm = 5.271420955657959\n",
      "out_norm = 5.451053142547607\n",
      "out_norm = 5.185364246368408\n",
      "out_norm = 5.233152866363525\n",
      "out_norm = 5.526966094970703\n",
      "out_norm = 5.2882866859436035\n",
      "out_norm = 5.509188175201416\n",
      "out_norm = 5.597162246704102\n",
      "out_norm = 5.479552268981934\n",
      "out_norm = 5.394810676574707\n",
      "out_norm = 5.179454326629639\n",
      "out_norm = 5.039365768432617\n",
      "out_norm = 5.30059814453125\n",
      "out_norm = 5.2813286781311035\n",
      "out_norm = 5.495495796203613\n",
      "out_norm = 5.530455112457275\n",
      "out_norm = 5.437389850616455\n",
      "out_norm = 5.115573883056641\n",
      "out_norm = 5.191061973571777\n",
      "out_norm = 4.9538421630859375\n",
      "out_norm = 5.318709850311279\n",
      "out_norm = 5.090663909912109\n",
      "out_norm = 5.411695957183838\n",
      "out_norm = 5.124027252197266\n",
      "out_norm = 5.2969279289245605\n",
      "out_norm = 5.216276168823242\n",
      "out_norm = 5.327692985534668\n",
      "out_norm = 5.260517597198486\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 28, 28)\n",
    "for i in range(30):\n",
    "    net = MLPBNPre(input_dim=28*28, hidden_dim=i*[512], output_dim=100)\n",
    "    out_norm = net(x).norm().item()\n",
    "    print(f'{out_norm = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d4acb9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBNPos(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Flatten())\n",
    "        in_dim = input_dim\n",
    "        for h_dim in hidden_dim:\n",
    "            layers.append(nn.Linear(in_dim, h_dim, bias=False))\n",
    "            layers.append(nn.BatchNorm1d(h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_dim = h_dim\n",
    "        layers.append(nn.Linear(in_dim, output_dim, bias=False))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d4b8b77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_norm = 13.410526275634766\n",
      "out_norm = 9.821581840515137\n",
      "out_norm = 9.03606128692627\n",
      "out_norm = 9.867572784423828\n",
      "out_norm = 8.556086540222168\n",
      "out_norm = 9.152222633361816\n",
      "out_norm = 9.228806495666504\n",
      "out_norm = 9.490620613098145\n",
      "out_norm = 9.340707778930664\n",
      "out_norm = 8.71877670288086\n",
      "out_norm = 9.14144515991211\n",
      "out_norm = 8.685832023620605\n",
      "out_norm = 9.378507614135742\n",
      "out_norm = 9.54654312133789\n",
      "out_norm = 8.792848587036133\n",
      "out_norm = 9.000468254089355\n",
      "out_norm = 9.063599586486816\n",
      "out_norm = 9.271416664123535\n",
      "out_norm = 8.872846603393555\n",
      "out_norm = 8.996878623962402\n",
      "out_norm = 9.420639991760254\n",
      "out_norm = 9.194003105163574\n",
      "out_norm = 9.413265228271484\n",
      "out_norm = 9.407005310058594\n",
      "out_norm = 9.761667251586914\n",
      "out_norm = 9.082772254943848\n",
      "out_norm = 9.130640983581543\n",
      "out_norm = 8.916101455688477\n",
      "out_norm = 8.82054615020752\n",
      "out_norm = 9.423371315002441\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 28, 28)\n",
    "for i in range(30):\n",
    "    net = MLPBN(input_dim=28*28, hidden_dim=i*[512], output_dim=100)\n",
    "    out_norm = net(x).norm().item()\n",
    "    print(f'{out_norm = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d19ae99",
   "metadata": {},
   "source": [
    "## Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3532f02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBNPre(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        layers = [nn.Flatten()]\n",
    "        in_dim = input_dim\n",
    "        for h_dim in hidden_dim:\n",
    "            # BatchNorm matches current input dimension\n",
    "            layers.append(nn.LayerNorm(in_dim, bias=False)) # bias learned by LinearLayer\n",
    "            layers.append(nn.Linear(in_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_dim = h_dim\n",
    "        layers.append(nn.Linear(in_dim, output_dim, bias=False))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1bd3bfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_norm = 12.980302810668945\n",
      "out_norm = 4.991347312927246\n",
      "out_norm = 4.997653961181641\n",
      "out_norm = 5.150710582733154\n",
      "out_norm = 5.36508321762085\n",
      "out_norm = 5.078702449798584\n",
      "out_norm = 5.298841953277588\n",
      "out_norm = 5.6470866203308105\n",
      "out_norm = 5.031192302703857\n",
      "out_norm = 5.031366348266602\n",
      "out_norm = 5.002532958984375\n",
      "out_norm = 4.955268859863281\n",
      "out_norm = 5.230772495269775\n",
      "out_norm = 5.224359512329102\n",
      "out_norm = 5.633170127868652\n",
      "out_norm = 5.3707275390625\n",
      "out_norm = 4.847537994384766\n",
      "out_norm = 4.637307643890381\n",
      "out_norm = 5.503502368927002\n",
      "out_norm = 5.319293022155762\n",
      "out_norm = 5.238644599914551\n",
      "out_norm = 5.7760090827941895\n",
      "out_norm = 5.5079474449157715\n",
      "out_norm = 5.622540473937988\n",
      "out_norm = 5.416176795959473\n",
      "out_norm = 5.0370965003967285\n",
      "out_norm = 5.123276233673096\n",
      "out_norm = 5.376845359802246\n",
      "out_norm = 5.7942423820495605\n",
      "out_norm = 5.010921001434326\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 28, 28)\n",
    "for i in range(30):\n",
    "    net = MLPBNPre(input_dim=28*28, hidden_dim=i*[512], output_dim=100)\n",
    "    out_norm = net(x).norm().item()\n",
    "    print(f'{out_norm = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d614241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPLNPos(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Flatten())\n",
    "        in_dim = input_dim\n",
    "        for h_dim in hidden_dim:\n",
    "            layers.append(nn.Linear(in_dim, h_dim, bias=False)) # Bias learned by LayerNorm\n",
    "            layers.append(nn.LayerNorm(h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_dim = h_dim\n",
    "        layers.append(nn.Linear(in_dim, output_dim, bias=False))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6ba87855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_norm = 12.144548416137695\n",
      "out_norm = 9.377350807189941\n",
      "out_norm = 8.982717514038086\n",
      "out_norm = 9.0455961227417\n",
      "out_norm = 9.14870548248291\n",
      "out_norm = 9.405213356018066\n",
      "out_norm = 10.759411811828613\n",
      "out_norm = 9.215036392211914\n",
      "out_norm = 9.327682495117188\n",
      "out_norm = 9.275083541870117\n",
      "out_norm = 9.386228561401367\n",
      "out_norm = 8.449091911315918\n",
      "out_norm = 9.29554271697998\n",
      "out_norm = 8.787675857543945\n",
      "out_norm = 8.455885887145996\n",
      "out_norm = 8.547210693359375\n",
      "out_norm = 8.376311302185059\n",
      "out_norm = 9.38184642791748\n",
      "out_norm = 8.61821460723877\n",
      "out_norm = 9.655498504638672\n",
      "out_norm = 9.376791000366211\n",
      "out_norm = 7.882428169250488\n",
      "out_norm = 9.666019439697266\n",
      "out_norm = 8.73833179473877\n",
      "out_norm = 9.05306339263916\n",
      "out_norm = 8.358280181884766\n",
      "out_norm = 8.523652076721191\n",
      "out_norm = 10.625301361083984\n",
      "out_norm = 9.722755432128906\n",
      "out_norm = 9.749566078186035\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 28, 28)\n",
    "for i in range(30):\n",
    "    net = MLPLNPos(input_dim=28*28, hidden_dim=i*[512], output_dim=100)\n",
    "    out_norm = net(x).norm().item()\n",
    "    print(f'{out_norm = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f3a732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
