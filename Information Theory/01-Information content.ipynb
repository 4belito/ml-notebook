{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "326628c3",
   "metadata": {},
   "source": [
    "# [Information content](https://en.wikipedia.org/wiki/Information_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d38523a",
   "metadata": {},
   "source": [
    "## Alphabet\n",
    "\n",
    "Let $(\\Omega, \\mathcal{F}, \\mathbb{P})$ be a probability space and\n",
    "$X : \\Omega \\to \\mathbb{R}$ be a discrete random variable.\n",
    "Define its alphabet by\n",
    "$$\n",
    "\\text{supp}(\\mathbb{P}_{X}) = \\{x \\in \\mathbb{R} : \\mathbb{P}(X = x) > 0\\},\n",
    "$$\n",
    "where $\\mathbb{P}_{X}$ is the probability measure induced by $X$, i.e.\n",
    "$$\\mathbb{P}_{X}(B)= \\mathbb{P}\\left(X^{-1}(B)\\right),\\quad \\text{ for any Borel set } B \\subset \\mathbb{R}.$$\n",
    "**Note:** In measure theory, this set is more precisely called the support of the distribution (or law) of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1bf838",
   "metadata": {},
   "source": [
    "## Probability Mass Function (pmf)\n",
    "Let\n",
    "* $(\\Omega, \\mathcal{F}, \\mathbb{P})$ be a probability space\n",
    "* $X : \\Omega \\to \\mathbb{R} $ be a discrete random variable\n",
    "<!-- * $\\mathcal{X} = \\{x_1, x_2, \\dots, x_n\\}$ its alphabet (i.e the suport of the random variable) -->\n",
    "Define its probability mass function\n",
    "\n",
    "$$\n",
    "p(B)= \\mathbb{P}_{X}(B):=  \\mathbb{P}(X^{-1}(B)), \\quad B\\subset \\mathbb{R}, \\text{ borel set.}\n",
    "$$\n",
    "\n",
    "The fundamental question of Information Theory is:\n",
    "\n",
    "How much uncertainty is in $X$ before observing it, and how much information is gained after observing it?\n",
    "\n",
    "This is not variance. It is a different notion of uncertainty based on logarithms and probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7e407d",
   "metadata": {},
   "source": [
    "## Information content on events\n",
    "\n",
    "Let $(\\Omega, \\mathcal{F}, \\mathbb{P})$ be a probability space. We define the information content function on events as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "I_{\\mathbb{P}}: \\mathcal{F} &\\to [0,\\infty] \\\\\n",
    "I_{\\mathbb{P}}(A) &:= -\\log \\mathbb{P}(A)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Note:** \n",
    "* When no ambiguity arises, the dependence of the information content on the probability measure is omitted, and we write $I(A)$ in place of $I_\\mathbb{P}(A)$.\n",
    "* Since $\\mathbb{P}$ is a probability measure, $\\mathbb{P}(A)\\in[0,1]$ and consequently $-\\log \\mathbb{P}(A)\\in [0,\\infty]$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94103b8",
   "metadata": {},
   "source": [
    "### Key properties\n",
    "1. If $\\mathbb{P}(A)$ is small, then $I(A)$ is large (rare events carry more information).\n",
    "\n",
    "2. $\\mathbb{P}(A) = 1 \\;\\Longleftrightarrow\\; I(A) = 0$  \n",
    "(no information is obtained if and only if the event is sure).\n",
    "\n",
    "3. $\\mathbb{P}(A) = 0 \\;\\Longleftrightarrow\\; I(A) = \\infty$  \n",
    "(infinite information is obtained if and only if the event is impossible).\n",
    "\n",
    "4. If $A$ and $B$ are independent events,\n",
    "$$\n",
    "I(A \\cap B) = I(A) + I(B)\n",
    "$$\n",
    "\n",
    "The logarithm base determines the units:\n",
    "\n",
    "- base $2$ → **bits**  \n",
    "- base $e$ → **nats**  \n",
    "- base $10$ → **hartleys**\n",
    "\n",
    "We will continue with base $2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0b6080",
   "metadata": {},
   "source": [
    "**Proof 1:** It is strightforward form the monotonicity of $\\log$ and $x\\mapsto -x$ functions.\n",
    "\n",
    "**Proof 2,3:** they are stright forward form the $\\log$ evaluaiton and bijectivity.\n",
    "\n",
    "**Proof 3:** If $A$ and $B$ are independent, then \n",
    "$$\n",
    "\\begin{align*}\n",
    "I(A,B) &= -\\log\\left(\\mathbb{P}(A,B)\\right)=-\\log\\left(\\mathbb{P}(A)\\mathbb{P}(B)\\right),\\\\\n",
    "&=-(\\log\\left(\\mathbb{P}(A)\\right)+\\log\\left(\\mathbb{P}(B)\\right)),\\\\\n",
    "&=I(A)+I(B).\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dcfd57",
   "metadata": {},
   "source": [
    "## Definition: Information Content of Joint Random Variables\n",
    "\n",
    "Information answer the question: How unexpected was this outcome if the model is correct?\n",
    "\n",
    "Let $(\\Omega, \\mathcal{F}, \\mathbb{P})$ be a probability space and $X=(X_1,X_2,\\dots,X_n)$ a discrete random vector defined on it. Let $\\mathbb{P}_{X}$ denote the probability measure (law) induced by $X$, i.e.\n",
    "\n",
    "$$\n",
    "\\mathbb{P}_{X}(B)\n",
    ":= \\mathbb{P}\\!\\left( X^{-1}(B) \\right),\n",
    "\\qquad B \\in \\mathcal{B}(\\mathbb{R}^n).\n",
    "$$\n",
    "\n",
    "We define the **information content function associated with $X$** by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "I_{\\mathbb{P}_{X}}: \\mathbb{R}^n &\\to [0,\\infty] \\\\\n",
    "I_{\\mathbb{P}_{X}}(x)\n",
    "&:= -\\log \\mathbb{P}_{X}(\\{x\\}) \\\\\n",
    "&= -\\log \\mathbb{P}(X_1 = x_1,\\dots,X_n = x_n).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The associated **information-content random variable** is then the composition\n",
    "\n",
    "$$\n",
    "I_{\\mathbb{P}_{X}}(X) : \\Omega \\to [0,\\infty],\n",
    "\\qquad\n",
    "\\omega \\mapsto I_{\\mathbb{P}_{X}}\\big( X(\\omega) \\big)\n",
    "= -\\log \\mathbb{P}_{X}\\big( X(\\omega) \\big).\n",
    "$$\n",
    "\n",
    "**Note:** \n",
    "* We omit explicit reference to $\\mathbb{P}_X$ whenever the context is clear. When no ambiguity arises, we simply write $I$ in place of $I_{\\mathbb{P}_X}$. In the discrete case, we also write $p(x)$ instead of \n",
    "$\\mathbb{P}_{X}(x)= \\mathbb{P}(X = x)$.\n",
    "* $I_{\\mathbb{P}_{X}}$ is a particular case of the information content when defined on the probability space \n",
    "  $(\\mathbb{R}^n,\\, \\mathcal{B}(\\mathbb{R}^n),\\, \\mathbb{P}_{X})$ \n",
    "  (or equivalently on $(\\mathcal{X}, \\mathcal{P}(\\mathcal{X}), \\mathbb{P}_{X})$ in the purely discrete case, where $\\mathcal{X}$ is the alphabet of $X$). \n",
    "  Here we identify the singleton $\\{x\\}$ with its element $x$ via\n",
    "  $$\n",
    "  I_{\\mathbb{P}_X}(x) := I_{\\mathbb{P}_X}(\\{x\\}).\n",
    "  $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
