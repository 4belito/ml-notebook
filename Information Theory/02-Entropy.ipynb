{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "326628c3",
   "metadata": {},
   "source": [
    "# [Entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "993f710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyitlib import discrete_random_variable as drv\n",
    "from scipy.stats import entropy as sci_entropy\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e504f306",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e2542a",
   "metadata": {},
   "source": [
    "#### Artifitial Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "679dd1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def artifitial_sample(values, prob,n_samples=100):\n",
    "    sample=[]\n",
    "    for val,p in zip(values, prob):\n",
    "        sample.extend(int(n_samples * p)* [val] )  \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fbe2bd",
   "metadata": {},
   "source": [
    "### Arbitrary example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43be5f60",
   "metadata": {},
   "source": [
    "\n",
    "#### Define discrete probability vector\n",
    "\n",
    "Consider the alphabets\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{X}&=\\{0,1,2\\},\\\\\n",
    "\\mathcal{Y}&=\\{0,1\\}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Define the joint probability mass function $p:\\mathcal{X}\\times\\mathcal{Y}\\to[0,1]$ by\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(0,0)&=0.05, & p(0,1)&=0.15,\\\\\n",
    "p(1,0)&=0.20, & p(1,1)&=0.10,\\\\\n",
    "p(2,0)&=0.30, & p(2,1)&=0.20.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c8956c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint probability matrix p(x,y)\n",
    "#.   Y=0    Y=1\n",
    "P_XY = np.array([\n",
    "    [0.05, 0.15],   # X = 0\n",
    "    [0.20, 0.10],   # X = 1\n",
    "    [0.30, 0.20]    # X = 2\n",
    "])\n",
    "\n",
    "P_XY_flat = P_XY.flatten()\n",
    "P_X = P_XY.sum(axis=1) \n",
    "P_Y = P_XY.sum(axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7385d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05, 0.15, 0.2 , 0.1 , 0.3 , 0.2 ])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_XY_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a172fca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2, 0.3, 0.5])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8a8eb7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.55, 0.45])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ea371eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [(i, j) for i in range(P_XY.shape[0])\n",
    "                          for j in range(P_XY.shape[1])]\n",
    "sample_XY = artifitial_sample(values, P_XY_flat)\n",
    "sample_X = [x for x,_ in sample_XY]\n",
    "sample_Y = [y for _, y in sample_XY]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b5de50",
   "metadata": {},
   "source": [
    "### Uniform example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e64c22e",
   "metadata": {},
   "source": [
    "\n",
    "#### Define discrete probability vector\n",
    "\n",
    "Consider the alphabets\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{X}&=\\{0,1,2\\},\\\\\n",
    "\\mathcal{Y}&=\\{0,1\\}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Define the joint probability mass function $p:\\mathcal{X}\\times\\mathcal{Y}\\to[0,1]$ by\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(0,0)&=0.05, & p(0,1)&=0.15,\\\\\n",
    "p(1,0)&=0.20, & p(1,1)&=0.10,\\\\\n",
    "p(2,0)&=0.30, & p(2,1)&=0.20.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be6cb7a",
   "metadata": {},
   "source": [
    "\n",
    "#### Define discrete probability vector\n",
    "\n",
    "Consider the alphabets\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{X}&=\\{0,1,2\\},\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Define the joint probability mass function $p:\\mathcal{X}\\to[0,1]$ by \n",
    "$$p(x)=\\frac{1}{3},\\quad x \\in \\mathcal{X}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5e0920d",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_unif = (1/3)*np.ones(3)  # Uniform distribution for X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ef3c51",
   "metadata": {},
   "source": [
    "### Funtional dependency $y=f(x)=(x-1)^2$ Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6591a45d",
   "metadata": {},
   "source": [
    "Consider the random variable $X$ with probability mass function\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p_X:\\mathcal{X}&\\to[0,1]\\\\\n",
    "p_X(0)&=0.2\\\\\n",
    "p_X(1)&=0.3\\\\\n",
    "p_X(2)&=0.5,\n",
    "\\end{aligned}\n",
    "$$\n",
    "and $Y=(X-1)^2$. Then \n",
    "$$\n",
    "\\begin{aligned}\n",
    "p_Y:\\mathcal{Y}&\\to[0,1]\\\\\n",
    "p_Y(0)&=0.3\\\\\n",
    "p_Y(1)&=0.7,\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and the joint probability mass function $p:\\mathcal{X}\\times\\mathcal{Y}\\to[0,1]$ is given by\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(0,0)&=0, & p(0,1)&=0.2,\\\\\n",
    "p(1,0)&=0.3, & p(1,1)&=0,\\\\\n",
    "p(2,0)&=0, & p(2,1)&=0.5.\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c21debfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pf_XY=array([[0. , 0.2],\n",
      "       [0.3, 0. ],\n",
      "       [0. , 0.5]])\n",
      "Pf_X=array([0.2, 0.3, 0.5])\n",
      "Pf_Y=array([0.3, 0.7])\n"
     ]
    }
   ],
   "source": [
    "Pf_XY = np.array([\n",
    "    [0.00, 0.20],   # X = 0  -> Y = 1\n",
    "    [0.30, 0.00],   # X = 1  -> Y = 0\n",
    "    [0.00, 0.50]    # X = 2  -> Y = 1\n",
    "])\n",
    "Pf_X=Pf_XY.sum(axis=1) \n",
    "Pf_Y=Pf_XY.sum(axis=0)\n",
    "print(f\"{Pf_XY=}\")\n",
    "print(f\"{Pf_X=}\")\n",
    "print(f\"{Pf_Y=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9245cf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesf = [(i, j) for i in range(P_XY.shape[0])\n",
    "                          for j in range(P_XY.shape[1])]\n",
    "samplef_XY = artifitial_sample(valuesf, Pf_XY.flatten())\n",
    "samplef_X = [x for x,_ in samplef_XY]\n",
    "samplef_Y = [y for _, y in samplef_XY]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620c47b0",
   "metadata": {},
   "source": [
    "### Independent example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d821120c",
   "metadata": {},
   "source": [
    "#### Define discrete probability vector\n",
    "\n",
    "Consider the alphabets\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{X}&=\\{0,1,2\\},\\\\\n",
    "\\mathcal{Y}&=\\{0,1\\}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Define the joint probability mass function $p:\\mathcal{X}\\times\\mathcal{Y}\\to[0,1]$ by\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(0,0)&=0.08, & p(0,1)&=0.12,\\\\\n",
    "p(1,0)&=0.20, & p(1,1)&=0.30,\\\\\n",
    "p(2,0)&=0.12, & p(2,1)&=0.18.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d4bd1a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PInd_XY=array([[0.08, 0.12],\n",
      "       [0.2 , 0.3 ],\n",
      "       [0.12, 0.18]])\n",
      "PInd_X=array([0.2, 0.5, 0.3])\n",
      "PInd_Y=array([0.4, 0.6])\n"
     ]
    }
   ],
   "source": [
    "# Joint probability matrix p(x,y)\n",
    "#.   Y=0    Y=1\n",
    "PInd_XY = np.array([\n",
    "    [0.08, 0.12],   # X = 0\n",
    "    [0.20, 0.30],   # X = 1\n",
    "    [0.12, 0.18]    # X = 2\n",
    "])\n",
    "PInd_X = PInd_XY.sum(axis=1) \n",
    "PInd_Y = PInd_XY.sum(axis=0)    \n",
    "print(f\"{PInd_XY=}\")\n",
    "print(f\"{PInd_X=}\")\n",
    "print(f\"{PInd_Y=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f976e371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_(X)P(Y)(x,y)=\n",
      "[[0.08 0.12]\n",
      " [0.2  0.3 ]\n",
      " [0.12 0.18]]\n",
      "P_(X,Y)(x,y)=\n",
      "[[0.08 0.12]\n",
      " [0.2  0.3 ]\n",
      " [0.12 0.18]]\n"
     ]
    }
   ],
   "source": [
    "PInd_XPInd_Y = np.outer(PInd_X, PInd_Y)  # Product of marginals\n",
    "print(f\"P_(X)P(Y)(x,y)=\\n{PInd_XPInd_Y}\")\n",
    "print(f\"P_(X,Y)(x,y)=\\n{PInd_XY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695b56ba",
   "metadata": {},
   "source": [
    "## Definition: Entropy (Average Uncertainty)\n",
    "\n",
    "Entropy measures how uncertain we are about the outcome of a random variable before we observe it. \n",
    "\n",
    "Let $(\\Omega,\\mathcal{F},\\mathbb{P})$ be a probability space and $X:\\Omega \\to \\mathcal{X} \\subset \\mathbb{R}^n$ a discrete random vector with alphabet $\\mathcal{X}$. Let $\\mathbb{P}_X$ be the probability measure (law) induced by $X$.\n",
    "\n",
    "Define the **(joint) entropy of $X$** as the expectation of its information content:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H_{\\mathbb{P}}(X)\n",
    "&:= \\mathbb{E}\\!\\left[ I_{\\mathbb{P}_X}(X) \\right] \\\\\n",
    "&= \\sum_{x \\in \\mathcal{X}} p_X(x)\\, I_{\\mathbb{P}_X}(x) \\\\\n",
    "&= \\sum_{x \\in \\mathcal{X}} p_X(x)\\,(-\\log p_X(x)) \\\\\n",
    "&= -\\sum_{x \\in \\mathcal{X}} p_X(x)\\log p_X(x).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Note:**  \n",
    "* When no ambiguity arises, the dependence on the probability measure is omitted, and we write  \n",
    "  $$ H(X) \\quad \\text{instead of} \\quad H_{\\mathbb{P}}(X). $$\n",
    "\n",
    "**Interpretation:**\n",
    "* High entropy means the outcome of the random variable is (on average) highly unpredictable.\n",
    "* Low entropy means the outcome of the random variable is (on average) highly predictable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded3b47f",
   "metadata": {},
   "source": [
    "## Properties: Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff739b1",
   "metadata": {},
   "source": [
    "1. $H(X) \\ge 0$\n",
    "2. Let $X$ be a discrete random variable with alphabet of length $n$. Then\n",
    "$\n",
    "H(X) \\le \\log n\n",
    "$\n",
    "with equality if and only if $X$ is uniform.\n",
    "\n",
    "3. $X$ is constant $\\Longleftrightarrow$ $H(X)=0$\n",
    "\n",
    "4. The entropy is concave as a function of the induced probability $\\mathbb{P}_X$.\n",
    "\n",
    "    More precisely, let $\\mathcal{M}_1(\\mathcal{X})$ denote the set of all pmf of measueres on the measurable space $(\\mathcal{X}, \\mathcal{P}(\\mathcal{X}))$, with $\\mathcal{X}$ finite.\n",
    "    Define\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    H : \\mathcal{M}_1(\\mathcal{X}) &\\longrightarrow [0,\\infty) \\\\\n",
    "    H(p) &:= -\\sum_{x \\in \\mathcal{X}} p(x)\\,\\log p(x),\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "    with the convention $0 \\log 0 := 0$.\n",
    "\n",
    "    Then $H$ is a strictly concave functional on $\\mathcal{M}_1(\\mathcal{X})$, i.e., for all $p_1,p_2 \\in \\mathcal{M}_1(\\mathcal{X})$ with $p_1 \\neq p_2$ and all $\\lambda \\in (0,1)$,\n",
    "    $$\n",
    "    H\\big( \\lambda p_1 + (1-\\lambda)p_2 \\big)\n",
    "    \\;>\\;\n",
    "    \\lambda H(p_1) + (1-\\lambda)H(p_2).\n",
    "    $$\n",
    "\n",
    "    Notice that for a discrete random variable $X$ with pmf $p_X$,\n",
    "    $$\n",
    "    H_{\\mathbb{P}}(X)\n",
    "    = -\\sum_{x \\in \\mathcal{X}} p_X(x)\\log p_X(x)\n",
    "    = H(p_X).\n",
    "    $$\n",
    "\n",
    "    $H(p_X)$ is called the entropy of the pmf $p_X$. The nature of the argument (a pmf of a probability measure) avoids ambiguity with the entropy of random variables or random vectors.\n",
    "\n",
    "5. $H$ is permutation inavaraiant. Let $X=(X_1,X_2,\\dots,X_n)$ be a random vector and let $\\pi_n$ a permutation of the varaiables in $X$, denote \n",
    "    the corresponding permuted vector as $\\pi_n(X)=(X_{\\pi(1)},X_{\\pi(2)},\\dots,X_{\\pi(n)})$. Then we have \n",
    "    $$H(\\pi_n(X))=H(X).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2447c854",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "888f55e7",
   "metadata": {},
   "source": [
    "#### **Proof (1.):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b8111a",
   "metadata": {},
   "source": [
    "It is straight forward from the positivity of $I(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11913b23",
   "metadata": {},
   "source": [
    "#### **Proof (2.):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9903c201",
   "metadata": {},
   "source": [
    "Let $\\varphi(t)=-\\log t$, which is convex on $(0,\\infty)$, and take a convex combination\n",
    "$\\sum_{i=1}^n \\alpha_i v_i$ with $v_i,\\alpha_i>0$ and $\\sum_{i=1}^n \\alpha_i=1$.\n",
    "\n",
    "By [Jensen’s inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality) ,\n",
    "$$\n",
    "\\varphi\\left(\\sum_{i=1}^n \\alpha_i v_i\\right)\n",
    "\\le\n",
    "\\sum_{i=1}^n \\alpha_i \\varphi\\left(v_i\\right).\n",
    "$$\n",
    "\n",
    "If we denote the alphabet of $X$ by $\\mathcal{X}=\\{x_1,x_2,\\dots,x_n\\}$ and use the previous inequality for\n",
    "$\\alpha_i=p(x_i)$ and $v_i=\\frac{1}{p(x_i)}$, for $i=1,2,\\dots,n$, we obtain\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "-\\log\\left(\\sum_{i=1}^n p(x_i)\\frac{1}{p(x_i)}\\right)\n",
    "&\\le\n",
    "-\\sum_{i=1}^n p(x_i) \\log\\left(\\frac{1}{p(x_i)}\\right),\\\\\n",
    "-\\log(n)\n",
    "&\\le\n",
    "\\sum_{i=1}^n p(x_i)\\log(p(x_i)),\\\\\n",
    "-\\sum_{i=1}^n p(x_i)\\log(p(x_i))\n",
    "&\\le\n",
    "\\log(n),\\\\\n",
    "H(X)\n",
    "&\\le\n",
    "\\log(n).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Equality holds in [Jensen’s inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality)  if and only if\n",
    "$$\n",
    "v_1 = v_2 = \\cdots = v_n\n",
    "\\quad\\text{for all $i$ with } \\alpha_i > 0,\n",
    "$$\n",
    "that is,\n",
    "$$\n",
    "\\frac{1}{p(x_1)} = \\frac{1}{p(x_2)} = \\cdots = \\frac{1}{p(x_n)}.\n",
    "$$\n",
    "\n",
    "Hence all positive probabilities are equal:\n",
    "$$\n",
    "p(x_1) = p(x_2) = \\cdots = p(x_n) = \\frac{1}{n},\n",
    "$$\n",
    "so $X$ is uniform on its (nonzero) alphabet.\n",
    "\n",
    "Therefore,\n",
    "$$\n",
    "H(X) \\le \\log n,\n",
    "$$\n",
    "with equality if and only if $X$ is uniformly distributed on $\\{x_1,\\dots,x_n\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9ae005",
   "metadata": {},
   "source": [
    "#### **Proof (3.):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87811d84",
   "metadata": {},
   "source": [
    "It $X=c$ cte, then \n",
    "$$\n",
    "H(X) = \\sum_{x \\in \\mathcal{X}} p(x) (-\\log p(x)) = 1(-\\log 1) = 0.\n",
    "$$\n",
    "On the other hand, if $H(X)=0$ we get\n",
    "$$\n",
    "\\sum_{x \\in \\mathcal{X}} p(x) (-\\log p(x)) = 0.\n",
    "$$\n",
    "the for every $x$ we get $p(x)\\in \\{0,1\\}$, otherwise one of the terms in the series is positve (and all are no negative). Tnen $X$ is constant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e56793",
   "metadata": {},
   "source": [
    "#### **Proof (4.):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9d988b",
   "metadata": {},
   "source": [
    "Let's start by prohing that the function $h(t)= -t\\log(t)$ is strictly concave in its domain $t>0$. Derivating twice we get\n",
    "$$\n",
    "\\begin{align*}\n",
    "h(t) &= -t\\log(t)\\\\\n",
    "h'(t)&= -\\log(t)-1\\\\\n",
    "h''(t)&= -\\frac{1}{t} < 0, \\quad  t>0.\n",
    "\\end{align*}\n",
    "$$\n",
    "Then for any $\\lambda\\in (0,1)$, and $s,t>0$ we have the inequality\n",
    "$$h(\\lambda t + (1-\\lambda) s)> \\lambda h(t) + (1-\\lambda) h(s).$$\n",
    "\n",
    "Using the concavity of $h$ we can prove the concavity of $H$. Let $,s,t\\in (0,1)$ and $p_1,p_2 \\in \\mathcal{M}_1(\\mathcal{X})$, then we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "H(\\lambda p_1+(1-\\lambda) p_2)&=\\sum_{x\\in\\mathcal{X}}h(\\lambda p_1(x)+(1-\\lambda) p_2(x)),\\\\\n",
    "&> \\sum_{x\\in\\mathcal{X}}\\lambda h(p_1(x))+(1-\\lambda) h(p_2(x)),\\\\\n",
    "&=\\lambda \\sum_{x\\in\\mathcal{X}}h(p_1(x))+(1-\\lambda)  \\sum_{x\\in\\mathcal{X}}h(p_2(x)),\\\\\n",
    "&=\\lambda H(p_1)+(1-\\lambda) H(p_2).\n",
    "\\end{align*}\n",
    "$$\n",
    "So $H$ is strictly concave."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7328ef8e",
   "metadata": {},
   "source": [
    "#### **Proof (5.):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9d72af",
   "metadata": {},
   "source": [
    "Notice that the alphabet of $\\pi_n(X)$ is $\\pi_n(\\mathcal{X})$, where $\\mathcal{X}$ is the alphabet of $X$ and the pmf of $\\pi_n(X)$ satisfies\n",
    "$$\n",
    "\\begin{align*}\n",
    "p_{\\pi_n(X)}(y)&=\\mathbb{P}(\\pi_n(X)=y),\\\\\n",
    "&=\\mathbb{P}\\left(\\cap_{i=1}^nX^{-1}_{\\pi_n(i)}(y_i)\\right),\\\\\n",
    "&=\\mathbb{P}\\left(\\cap_{j=1}^nX^{-1}_{j}(y_{\\pi_n^{-1}(j)})\\right),\\\\\n",
    "&=\\mathbb{P}(X=\\pi^{-1}_n(y)),\\\\\n",
    "&=p_{X}\\left(\\pi^{-1}_n(y)\\right).\n",
    "\\end{align*}\n",
    "$$\n",
    "Then we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "H(\\pi_n(X))&=- \\sum_{y\\in \\pi_n(\\mathcal{X})} p_{\\pi_n(X)}(y)\\log\\left( p_{\\pi_n(X)}(y)\\right),\\\\\n",
    "&=- \\sum_{y\\in \\pi_n(\\mathcal{X})} p_{X}\\left(\\pi^{-1}_n(y)\\right)\\log\\left(p_{X}\\left(\\pi^{-1}_n(y)\\right)\\right),\\\\\n",
    "&=- \\sum_{x\\in \\mathcal{X}} p_{X}(x)\\log\\left( p_{X}(x)\\right),\\\\\n",
    "&= H(X).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82634693",
   "metadata": {},
   "source": [
    "## Code: Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c924662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(P,base=2):\n",
    "    P_pos = P[P > 0]\n",
    "    return -((P_pos*np.log(P_pos)).sum()/np.log(base)).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be47510f",
   "metadata": {},
   "source": [
    "#### Entropy\n",
    "\n",
    "$$H_{\\mathbb{P}}(X)= -\\sum_{x \\in \\mathcal{X}} p_X(x)\\log p_X(x).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "75e5447a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(x,y)=2.408694969562842\n",
      "H(x)=1.4854752972273346\n",
      "H(y)=0.9927744539878083\n"
     ]
    }
   ],
   "source": [
    "print(f\"H(x,y)={sci_entropy(P_XY_flat,base=2)}\")\n",
    "print(f\"H(x)={sci_entropy(P_X,base=2)}\")\n",
    "print(f\"H(y)={sci_entropy(P_Y,base=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "38135511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(x,y)=2.408694969562842\n",
      "H(x)=1.4854752972273346\n",
      "H(y)=0.9927744539878083\n"
     ]
    }
   ],
   "source": [
    "print(f\"H(x,y)={entropy(P_XY_flat)}\")\n",
    "print(f\"H(x)={entropy(P_X)}\")\n",
    "print(f\"H(y)={entropy(P_Y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "28ab112e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(x,y)=2.4 < 2.6\n",
      "H(x)=1.5 < 1.6\n",
      "H(y)=0.99 < 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"H(x,y)={entropy(P_XY_flat):.2} < {np.log2(len(P_XY_flat)):.2}\")\n",
    "print(f\"H(x)={entropy(P_X):.2} < {np.log2(len(P_X)):.2}\")\n",
    "print(f\"H(y)={entropy(P_Y):.2} < {np.log2(len(P_Y)):.2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d8f632",
   "metadata": {},
   "source": [
    "notice like \"less unifom\" variables have smaller entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1434c0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(x)=1.584962500721156 = 1.584962500721156= log(|X_cal|)\n"
     ]
    }
   ],
   "source": [
    "print(f\"H(x)={entropy(P_unif)} = {np.log2(len(P_unif))}= log(|X_cal|)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5637a881",
   "metadata": {},
   "source": [
    "## Definition: Conditional Entropy\n",
    "\n",
    "Let $(\\Omega,\\mathcal{F},\\mathbb{P})$ be a probability space and let \n",
    "$X:\\Omega\\to\\mathcal{X}\\subset\\mathbb{R}^n$ and $Y:\\Omega\\to\\mathcal{Y}\\subset\\mathbb{R}^m$ be discrete random vectors\n",
    "\n",
    "The **conditional entropy of $Y$ given $X$** is defined as the average of the\n",
    "entropy of $Y$ conditioned on each value of $X$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H_{\\mathbb{P}}(Y \\mid X=x)&:= -\\sum_{y \\in \\mathcal{Y}} \\mathbb{P}(Y=y \\mid X=x)\\log\\mathbb{P}(Y=y \\mid X=x),\\\\\n",
    "H_{\\mathbb{P}}(Y \\mid X)\n",
    "&:= \\sum_{x \\in \\mathcal{X}} \\mathbb{P}(X=x)\\, H_{\\mathbb{P}}(Y \\mid X=x) ,\\\\\n",
    "&= - \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}}\n",
    " \\mathbb{P}(X=x,\\,Y=y) \\, \\log  \\mathbb{P}(Y=y\\mid X=x).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Note:**\n",
    "* When no ambiguity arises, the dependence on the probability measure is omitted and we simply write $H(Y \\mid X)$\n",
    "  instead of $H_{\\mathbb{P}}(Y \\mid X)$.\n",
    "* When no ambiguity arises, we also write $p(x,y)$, $p(x)$ and $p(y \\mid x)$ instead of\n",
    "  $\\mathbb{P}(X=x,Y=y)$, $\\mathbb{P}(X=x)$ and $\\mathbb{P}(Y=y \\mid X=x)$ respectively. \n",
    "  So usually we write\n",
    "  $$\n",
    "  \\begin{aligned}\n",
    "  H(Y \\mid X=x)&:= -\\sum_{y \\in \\mathcal{Y}} p(y \\mid x)\\log p(y \\mid x),\\\\\n",
    "  H(Y \\mid X)\n",
    "  &:= \\sum_{x \\in \\mathcal{X}} p(x)\\, H(Y \\mid X=x) ,\\\\\n",
    "  &= - \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}}\n",
    "  p(x,\\,y) \\, \\log  p(y\\mid x).\n",
    "  \\end{aligned}\n",
    "  $$\n",
    "\n",
    "\n",
    "**Interpretation:**\n",
    "* $H_{\\mathbb{P}}(Y \\mid X)$ is the average remaining uncertainty in $Y$ after observing $X$.\n",
    "* $0\\le H_{\\mathbb{P}}(Y \\mid X) \\le H_{\\mathbb{P}}(Y)$.\n",
    "* $H_{\\mathbb{P}}(Y \\mid X) = 0$ if and only if $Y$ is completely determined by $X$ (i.e. $Y = f(X)$ almost surely).\n",
    "* $H_{\\mathbb{P}}(Y \\mid X) = H_{\\mathbb{P}}(Y)$ if and only if $X$ and $Y$ are independent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b2a919",
   "metadata": {},
   "source": [
    "## Properties: Conditional Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c22a50",
   "metadata": {},
   "source": [
    "### Conditional entropy as subspace entropy\n",
    "\n",
    "Let $(\\Omega,\\mathcal{F},\\mathbb{P})$ be a probability space and let \n",
    "$X:\\Omega\\to\\mathcal{X}\\subset\\mathbb{R}^n$ and\n",
    "$Y:\\Omega\\to\\mathcal{Y}\\subset\\mathbb{R}^m$ be discrete random vectors.\n",
    "Fix $x \\in \\mathcal{X}$ with $p(x)>0$ and define\n",
    "$$\n",
    "A := \\{\\omega \\in \\Omega : X(\\omega)=x\\}.\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "H_{\\mathbb{P}}(Y \\mid X = x)\n",
    "= H_{\\mathbb{P}(\\cdot \\mid A)}(Y).\n",
    "$$\n",
    "\n",
    "That is, the conditional entropy of $Y$ given $X=x$ is the entropy of $Y$\n",
    "with respect to the conditional probability space\n",
    "$$\n",
    "\\big( A,\\ \\mathcal{F}\\!\\mid_A,\\ \\mathbb{P}(\\,\\cdot \\mid A) \\big).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cbedb0",
   "metadata": {},
   "source": [
    "#### **Proof:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9961b8",
   "metadata": {},
   "source": [
    "By definition,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H_{\\mathbb{P}}(Y \\mid X = x)\n",
    "&:= -\\sum_{y \\in \\mathcal{Y}}\n",
    "\\mathbb{P}(Y=y \\mid X=x)\\,\\log \\mathbb{P}(Y=y \\mid X=x).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "But for every $y \\in \\mathcal{Y}$,\n",
    "$$\n",
    "\\mathbb{P}(Y=y \\mid X=x)\n",
    "= \\mathbb{P}(Y=y \\mid A)\n",
    "= \\mathbb{P}(\\cdot \\mid A)\\big(Y^{-1}(\\{y\\})\\big).\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H_{\\mathbb{P}}(Y \\mid X = x)\n",
    "&= -\\sum_{y \\in \\mathcal{Y}}\n",
    "\\mathbb{P}(\\cdot \\mid A)\\big(Y=y\\big)\\,\n",
    "\\log \\mathbb{P}(\\cdot \\mid A)\\big(Y=y\\big) \\\\\n",
    "&= H_{\\mathbb{P}(\\cdot \\mid A)}(Y),\n",
    "\\end{aligned}\n",
    "$$\n",
    "which is exactly the entropy of $Y$ in the probability space\n",
    "$\\big( A,\\ \\mathcal{F}\\!\\mid_A,\\ \\mathbb{P}(\\,\\cdot \\mid A) \\big)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b95181",
   "metadata": {},
   "source": [
    "### Positivity\n",
    "\n",
    "\n",
    "For all $x \\in \\mathcal{X}$ with $\\mathbb{P}(X=x)>0$,\n",
    "$$\n",
    "H_{\\mathbb{P}}(Y \\mid X = x) \\ge 0,\n",
    "\\qquad\n",
    "H_{\\mathbb{P}}(Y \\mid X) \\ge 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36f0a15",
   "metadata": {},
   "source": [
    "**Proof (1.).**\n",
    "Using the ositivity of the entropy and the previous result, we have\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "H_{\\mathbb{P}}(Y \\mid X = x)&= H_{\\mathbb{P}(\\cdot \\mid X=x)}(Y)\\geq 0, \\quad x \\in \\mathcal{X},\\\\\n",
    "H_{\\mathbb{P}}(Y \\mid X)&:= \\sum_{x \\in \\mathcal{X}} \\mathbb{P}(X=x)\\, H_{\\mathbb{P}}(Y \\mid X=x)\\geq 0.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32563ac",
   "metadata": {},
   "source": [
    "**Proof (2.):** Notice that $H(Y|X=x)$ is the entropy of the random vector $Y|X=x$ in the space $(\\{X=x\\},\\mathcal{F}\\mid\\{X=x\\},\\mathbb{P}\\mid \\{X=x\\})$\n",
    "$$\n",
    "H_{\\mathbb{P}}(Y \\mid X=x) = -\\sum_{y \\in \\mathcal{Y}} p(y \\mid x)\\log p(y \\mid x)=H_{\\mathbb{P}\\mid \\{X=x\\}}(Y)\n",
    "$$\n",
    "\n",
    "so it is non-negative. $H(Y|X) \\ge 0$ is a direct consequence of $H(Y|X=x) \\ge 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7e5fb9",
   "metadata": {},
   "source": [
    "### Functional dependency\n",
    "Let $X,Y$ be discrete random vectors with finite alphabets. Then\n",
    "$$\n",
    "H(Y \\mid X)=0\n",
    "\\quad\\Longleftrightarrow\\quad\n",
    "\\exists \\, f \\text{ such that } Y = f(X) \\text{ almost surely.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1ef6bc",
   "metadata": {},
   "source": [
    "#### **Proof:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e831c9b",
   "metadata": {},
   "source": [
    "Since\n",
    "$$\n",
    "H(Y\\mid X=x) \\ge 0 \\quad \\text{for all } x.\n",
    "$$\n",
    "\n",
    "If $H(Y\\mid X)=0$, then\n",
    "$$\n",
    "0 = \\sum_{x}p(x)\\,H(Y\\mid X=x)\n",
    "$$\n",
    "is a convex combination of nonnegative numbers. Therefore every term with positive weight must be zero:\n",
    "$$\n",
    "p(x)>0 \\;\\Longrightarrow\\; H_{\\mathbb{P}(\\cdot\\mid X=x)}(Y)= H(Y\\mid X=x)=0.\n",
    "$$\n",
    "\n",
    "Then from property 3. of entropy, for every $x$ such that $p(x)>0$, the random variable $Y$ is constant over the set $\\{X=x\\}$ ($Y|X=x$ is constant), so there exist $f(x)$ such that \n",
    "$$\n",
    "\\mathbb{P}(Y = f(x) \\mid X=x) = 1.\n",
    "$$\n",
    "\n",
    "Then\n",
    "$$\n",
    "\\mathbb{P}\\big(Y = f(X)\\big)\n",
    "= \\sum_{x} \\mathbb{P}\\big(X=x,\\,Y=f(x)\\big)\n",
    "= \\sum_{x} \\mathbb{P}(X=x)\\,\\mathbb{P}\\big(Y=f(x)\\mid X=x\\big)\n",
    "= \\sum_{x} \\mathbb{P}(X=x)\\cdot 1\n",
    "= 1.\n",
    "$$\n",
    "\n",
    "So $Y = f(X)$ almost surely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd912769",
   "metadata": {},
   "source": [
    "Conversely, suppose $Y=f(X)$ a.s. Then for any $x$ with $\\mathbb{P}(X=x)>0$,\n",
    "$$\n",
    "\\mathbb{P}(Y=f(x)\\mid X=x) = 1,\n",
    "$$\n",
    "so the $Y$ is constant over $\\{X=x\\}$ (conditional distribution of $Y$ given $X=x$ is a point mass), hence\n",
    "$$\n",
    "H_{\\mathbb{P}(\\cdot\\mid X=x)}(Y) = H(Y\\mid X=x)=0 \\quad \\text{for all } x \\text{ with } p(x)>0.\n",
    "$$\n",
    "Therefore\n",
    "$$\n",
    "H(Y\\mid X) = \\sum_{x} \\mathbb{P}(X=x)\\,H(Y\\mid X=x) = 0.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8514d89d",
   "metadata": {},
   "source": [
    "### Conditioning cannot increase entropy\n",
    "For discrete random vectors $X$ and $Y$ with finite alphabets,\n",
    "$$\n",
    "H(Y | X) \\le H(Y),\n",
    "$$\n",
    "with equality if and only if $Y$ is independent of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebebc0f6",
   "metadata": {},
   "source": [
    "**Proof:**\n",
    "\n",
    "Notice that $H(Y)=H(\\mathbb{P}_Y)$ and $H(Y|X=x)=H(\\mathbb{P}_{Y|X=x})$ so using the concavity of $H$ we obtain\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "H_{\\mathbb{P}}(Y)&=H(\\mathbb{P}_Y)=H\\left(\\sum_{x}\\mathbb{P}_X(x)\\mathbb{P}_{Y|X=x}\\right)\\ge \\sum_{x}\\mathbb{P}_X(x)H\\left(\\mathbb{P}_{Y|X=x}\\right)= \\sum_{x}\\mathbb{P}_X(x)H(Y|X=x)=H(Y|X).\n",
    "\\end{align*}\n",
    "$$\n",
    "Since the concavity is strict, the equality only happens when $\\mathbb{P}_{Y|X=x*}=\\mathbb{P}_{Y|X=x}$ for certain $x^*$ in $\\mathcal{X}$ and any $x\\in\\mathcal{X}$. Then\n",
    "$$\\mathbb{P}(Y=y)=\\sum_{x}\\mathbb{P}_X(x)\\mathbb{P}_{Y|X=x}=\\mathbb{P}_{Y|X=x*}\\sum_{x}\\mathbb{P}_X(x)=\\mathbb{P}_{Y|X=x*}=\\mathbb{P}_{Y|X=x}= \\mathbb{P}(Y=y|X=x),\\quad x\\in\\mathcal{X},$$\n",
    "and $Y$ and $X$ are independant.\n",
    "On the other hand if $Y$ and $X$ are independant, we have:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "H(Y|X)&=\\sum_{x}p(x)H(Y|X=x),\\\\\n",
    "&=- \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}}\n",
    "  p(x,\\,y) \\, \\log  p(y\\mid x),\\\\\n",
    "  &=- \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}}\n",
    "  p(x)p(y) \\, \\log  p(y),\\\\\n",
    "    &=- \\sum_{x \\in \\mathcal{X}}\n",
    "  p(x) \\sum_{y \\in \\mathcal{Y}}p(y) \\, \\log  p(y),\\\\\n",
    "  &=-\\sum_{y \\in \\mathcal{Y}}p(y) \\, \\log  p(y),\\\\\n",
    "  &=H(Y).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00deffe8",
   "metadata": {},
   "source": [
    "### Alternaty Form:\n",
    "\n",
    "$$\n",
    "H(Y|X) = H(X,Y) - H(X)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33045b2",
   "metadata": {},
   "source": [
    "**Proof:**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H(Y| X) \n",
    "&= \\sum_{x} p(x)\\left( - \\sum_{y} p(y| x)\\log p(y| x)\\right)\\\\\n",
    "&= -\\sum_{x,y} p(y| x)p(x) \\log p(y| x)\\\\\n",
    "&= -\\sum_{x,y} p(x,y)\\log p(y| x)\\\\\n",
    "&= -\\sum_{x,y} p(x,y)\\log \\left(\\frac{p(x,y)}{p(x)}\\right)\\\\\n",
    "&= -\\sum_{x,y} p(x,y)\\left(\\log p(x,y) - \\log p(x)\\right)\\\\\n",
    "&= -\\sum_{x,y} p(x,y)\\log p(x,y) +\\sum_{x,y} p(x,y)\\log p(x)\\\\\n",
    "&= -\\sum_{x,y} p(x,y)\\log p(x,y) +\\sum_{x} p(x)\\log p(x)\\\\\n",
    "&= H(X,Y) - H(X).\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa8310d",
   "metadata": {},
   "source": [
    "### Basic inequality:\n",
    "For two discrete random vectors $X$ and $Y$,\n",
    "$$\n",
    "H(X,Y) \\le H(X) + H(Y)\n",
    "$$\n",
    "with equality if and only if $X$ and $Y$ are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad82755",
   "metadata": {},
   "source": [
    "**Proof:** \n",
    "\n",
    "From the previous identity we have\n",
    "$$\n",
    "H(X,Y) = H(X) + H(Y | X).\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "$$\n",
    "H(X,Y) \\le H(X) + H(Y)\n",
    "\\quad \\Longleftrightarrow \\quad\n",
    "H(Y | X) \\le H(Y).\n",
    "$$\n",
    "\n",
    "Since conditioning cannot increase entropy, and the equality is hold if and only if $X$ and $Y$ are independant, we get the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d04fec",
   "metadata": {},
   "source": [
    "## Code: Conditional Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b85dad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_conditional(P_XY, conditioned_idx):\n",
    "    # Marginal distribution of the conditioning variable\n",
    "    P_cond = P_XY.sum(axis=conditioned_idx, keepdims=True)\n",
    "\n",
    "    # Mask of valid entries\n",
    "    mask = P_XY > 0\n",
    "    \n",
    "    # H(A|B) = - sum p(a,b) log p(a|b)\n",
    "    return -np.sum(P_XY[mask] * np.log2((P_XY / P_cond)[mask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "20c3cff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(Y|X)=0.9232196723355077 < \n",
      "H(Y|X)=0.0\n",
      "H(X|Y)=0.6041843979966417\n"
     ]
    }
   ],
   "source": [
    "drv_ent_cond=drv.entropy_conditional(sample_Y, sample_X, base=2).item()\n",
    "drv_ent_cond_fY=drv.entropy_conditional(samplef_Y, samplef_X, base=2).item()\n",
    "drv_ent_cond_fX=drv.entropy_conditional(samplef_X, samplef_Y, base=2).item()\n",
    "print(f\"H(Y|X)={drv_ent_cond} < \")\n",
    "print(f\"H(Y|X)={drv_ent_cond_fY}\")\n",
    "print(f\"H(X|Y)={drv_ent_cond_fX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8f3dee64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(Y|X)=0.923219672335508\n",
      "H(Y|X)=-0.0\n",
      "H(X|Y)=0.6041843979966417\n"
     ]
    }
   ],
   "source": [
    "ent_cond=entropy_conditional(P_XY,conditioned_idx=1).item()\n",
    "ent_cond_fY=entropy_conditional(Pf_XY,conditioned_idx=1).item()\n",
    "ent_cond_fX=entropy_conditional(Pf_XY,conditioned_idx=0).item()\n",
    "print(f\"H(Y|X)={ent_cond}\")\n",
    "print(f\"H(Y|X)={ent_cond_fY}\")\n",
    "print(f\"H(X|Y)={ent_cond_fX}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b4b6b1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(Y|X)=0.923219672335508 <= 0.9927744539878083=H(Y)\n",
      "H(Y|X)=-0.0 <= 0.8812908992306927=H(Y)\n",
      "H(X|Y)=0.6041843979966417 <= 1.4854752972273346=H(X)\n"
     ]
    }
   ],
   "source": [
    "print(f\"H(Y|X)={ent_cond} <= {entropy(P_Y)}=H(Y)\")\n",
    "print(f\"H(Y|X)={ent_cond_fY} <= {entropy(Pf_Y)}=H(Y)\")\n",
    "print(f\"H(X|Y)={ent_cond_fX} <= {entropy(Pf_X)}=H(X)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dc204bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(X,Y)=2.408694969562842 = 2.4086949695628426 = H(Y|X)+H(X)\n",
      "H(X,Y)=1.4854752972273346 = 1.4854752972273346 = H(Y|X)+H(X)\n",
      "H(X,Y)=1.4854752972273346 = 1.4854752972273344 = H(X|Y)+H(Y)\n"
     ]
    }
   ],
   "source": [
    "print(f\"H(X,Y)={entropy(P_XY)} = {ent_cond+ entropy(P_X)} = H(Y|X)+H(X)\")\n",
    "print(f\"H(X,Y)={entropy(Pf_XY)} = {ent_cond_fY+ entropy(Pf_X)} = H(Y|X)+H(X)\")\n",
    "print(f\"H(X,Y)={entropy(Pf_XY)} = {ent_cond_fX+ entropy(Pf_Y)} = H(X|Y)+H(Y)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf8ee8e",
   "metadata": {},
   "source": [
    "## Definition: Mutual Information\n",
    "\n",
    "Mutual Information measures how much two random vectors depend on each other.\n",
    "\n",
    "$$\n",
    "H_{\\mathbb{P}}(X\\Cap Y) := H_{\\mathbb{P}}(Y) - H_{\\mathbb{P}}(Y \\mid X)\n",
    "$$\n",
    "\n",
    "### Interpretation:\n",
    "* $ H(X\\Cap Y)$ is the **amount of information shared by $X$ and $Y$**\n",
    "* Although it is not a set operation, it is often visualized with a Venn-style diagram where\n",
    "\n",
    "    $$\n",
    "     H(X\\Cap Y) \\;\\; \\text{behaves like} \\;\\;\n",
    "    \\text{“overlap” between } H(X) \\text{ and } H(Y),\n",
    "    $$\n",
    "\n",
    "    and the total entropy decomposes as\n",
    "\n",
    "    $$\n",
    "    H(Y) = H(Y \\mid X) +  H(X\\Cap Y).\n",
    "    $$\n",
    "\n",
    "**Note:** Although mutual information is typically written $I(X;Y)$, we prefer $H(X \\Cap Y)$ because it treats MI as a measure of the size of the shared uncertainty between $X$ and $Y$, keeping it compatible with the interpretation of entropy as a size measure of uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a42c16",
   "metadata": {},
   "source": [
    "## Properties: Mutual Information \n",
    "1. $0\\leq H(X\\Cap Y)\\leq \\min\\{H(X),H(Y)\\}$\n",
    "\n",
    "\n",
    "2. $X$ and $Y$ are independent if and only if\n",
    "    $$\n",
    "     H(X\\Cap Y) = 0.\n",
    "    $$\n",
    "3. Let $X,Y$ be discrete random vectors with finite alphabets. Then\n",
    "$$\n",
    " H(X\\Cap Y) = H(Y)\n",
    "\\quad\\Longleftrightarrow\\quad\n",
    "\\exists \\, f \\text{ such that } Y = f(X) \\text{ almost surely.}\n",
    "$$\n",
    "4. Equivalent form and symetry\n",
    "$$ H(X\\Cap Y) = H(X) + H(Y) - H(X,Y)= H(Y\\Cap X)$$\n",
    "5. KL-divergence form  \n",
    "$$ H(X\\Cap Y) = \\sum_{x,y} p(x,y)\\log \\frac{p(x,y)}{p(x)p(y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f227deae",
   "metadata": {},
   "source": [
    "#### **Proof (1.) and (2.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3a7f67",
   "metadata": {},
   "source": [
    "This is strightforward from, definition, the \"conditionning cannot increase entropy\" propert and the fact the simetry of $\\text{MI}$. Simetry will be proved on 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44fd622",
   "metadata": {},
   "source": [
    "#### **Proof (3.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613c1001",
   "metadata": {},
   "source": [
    "This is strightforward from the entropy \"functional dependency\" property and the definition of mutual information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9bf346",
   "metadata": {},
   "source": [
    "#### **Proof (4.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7160775",
   "metadata": {},
   "source": [
    "This is strightforward from the alternative from property."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082bf000",
   "metadata": {},
   "source": [
    "#### **Proof (5.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a188381e",
   "metadata": {},
   "source": [
    "Developing the KL divergence form we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_{x,y} p(x,y)\\log \\frac{p(x,y)}{p(x)p(y)}&= \\sum_{x,y} p(x,y)\\left(\\log p(x,y) -\\log p(x) - \\log p(y) \\right),\\\\\n",
    "&= \\sum_{x,y} p(x,y)\\log p(x,y) - \\sum_{x,y} p(x,y)\\log p(x)-  \\sum_{x,y} p(x,y)\\log p(y),\\\\\n",
    "&= - \\sum_{x} p(x)\\log p(x)-  \\sum_{y} p(y)\\log p(y) + \\sum_{x,y} p(x,y)\\log p(x,y) ,\\\\\n",
    "&=H(X) + H(Y) - H(X,Y),\\\\\n",
    "& = H(Y;X)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e77e41",
   "metadata": {},
   "source": [
    "## Code: Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2f94de87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_info(P_XY, base=2):\n",
    "    P_X = P_XY.sum(axis=1)\n",
    "    P_Y = P_XY.sum(axis=0)\n",
    "    P_XP_Y = np.outer(P_X, P_Y)\n",
    "\n",
    "    # Mask of valid entries\n",
    "    mask = P_XY > 0\n",
    "\n",
    "    # I(X;Y) = sum p(x,y) log( p(x,y) / (p(x)p(y)) )\n",
    "    return (np.sum(P_XY[mask] * np.log((P_XY / P_XP_Y)[mask]))/np.log(base)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cd2b96e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04821170079675466"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutual_info(P_XY,base=np.e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c4278ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04821170079675517"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutual_info_score(sample_X,sample_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bc4b795d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MI(X,Y)=0.06955478165230043 = 0.06955478165230078 = H(X)-H(X|Y)\n",
      "MI(X,Y)=0.06955478165230043 = 0.06955478165230033 = H(Y)-H(Y|X)\n",
      "MI(X,Y)=0.06955478165230043 = 0.06955478165230078 = H(X)+H(Y)-H(X,Y)\n"
     ]
    }
   ],
   "source": [
    "print(f\"MI(X,Y)={mutual_info(P_XY)} = {entropy(P_X)-entropy_conditional(P_XY,0)} = H(X)-H(X|Y)\")\n",
    "print(f\"MI(X,Y)={mutual_info(P_XY)} = {entropy(P_Y)-entropy_conditional(P_XY,1)} = H(Y)-H(Y|X)\")\n",
    "print(f\"MI(X,Y)={mutual_info(P_XY)} = {entropy(P_X)+entropy(P_Y)-entropy(P_XY)} = H(X)+H(Y)-H(X,Y)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf26a644",
   "metadata": {},
   "source": [
    "#### Bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f1bfa662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MI(X,Y)=0.06955478165230043 <= min(1.4854752972273346, 0.9927744539878083) = min(H(X),H(Y))\n"
     ]
    }
   ],
   "source": [
    "print(f\"MI(X,Y)={mutual_info(P_XY)} <= min{entropy(P_X),entropy(P_Y)} = min(H(X),H(Y))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ebf3ad",
   "metadata": {},
   "source": [
    "#### Independence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b776d132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MI(X,Y)=-2.5627412030519346e-17\n"
     ]
    }
   ],
   "source": [
    "print(f\"MI(X,Y)={mutual_info(PInd_XY)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfa7cc6",
   "metadata": {},
   "source": [
    "#### Functional dependance $Y=f(X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b5c6b3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MI(X,Y)=0.8812908992306927 < 1.4854752972273346 = H(X)\n",
      "MI(X,Y)=0.8812908992306927 = 0.8812908992306927 = H(Y)\n"
     ]
    }
   ],
   "source": [
    "print(f\"MI(X,Y)={mutual_info(Pf_XY)} < {entropy(Pf_X)} = H(X)\")\n",
    "print(f\"MI(X,Y)={mutual_info(Pf_XY)} = {entropy(Pf_Y)} = H(Y)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a97c354",
   "metadata": {},
   "source": [
    "## Definition: Continuous case\n",
    "\n",
    "If $X,Y$ are continuous, sums become integrals:\n",
    "\n",
    "$$\n",
    " H(Y\\Cap X) = \\int\\int p(x,y)\\log\\left(\\frac{p(x,y)}{p(x)p(y)}\\right)dxdy\n",
    "$$\n",
    "\n",
    "Now it is not bounded above anymore. Still non-negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c9239",
   "metadata": {},
   "source": [
    "## Definition: Cross-Entropy (Average Surprise Under a Model)\n",
    "\n",
    "Let $\\mathcal{M}_1(\\mathcal{X})$ denote the set of all pmf of probability measures on the measurable space $(\\mathcal{X},\\,\\mathcal{P}(\\mathcal{X}))$, with $\\mathcal{X}$ finite.\n",
    "\n",
    "We define the **cross-entropy** as the functional\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H : \\mathcal{M}_1(\\mathcal{X})^2 &\\longrightarrow [0,\\infty] \\\\\n",
    "H(p \\Vert q) \n",
    "&:= \\sum_{x \\in \\mathcal{X}} p(x)\\,\\big(-\\log q(x)\\big),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "with the convention $0\\log 0 := 0$.\n",
    "\n",
    "**Remark:**\n",
    "If there exists $x \\in \\mathcal{X}$ such that\n",
    "$$\n",
    "p(x) > 0 \\quad \\text{and} \\quad q(x) = 0,\n",
    "$$\n",
    "then\n",
    "$$\n",
    "H(p \\Vert q) = \\infty.\n",
    "$$\n",
    "This reflects the fact that outcomes that actually occur under $p$ are assigned zero probability under the model $q$.\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "* High cross-entropy $H(p\\Vert q)$ means that outcomes generated by the true distribution $p$\n",
    "  are, on average, very hard to predict when we use the model $q$\n",
    "  (i.e. the model $q$ assigns low probability to typical events of $p$).\n",
    "\n",
    "* Low cross-entropy $H(p\\Vert q)$ means that outcomes generated by $p$\n",
    "  are, on average, easy to predict using the model $q$, indicating that\n",
    "  $q$ is close to the true distribution $p$.\n",
    "\n",
    "* In particular, the cross-entropy can be seen as the entropy of $p$ plus a\n",
    "  **penalty term** called Kullback–Leibler (KL) Divergence that measures how poorly $q$ approximates $p$. In the KL-divergence section, we prove\n",
    "  $$H(p \\Vert q) =  D_{KL}(p \\,\\|\\, q)+ H(p)\\geq H(p), $$\n",
    "    with equality if and only if $p = q$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b33357",
   "metadata": {},
   "source": [
    "## Code: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7af8124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(P, Q,base=2):\n",
    "    P_pos = P[P > 0]\n",
    "    Q_pos = Q[P > 0]\n",
    "    return -((P_pos * np.log(Q_pos)).sum()/np.log(base)).item()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "827bb06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(p_(X,Y)||p_(X,Y)) = 1.6695801269814066\n"
     ]
    }
   ],
   "source": [
    "# All possible (x,y) in the same order as flatten\n",
    "values = [(i, j) for i in range(P_XY.shape[0])\n",
    "                  for j in range(P_XY.shape[1])]\n",
    "\n",
    "P_flat = P_XY.flatten()\n",
    "n_classes = len(values)\n",
    "label_map = {pair: i for i, pair in enumerate(values)}\n",
    "y_true = np.array([label_map[pair] for pair in sample_XY])\n",
    "y_pred = np.tile(P_flat, (len(sample_XY), 1)) \n",
    "\n",
    "print(f\"H(p_(X,Y)||p_(X,Y)) = {log_loss(y_true, y_pred, labels=np.arange(n_classes))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "96edc437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(p_(X,Y)||p_(X,Y)) = 1.6695801269814072\n"
     ]
    }
   ],
   "source": [
    "print(f\"H(p_(X,Y)||p_(X,Y)) = {cross_entropy(P_XY.flatten(), P_XY.flatten(),base=np.e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ddaff4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(p_(X,Y)) = 1.6695801269814072\n"
     ]
    }
   ],
   "source": [
    "print(f\"H(p_(X,Y)) = {entropy(P_XY,base=np.e)}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
