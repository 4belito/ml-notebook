{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "326628c3",
   "metadata": {},
   "source": [
    "# [Entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695b56ba",
   "metadata": {},
   "source": [
    "## Definition: Entropy (Average Uncertainty)\n",
    "\n",
    "Entropy measures how uncertain we are about the outcome of a random variable before we observe it. \n",
    "\n",
    "Let $(\\Omega,\\mathcal{F},\\mathbb{P})$ be a probability space and $X:\\Omega \\to \\mathcal{X} \\subset \\mathbb{R}^n$ a discrete random vector with alphabet $\\mathcal{X}$. Let $\\mathbb{P}_X$ be the probability measure (law) induced by $X$.\n",
    "\n",
    "Define the **(joint) entropy of $X$** as the expectation of its information content:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H_{\\mathbb{P}}(X)\n",
    "&:= \\mathbb{E}\\!\\left[ I_{\\mathbb{P}_X}(X) \\right] \\\\\n",
    "&= \\sum_{x \\in \\mathcal{X}} \\mathbb{P}_X(x)\\, I_{\\mathbb{P}_X}(x) \\\\\n",
    "&= \\sum_{x \\in \\mathcal{X}} \\mathbb{P}_X(x)\\,(-\\log \\mathbb{P}_X(x)) \\\\\n",
    "&= -\\sum_{x \\in \\mathcal{X}} \\mathbb{P}_X(x)\\log \\mathbb{P}_X(x).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Note:**  \n",
    "* When no ambiguity arises, the dependence on the probability measure is omitted, and we write  \n",
    "  $$ H(X) \\quad \\text{instead of} \\quad H_{\\mathbb{P}}(X). $$\n",
    "* When no ambiguity arises, we also write $p$ instead of $\\mathbb{P}_X$, so that\n",
    "  $$\n",
    "  H(X) = -\\sum_{x \\in \\mathcal{X}} p(x)\\log p(x).\n",
    "  $$\n",
    "\n",
    "**Interpretation:**\n",
    "* High entropy means the outcome of the random variable is (on average) highly unpredictable.\n",
    "* Low entropy means the outcome of the random variable is (on average) highly predictable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff739b1",
   "metadata": {},
   "source": [
    "### Properties\n",
    "\n",
    "1. $H(X) \\ge 0$\n",
    "2. Let $X$ be a discrete random variable with alphabet of length $n$. Then\n",
    "$\n",
    "H(X) \\le \\log n\n",
    "$\n",
    "with equality if and only if $X$ is uniform.\n",
    "\n",
    "3. $X$ is constant $\\Longleftrightarrow$ $H(X)=0$\n",
    "\n",
    "4. The entropy is concave as a function of the induced probability $\\mathbb{P}_X$.\n",
    "\n",
    "    More precisely, let $\\mathcal{M}_1(\\mathcal{X})$ denote the set of all probability\n",
    "    measures on the measurable space $(\\mathcal{X}, \\mathcal{P}(\\mathcal{X}))$.\n",
    "    Define\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    H : \\mathcal{M}_1(\\mathcal{X}) &\\longrightarrow [0,\\infty) \\\\\n",
    "    H(\\mu) &:= -\\sum_{x \\in \\mathcal{X}} \\mu(x)\\,\\log \\mu(x),\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "    with the convention $0 \\log 0 := 0$.\n",
    "\n",
    "    Then $H$ is a strictly concave functional on $\\mathcal{M}_1(\\mathcal{X})$,    \n",
    "    i.e., for all $\\mu_1,\\mu_2 \\in \\mathcal{M}_1(\\mathcal{X})$ and all $\\lambda \\in (0,1)$,\n",
    "    $$\n",
    "    H\\big( \\lambda \\mu_1 + (1-\\lambda)\\mu_2 \\big)\n",
    "    \\;>\\;\n",
    "    \\lambda H(\\mu_1) + (1-\\lambda)H(\\mu_2).\n",
    "    $$\n",
    "\n",
    "\n",
    "    Notice that for a discrete random variable $X$ with law $\\mathbb{P}_X$,\n",
    "    $$\n",
    "    H_{\\mathbb{P}}(X)\n",
    "    = -\\sum_{x \\in \\mathcal{X}} \\mathbb{P}_X(x)\\log \\mathbb{P}_X(x)\n",
    "    = H(\\mathbb{P}_X).\n",
    "    $$\n",
    "\n",
    "    $H(\\mathbb{P}_X)$ is called the entropy of the distribution $\\mathbb{P}_X$, the nature of the argument (a probability measure) avoids ambiguity with the entropy of random vectors.\n",
    "\n",
    "5. $H$ is permutation inavaraiant. Let $X=(X_1,X_2,\\dots,X_n)$ be a random vector and let $\\pi_n$ a permutation of the varaiables in $X$, denote \n",
    "    the corresponding permuted vector as $\\pi_n(X)=(X_{\\pi(1)},X_{\\pi(2)},\\dots,X_{\\pi(n)})$. Then we have \n",
    "    $$H(\\pi_n(X))=H(X).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888f55e7",
   "metadata": {},
   "source": [
    "#### **Proof (1.):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b8111a",
   "metadata": {},
   "source": [
    "It is straight forward from the positivity of $I(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11913b23",
   "metadata": {},
   "source": [
    "#### **Proof (2.):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9903c201",
   "metadata": {},
   "source": [
    "Let $\\varphi(t)=-\\log t$, which is convex on $(0,\\infty)$, and take a convex combination\n",
    "$\\sum_{i=1}^n \\alpha_i v_i$ with $v_i,\\alpha_i>0$ and $\\sum_{i=1}^n \\alpha_i=1$.\n",
    "\n",
    "By [Jensen’s inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality) ,\n",
    "$$\n",
    "\\varphi\\left(\\sum_{i=1}^n \\alpha_i v_i\\right)\n",
    "\\le\n",
    "\\sum_{i=1}^n \\alpha_i \\varphi\\left(v_i\\right).\n",
    "$$\n",
    "\n",
    "If we denote the alphabet of $X$ by $\\mathcal{X}=\\{x_1,x_2,\\dots,x_n\\}$ and use the previous inequality for\n",
    "$\\alpha_i=p(x_i)$ and $v_i=\\frac{1}{p(x_i)}$, for $i=1,2,\\dots,n$, we obtain\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "-\\log\\left(\\sum_{i=1}^n p(x_i)\\frac{1}{p(x_i)}\\right)\n",
    "&\\le\n",
    "-\\sum_{i=1}^n p(x_i) \\log\\left(\\frac{1}{p(x_i)}\\right),\\\\\n",
    "-\\log(n)\n",
    "&\\le\n",
    "\\sum_{i=1}^n p(x_i)\\log(p(x_i)),\\\\\n",
    "-\\sum_{i=1}^n p(x_i)\\log(p(x_i))\n",
    "&\\le\n",
    "\\log(n),\\\\\n",
    "H(X)\n",
    "&\\le\n",
    "\\log(n).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Equality holds in [Jensen’s inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality)  if and only if\n",
    "$$\n",
    "v_1 = v_2 = \\cdots = v_n\n",
    "\\quad\\text{for all $i$ with } \\alpha_i > 0,\n",
    "$$\n",
    "that is,\n",
    "$$\n",
    "\\frac{1}{p(x_1)} = \\frac{1}{p(x_2)} = \\cdots = \\frac{1}{p(x_n)}.\n",
    "$$\n",
    "\n",
    "Hence all positive probabilities are equal:\n",
    "$$\n",
    "p(x_1) = p(x_2) = \\cdots = p(x_n) = \\frac{1}{n},\n",
    "$$\n",
    "so $X$ is uniform on its (nonzero) alphabet.\n",
    "\n",
    "Therefore,\n",
    "$$\n",
    "H(X) \\le \\log n,\n",
    "$$\n",
    "with equality if and only if $X$ is uniformly distributed on $\\{x_1,\\dots,x_n\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9ae005",
   "metadata": {},
   "source": [
    "#### **Proof (3.):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87811d84",
   "metadata": {},
   "source": [
    "It $X=c$ cte, then \n",
    "$$\n",
    "H(X) = \\sum_{x \\in \\mathcal{X}} p(x) (-\\log p(x)) = 1(-\\log 1) = 0.\n",
    "$$\n",
    "On the other hand, if $H(X)=0$ we get\n",
    "$$\n",
    "\\sum_{x \\in \\mathcal{X}} p(x) (-\\log p(x)) = 0.\n",
    "$$\n",
    "the for every $x$ we get $p(x)\\in \\{0,1\\}$, otherwise one of the terms in the series is positve (and all are no negative). Tnen $X$ is constant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e56793",
   "metadata": {},
   "source": [
    "#### **Proof (4.):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9d988b",
   "metadata": {},
   "source": [
    "Let's start by prohing that the function $h(t)= -t\\log(t)$ is strictly concave in its domain $t>0$. Derivating twice we get\n",
    "$$\n",
    "\\begin{align*}\n",
    "h(t) &= -t\\log(t)\\\\\n",
    "h'(t)&= -\\log(t)-1\\\\\n",
    "h''(t)&= -\\frac{1}{t} < 0, \\quad  t>0.\n",
    "\\end{align*}\n",
    "$$\n",
    "Then for any $\\lambda\\in (0,1)$, and $s,t>0$ we have the inequality\n",
    "$$h(\\lambda t + (1-\\lambda) s)> \\lambda h(t) + (1-\\lambda) h(s).$$\n",
    "\n",
    "Using the concavity of $h$ we can prove the concavity of $H$. Let $,s,t\\in (0,1)$ and $\\mu_1,\\mu_2 \\in \\mathcal{P}(\\mathcal{X})$, then we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "H(\\lambda \\mu_1+(1-\\lambda) \\mu_2)&=\\sum_{x\\in\\mathcal{X}}h(\\lambda \\mu_1(x)+(1-\\lambda) \\mu_2(x)),\\\\\n",
    "&> \\sum_{x\\in\\mathcal{X}}\\lambda h(\\mu_1(x))+(1-\\lambda) h(\\mu_2(x)),\\\\\n",
    "&=\\lambda \\sum_{x\\in\\mathcal{X}}h(\\mu_1(x))+(1-\\lambda)  \\sum_{x\\in\\mathcal{X}}h(\\mu_2(x)),\\\\\n",
    "&=\\lambda H(\\mu_1)+(1-\\lambda) H(\\mu_2).\n",
    "\\end{align*}\n",
    "$$\n",
    "So $H$ is strictly concave."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7328ef8e",
   "metadata": {},
   "source": [
    "#### **Proof (5.):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9d72af",
   "metadata": {},
   "source": [
    "Notice that the alphabet of $\\pi_n(X)$ is $\\pi_n(\\mathcal{X})$, where $\\mathcal{X}$ is the alphabet of $X$. Then we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "H(\\pi_n(X))&=- \\sum_{y\\in \\pi_n(\\mathcal{X})} \\mathbb{P}_{\\pi_n(X)}(y)\\log\\left( \\mathbb{P}_{\\pi_n(X)}(y)\\right),\\\\\n",
    "&=- \\sum_{y\\in \\pi_n(\\mathcal{X})} \\mathbb{P}(\\pi_n(X)=y)\\log\\left(\\mathbb{P}(\\pi_n(X)=y)\\right),\\\\\n",
    "&=- \\sum_{y\\in \\pi_n(\\mathcal{X})} \\mathbb{P}\\left(\\cap_{i=1}^nX^{-1}_{\\pi_n(i)}(y_i)\\right)\\log\\left(\\mathbb{P}\\left(\\cap_{i=1}^nX^{-1}_{\\pi_n(i)}(y_i)\\right)\\right),\\\\\n",
    "&=- \\sum_{y\\in \\pi_n(\\mathcal{X})} \\mathbb{P}\\left(\\cap_{i=1}^nX^{-1}_{i}(y_{\\pi_n^{-1}(i)})\\right)\\log\\left(\\mathbb{P}\\left(\\cap_{i=1}^nX^{-1}_{i}(y_{\\pi_n^{-1}(i)})\\right)\\right),\\\\\n",
    "&=- \\sum_{y\\in \\pi_n(\\mathcal{X})} \\mathbb{P}(X=\\pi^{-1}_n(y))\\log\\left(\\mathbb{P}(X=\\pi^{-1}_n(y))\\right),\\\\\n",
    "&=- \\sum_{x\\in \\mathcal{X}} \\mathbb{P}(X=x)\\log\\left(\\mathbb{P}(X=x)\\right),\\\\\n",
    "&=- \\sum_{x\\in \\mathcal{X}} \\mathbb{P}_{X}(x)\\log\\left( \\mathbb{P}_{X}(x)\\right),\\\\\n",
    "&= H(X)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5637a881",
   "metadata": {},
   "source": [
    "## Definition: Conditional Entropy (Average Remaining Uncertainty)\n",
    "\n",
    "Let $(\\Omega,\\mathcal{F},\\mathbb{P})$ be a probability space and let \n",
    "$X:\\Omega\\to\\mathcal{X}\\subset\\mathbb{R}^n$ and $Y:\\Omega\\to\\mathcal{Y}\\subset\\mathbb{R}^m$ be discrete random vectors\n",
    "with joint probability mass function\n",
    "$$\n",
    "p(x,y) := \\mathbb{P}(X=x,\\,Y=y),\n",
    "$$\n",
    "and conditional probability\n",
    "$$\n",
    "p(y \\mid x) := \\mathbb{P}(Y=y \\mid X=x),\n",
    "\\qquad \\text{for } p(x)>0,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "p(x) := \\mathbb{P}(X=x) = \\sum_{y \\in \\mathcal{Y}} p(x,y).\n",
    "$$\n",
    "\n",
    "The **conditional entropy of $Y$ given $X$** is defined as the average of the\n",
    "entropy of $Y$ conditioned on each value of $X$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H_{\\mathbb{P}}(Y \\mid X)\n",
    "&:= \\sum_{x \\in \\mathcal{X}} \\mathbb{P}(X=x)\\, H_{\\mathbb{P}}(Y \\mid X=x) \\\\[4pt]\n",
    "&= \\sum_{x \\in \\mathcal{X}} p(x)\n",
    "\\left(\n",
    "-\\sum_{y \\in \\mathcal{Y}} p(y \\mid x)\\log p(y \\mid x)\n",
    "\\right) \\\\[4pt]\n",
    "&= - \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}}\n",
    "p(x,y)\\, \\log p(y \\mid x).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Note:**\n",
    "* When no ambiguity arises, the dependence on the probability measure is omitted and we simply write $H(Y \\mid X)$\n",
    "  instead of $H_{\\mathbb{P}}(Y \\mid X)$.\n",
    "* When no ambiguity arises, we also write $p(x,y)$, $p(x)$ and $p(y \\mid x)$ instead of\n",
    "  $\\mathbb{P}(X=x,Y=y)$, $\\mathbb{P}(X=x)$ and $\\mathbb{P}(Y=y \\mid X=x)$ respectively.\n",
    "\n",
    "**Interpretation:**\n",
    "* $H_{\\mathbb{P}}(Y \\mid X)$ is the average remaining uncertainty in $Y$ after observing $X$.\n",
    "* $H_{\\mathbb{P}}(Y \\mid X) = 0$ if and only if $Y$ is completely determined by $X$ (i.e. $Y = f(X)$ almost surely).\n",
    "* $H_{\\mathbb{P}}(Y \\mid X) = H_{\\mathbb{P}}(Y)$ if and only if $X$ and $Y$ are independent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c22a50",
   "metadata": {},
   "source": [
    "### Property: Conditional entropy as subspace entropy\n",
    "\n",
    "Let $(\\Omega,\\mathcal{F},\\mathbb{P})$ be a probability space and let \n",
    "$X:\\Omega\\to\\mathcal{X}\\subset\\mathbb{R}^n$ and\n",
    "$Y:\\Omega\\to\\mathcal{Y}\\subset\\mathbb{R}^m$ be discrete random vectors.\n",
    "Fix $x \\in \\mathcal{X}$ with $p(x)>0$ and define\n",
    "$$\n",
    "A := \\{\\omega \\in \\Omega : X(\\omega)=x\\}.\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "H_{\\mathbb{P}}(Y \\mid X = x)\n",
    "= H_{\\mathbb{P}(\\cdot \\mid A)}(Y).\n",
    "$$\n",
    "\n",
    "That is, the conditional entropy of $Y$ given $X=x$ is the entropy of $Y$\n",
    "with respect to the conditional probability space\n",
    "$$\n",
    "\\big( A,\\ \\mathcal{F}\\!\\mid_A,\\ \\mathbb{P}(\\,\\cdot \\mid A) \\big).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cbedb0",
   "metadata": {},
   "source": [
    "#### **Proof:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9961b8",
   "metadata": {},
   "source": [
    "By definition,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H_{\\mathbb{P}}(Y \\mid X = x)\n",
    "&:= -\\sum_{y \\in \\mathcal{Y}}\n",
    "\\mathbb{P}(Y=y \\mid X=x)\\,\\log \\mathbb{P}(Y=y \\mid X=x).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "But for every $y \\in \\mathcal{Y}$,\n",
    "$$\n",
    "\\mathbb{P}(Y=y \\mid X=x)\n",
    "= \\mathbb{P}(Y=y \\mid A)\n",
    "= \\mathbb{P}(\\cdot \\mid A)\\big(Y^{-1}(\\{y\\})\\big).\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H_{\\mathbb{P}}(Y \\mid X = x)\n",
    "&= -\\sum_{y \\in \\mathcal{Y}}\n",
    "\\mathbb{P}(\\cdot \\mid A)\\big(Y=y\\big)\\,\n",
    "\\log \\mathbb{P}(\\cdot \\mid A)\\big(Y=y\\big) \\\\\n",
    "&= H_{\\mathbb{P}(\\cdot \\mid A)}(Y),\n",
    "\\end{aligned}\n",
    "$$\n",
    "which is exactly the entropy of $Y$ in the probability space\n",
    "$\\big( A,\\ \\mathcal{F}\\!\\mid_A,\\ \\mathbb{P}(\\,\\cdot \\mid A) \\big)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b95181",
   "metadata": {},
   "source": [
    "### Property: Positivity\n",
    "\n",
    "\n",
    "For all $x \\in \\mathcal{X}$ with $p(x)>0$,\n",
    "$$\n",
    "H_{\\mathbb{P}}(Y \\mid X = x) \\ge 0,\n",
    "\\qquad\n",
    "H_{\\mathbb{P}}(Y \\mid X) \\ge 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36f0a15",
   "metadata": {},
   "source": [
    "**Proof.**\n",
    "Using the ositivity of the entropy and the previous result, we have\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "H_{\\mathbb{P}}(Y \\mid X = x)&= H_{\\mathbb{P}(\\cdot \\mid X=x)}(Y)\\geq 0, \\quad x \\in \\mathcal{X},\\\\\n",
    "H_{\\mathbb{P}}(Y \\mid X)&:= \\sum_{x \\in \\mathcal{X}} \\mathbb{P}(X=x)\\, H_{\\mathbb{P}}(Y \\mid X=x)\\geq 0.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85191cbf",
   "metadata": {},
   "source": [
    "#### **Proof:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32563ac",
   "metadata": {},
   "source": [
    "Notice that $H(Y|X=x)$ is the entropy of the random vector $Y|X=x$ in the space $(\\{X=x\\},\\mathcal{F}\\mid\\{X=x\\},\\mathbb{P}\\mid \\{X=x\\})$\n",
    "$$\n",
    "H_{\\mathbb{P}}(Y \\mid X=x) = -\\sum_{y \\in \\mathcal{Y}} p(y \\mid x)\\log p(y \\mid x)=H_{\\mathbb{P}\\mid \\{X=x\\}}(Y)\n",
    "$$\n",
    "\n",
    "so it is non-negative. $H(Y|X) \\ge 0$ is a direct consequence of $H(Y|X=x) \\ge 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7e5fb9",
   "metadata": {},
   "source": [
    "### Property: Functional dependency\n",
    "Let $X,Y$ be discrete random vectors with finite alphabets. Then\n",
    "$$\n",
    "H(Y \\mid X)=0\n",
    "\\quad\\Longleftrightarrow\\quad\n",
    "\\exists \\, f \\text{ such that } Y = f(X) \\text{ almost surely.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1ef6bc",
   "metadata": {},
   "source": [
    "#### **Proof:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e831c9b",
   "metadata": {},
   "source": [
    "Since\n",
    "$$\n",
    "H(Y\\mid X=x) \\ge 0 \\quad \\text{for all } x.\n",
    "$$\n",
    "\n",
    "If $H(Y\\mid X)=0$, then\n",
    "$$\n",
    "0 = \\sum_{x}p(x)\\,H(Y\\mid X=x)\n",
    "$$\n",
    "is a convex combination of nonnegative numbers. Therefore every term with positive weight must be zero:\n",
    "$$\n",
    "p(x)>0 \\;\\Longrightarrow\\; H_{\\mathbb{P}(\\cdot\\mid X=x)}(Y)= H(Y\\mid X=x)=0.\n",
    "$$\n",
    "\n",
    "Then from property 3. of entropy, for every $x$ such that $p(x)>0$, the random variable $Y$ is constant over the set $\\{X=x\\}$ ($Y|X=x$ is constant), so there exist $f(x)$ such that \n",
    "$$\n",
    "\\mathbb{P}(Y = f(x) \\mid X=x) = 1.\n",
    "$$\n",
    "\n",
    "Then\n",
    "$$\n",
    "\\mathbb{P}\\big(Y = f(X)\\big)\n",
    "= \\sum_{x} \\mathbb{P}\\big(X=x,\\,Y=f(x)\\big)\n",
    "= \\sum_{x} \\mathbb{P}(X=x)\\,\\mathbb{P}\\big(Y=f(x)\\mid X=x\\big)\n",
    "= \\sum_{x} \\mathbb{P}(X=x)\\cdot 1\n",
    "= 1.\n",
    "$$\n",
    "\n",
    "So $Y = f(X)$ almost surely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd912769",
   "metadata": {},
   "source": [
    "Conversely, suppose $Y=f(X)$ a.s. Then for any $x$ with $\\mathbb{P}(X=x)>0$,\n",
    "$$\n",
    "\\mathbb{P}(Y=f(x)\\mid X=x) = 1,\n",
    "$$\n",
    "so the $Y$ is constant over $\\{X=x\\}$ (conditional distribution of $Y$ given $X=x$ is a point mass), hence\n",
    "$$\n",
    "H_{\\mathbb{P}(\\cdot\\mid X=x)}(Y) = H(Y\\mid X=x)=0 \\quad \\text{for all } x \\text{ with } p(x)>0.\n",
    "$$\n",
    "Therefore\n",
    "$$\n",
    "H(Y\\mid X) = \\sum_{x} \\mathbb{P}(X=x)\\,H(Y\\mid X=x) = 0.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8514d89d",
   "metadata": {},
   "source": [
    "### Property: Conditioning cannot increase entropy\n",
    "For discrete random vectors $X$ and $Y$ with finite alphabets,\n",
    "$$\n",
    "H(Y | X) \\le H(Y),\n",
    "$$\n",
    "with equality if and only if $Y$ is independent of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebebc0f6",
   "metadata": {},
   "source": [
    "**Proof:**\n",
    "\n",
    "Notice that $H(Y)=H(\\mathbb{P}_Y)$ and $H(Y|X=x)=H(\\mathbb{P}_{Y|X=x})$ so using the concavity of $H$ we obtain\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "H_{\\mathbb{P}}(Y)&=H(\\mathbb{P}_Y)=H\\left(\\sum_{x}\\mathbb{P}_X(x)\\mathbb{P}_{Y|X=x}\\right)\\ge \\sum_{x}\\mathbb{P}_X(x)H\\left(\\mathbb{P}_{Y|X=x}\\right)= \\sum_{x}\\mathbb{P}_X(x)H(Y|X=x)=H(Y|X).\n",
    "\\end{align*}\n",
    "$$\n",
    "Since the concavity is strict, the equality only happens when $\\mathbb{P}_{Y|X=x*}=\\mathbb{P}_{Y|X=x}$ for certain $x^*$ in $\\mathcal{X}$ and any $x\\in\\mathcal{X}$. Then\n",
    "$$\\mathbb{P}(Y=y)=\\sum_{x}\\mathbb{P}_X(x)\\mathbb{P}_{Y|X=x}=\\mathbb{P}_{Y|X=x*}\\sum_{x}\\mathbb{P}_X(x)=\\mathbb{P}_{Y|X=x*}=\\mathbb{P}_{Y|X=x}= \\mathbb{P}(Y=y|X=x),\\quad x\\in\\mathcal{X},$$\n",
    "and $Y$ and $X$ are independant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00deffe8",
   "metadata": {},
   "source": [
    "### Property: Alternaty Form:\n",
    "\n",
    "$$\n",
    "H(Y|X) = H(X,Y) - H(X)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33045b2",
   "metadata": {},
   "source": [
    "**Proof:**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H(Y| X) \n",
    "&= \\sum_{x} p(x)\\left( - \\sum_{y} p(y| x)\\log p(y| x)\\right)\\\\\n",
    "&= -\\sum_{x,y} p(y| x)p(x) \\log p(y| x)\\\\\n",
    "&= -\\sum_{x,y} p(x,y)\\log p(y| x)\\\\\n",
    "&= -\\sum_{x,y} p(x,y)\\log \\left(\\frac{p(x,y)}{p(x)}\\right)\\\\\n",
    "&= -\\sum_{x,y} p(x,y)\\left(\\log p(x,y) - \\log p(x)\\right)\\\\\n",
    "&= -\\sum_{x,y} p(x,y)\\log p(x,y) +\\sum_{x,y} p(x,y)\\log p(x)\\\\\n",
    "&= -\\sum_{x,y} p(x,y)\\log p(x,y) +\\sum_{x} p(x)\\log p(x)\\\\\n",
    "&= H(X,Y) - H(X).\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa8310d",
   "metadata": {},
   "source": [
    "### Property: Basic inequality:\n",
    "For two discrete random vectors $X$ and $Y$,\n",
    "$$\n",
    "H(X,Y) \\le H(X) + H(Y)\n",
    "$$\n",
    "with equality if and only if $X$ and $Y$ are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad82755",
   "metadata": {},
   "source": [
    "**Proof:** \n",
    "\n",
    "From the previous identity we have\n",
    "$$\n",
    "H(X,Y) = H(X) + H(Y | X).\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "$$\n",
    "H(X,Y) \\le H(X) + H(Y)\n",
    "\\quad \\Longleftrightarrow \\quad\n",
    "H(Y | X) \\le H(Y).\n",
    "$$\n",
    "\n",
    "Since conditioning cannot increase entropy, and the equality is hold if and only if $X$ and $Y$ are independant, we get the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf8ee8e",
   "metadata": {},
   "source": [
    "## Definition: Mutual Information\n",
    "Mutual information measures how much two random vectors depend on each other. \n",
    "\n",
    "$$\n",
    "I(Y;X) = H(Y) - H(Y|X)\n",
    "$$\n",
    "It quantifies the amount of uncertainty in one random vector that is removed by knowing the other random vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a42c16",
   "metadata": {},
   "source": [
    "### Properties: \n",
    "1. $I(Y;X)\\geq 0$\n",
    "\n",
    "\n",
    "2. $X$ and $Y$ are independent if and only if\n",
    "    $$\n",
    "    I(Y;X) = 0.\n",
    "    $$\n",
    "    Two random vectors are independent if the knowing of one random vector tells us nothing about the other.\n",
    "3. Let $X,Y$ be discrete random vectors with finite alphabets. Then\n",
    "$$\n",
    "I(Y;X) = H(Y)\n",
    "\\quad\\Longleftrightarrow\\quad\n",
    "\\exists \\, f \\text{ such that } Y = f(X) \\text{ almost surely.}\n",
    "$$\n",
    "4. Equivalent form and symetry\n",
    "$$I(Y;X) = H(X) + H(Y) - H(X,Y)=I(X;Y)$$\n",
    "5. KL-divergence form  \n",
    "$$I(Y;X) = \\sum_{x,y} p(x,y)\\log \\frac{p(x,y)}{p(x)p(y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f227deae",
   "metadata": {},
   "source": [
    "#### **Proof (1.) and (2.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3a7f67",
   "metadata": {},
   "source": [
    "This is strightforward from the \"conditionning cannot increase entropy\" property."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44fd622",
   "metadata": {},
   "source": [
    "#### **Proof (3.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613c1001",
   "metadata": {},
   "source": [
    "This is strightforward from the entropy \"functional dependency\" property and the definition of mutual information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9bf346",
   "metadata": {},
   "source": [
    "#### **Proof (4.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7160775",
   "metadata": {},
   "source": [
    "This is strightforward from the alternative from property."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082bf000",
   "metadata": {},
   "source": [
    "#### **Proof (5.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a188381e",
   "metadata": {},
   "source": [
    "Developing the KL divergence form we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_{x,y} p(x,y)\\log \\frac{p(x,y)}{p(x)p(y)}&= \\sum_{x,y} p(x,y)\\left(\\log p(x,y) -\\log p(x) - \\log p(y) \\right),\\\\\n",
    "&= \\sum_{x,y} p(x,y)\\log p(x,y) - \\sum_{x,y} p(x,y)\\log p(x)-  \\sum_{x,y} p(x,y)\\log p(y),\\\\\n",
    "&= - \\sum_{x} p(x)\\log p(x)-  \\sum_{y} p(y)\\log p(y) + \\sum_{x,y} p(x,y)\\log p(x,y) ,\\\\\n",
    "&=H(X) + H(Y) - H(X,Y),\\\\\n",
    "& = H(Y;X)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a97c354",
   "metadata": {},
   "source": [
    "## Definition: Continuous case\n",
    "\n",
    "If $X,Y$ are continuous, sums become integrals:\n",
    "\n",
    "$$\n",
    "I(X;Y) = \\int\\int p(x,y)\\log\\left(\\frac{p(x,y)}{p(x)p(y)}\\right)dxdy\n",
    "$$\n",
    "\n",
    "Now it is not bounded above anymore. Still non-negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c9239",
   "metadata": {},
   "source": [
    "## Definition: Cross-Entropy (Average Surprise Under a Model)\n",
    "\n",
    "Let $\\mathcal{M}_1(\\mathcal{X})$ denote the set of all probability measures on the measurable space $(\\mathcal{X},\\,\\mathcal{P}(\\mathcal{X}))$.\n",
    "\n",
    "We define the **cross-entropy** as the functional\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H : \\mathcal{M}_1(\\mathcal{X})^2 &\\longrightarrow [0,\\infty] \\\\\n",
    "H(p \\Vert q) \n",
    "&:= \\sum_{x \\in \\mathcal{X}} p(x)\\,\\big(-\\log q(x)\\big),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "with the convention $0\\log 0 := 0$.\n",
    "\n",
    "**Remark:**\n",
    "If there exists $x \\in \\mathcal{X}$ such that\n",
    "$$\n",
    "p(x) > 0 \\quad \\text{and} \\quad q(x) = 0,\n",
    "$$\n",
    "then\n",
    "$$\n",
    "H(p \\Vert q) = \\infty.\n",
    "$$\n",
    "This reflects the fact that outcomes that actually occur under $p$ are assigned zero probability under the model $q$.\n",
    "\n",
    "**Interpretation:**\n",
    "* High cross-entropy $H(p\\Vert q)$ means that outcomes generated by the true distribution $p$ are, on average, very hard to predict when using the model $q$.\n",
    "* Low cross-entropy $H(p\\Vert q)$ means that outcomes generated by $p$ are, on average, easy to predict using $q$, indicating that $q$ is close to the true distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439b7363",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
