{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b011cd39",
   "metadata": {},
   "source": [
    "# Cheat-Sheet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79b7439",
   "metadata": {},
   "source": [
    "| Concept | Definition |\n",
    "|--------|------------|\n",
    "| Self-Information | $I(x) = -\\log p(x)$ |\n",
    "| Entropy | $H(X) = -\\sum_{x \\in \\mathcal{X}} p(x)\\log p(x)$ |\n",
    "| Conditional Entropy | $H(Y \\mid X) = -\\sum_{x \\in \\mathcal{X}} p(X)\\sum_{y \\in \\mathcal{Y}} p(y\\mid y)\\log p(x \\mid y)$ |\n",
    "| Mutual Information | $ H(X \\Cap Y) = H(X) - H(X \\mid Y)$ |\n",
    "| KL Divergence | $D_{KL}(p \\,\\\\|\\, q) = \\sum_{x} p(x)\\log \\frac{p(x)}{q(x)}$ |\n",
    "| Cross-Entropy | $H(p \\,\\\\|\\, q) = \\sum_x p(x)(-\\log q(x))$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4acb31",
   "metadata": {},
   "source": [
    "## Intuitive interpretation\n",
    "\n",
    "- Entropy $H(p)$ → uncertainty of a distribution\n",
    "- Mutual information $ H(X \\Cap Y)$ → shared information between variables\n",
    "- KL divergence $D_{KL}(p\\|q)$ → how *wrong* $q$ is as a model for $p$\n",
    "\n",
    "KL divergence measures the expected additional information needed to represent\n",
    "samples from $p$ when assuming the distribution is $q$ instead of $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6305670",
   "metadata": {},
   "source": [
    "| Property | Expression |\n",
    "|--------|------------|\n",
    "| Entropy bounds | $0 \\le H(X) \\le \\log(\\|\\mathcal{X}\\|)$, with equalities iff $X$ is constant or uniform |\n",
    "| Conditional entropy bounds | $0 \\le H(Y \\mid X) \\le H(Y)$ with equalities iff functional dependence / independence |\n",
    "| Mutual information bounds | $0 \\le  H(X \\Cap Y) \\le \\min\\{H(X),H(Y)\\}$ with equalities iff independence /functional dependence|\n",
    "| Alternative form | $H(Y \\mid X) = H(X,Y) - H(X)$ |\n",
    "| Equivalent form | $ H(X,Y) = H(X) + H(Y) -  H(X \\Cap Y)$ |\n",
    "| KL-divergence form | $ H(X \\Cap Y) = D_{KL}\\!\\left( p_{(X,Y)} \\,\\|\\|\\, p_X p_Y \\right)$ |\n",
    "| Main relation | $H(p \\,\\|\\|\\, q) = D_{KL}(p \\,\\|\\|\\, q) + H(p)$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724775a5",
   "metadata": {},
   "source": [
    "**Informal set–measure analogy for information quantities**\n",
    "\n",
    "Let $\\mathcal{X}$ and $\\mathcal{Y}$ denote the “uncertainty regions” of\n",
    "random variables $X$ and $Y$ (as in information diagrams).  \n",
    "Then entropy may be viewed as a measure $\\mu$ on these regions:\n",
    "\n",
    "- Joint entropy:\n",
    "  $$H(X,Y) = \\mu(\\mathcal{X} \\cup \\mathcal{Y})$$\n",
    "\n",
    "- Conditional entropy:\n",
    "  $$H(Y \\mid X) = \\mu(\\mathcal{Y} \\setminus \\mathcal{X})$$\n",
    "\n",
    "- Mutual information:\n",
    "  $$I(X;Y) = \\mu(\\mathcal{X} \\cap \\mathcal{Y})$$\n",
    "\n",
    "This notation captures the Venn-diagram intuition while staying consistent\n",
    "with information theory."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
