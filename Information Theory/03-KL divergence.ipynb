{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b011cd39",
   "metadata": {},
   "source": [
    "# [Kullback–Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c8fd03",
   "metadata": {},
   "source": [
    "## Definition: Kullback–Leibler (KL) Divergence\n",
    "\n",
    "Let $\\mathcal{M}_1(\\mathcal{X})$ denote the set of all probability measures on the\n",
    "measurable space $(\\mathcal{X}, \\mathcal{P}(\\mathcal{X}))$.\n",
    "\n",
    "Define\n",
    "$$\n",
    "\\widehat{\\mathcal{M}_1(\\mathcal{X})^2}\n",
    ":= \\big\\{\\, (p,q)\\in \\mathcal{M}_1(\\mathcal{X}) \\times \\mathcal{M}_1(\\mathcal{X})\n",
    "\\; : \\; p(x) > 0 \\Rightarrow q(x) > 0,\\ \\forall x \\in \\mathcal{X} \\,\\big\\}.\n",
    "$$\n",
    "\n",
    "We define the **Kullback–Leibler (KL) divergence** as the function\n",
    "$$\n",
    "\\begin{aligned}\n",
    "D_{KL} : \\widehat{\\mathcal{M}_1(\\mathcal{X})^2} &\\longrightarrow [0,\\infty) \\\\\n",
    "D_{KL}(p \\Vert q) \n",
    "&:= \\sum_{x \\in \\mathcal{X}} \n",
    "p(x)\\,\\log\\!\\left(\\frac{p(x)}{q(x)}\\right),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "with the convention $0 \\log 0 := 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacd2a82",
   "metadata": {},
   "source": [
    "## Basic properties\n",
    "\n",
    "1. **Non-negativity (Gibbs' inequality)**\n",
    "\n",
    "$$\n",
    "D_{KL}(p \\,\\|\\, q) \\ge 0\n",
    "$$\n",
    "\n",
    "with equality if and only if $p(x) = q(x)$ for all $x\\in\\mathcal{X}$ (i.e. $p = q$).\n",
    "\n",
    "2. **Not symmetric and no triangle inequality**\n",
    "\n",
    "In general,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "D_{KL}(p \\,\\|\\, q) &\\neq D_{KL}(q \\,\\|\\, p),\\\\\n",
    "D_{KL}(p \\,\\|\\, r) &\\not\\le D_{KL}(p \\,\\|\\, q)+ D_{KL}(q \\,\\|\\, r)\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Therefore, KL divergence is **not a metric** (it does not define a distance in the mathematical sense).\n",
    "\n",
    "3. **Not bounded**\n",
    "\n",
    "KL divergence can take arbitrarily large values and may be $+\\infty$ if\n",
    "there exists $x$ such that\n",
    "\n",
    "$$\n",
    "p(x) > 0 \\quad \\text{and} \\quad q(x) = 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196fdb55",
   "metadata": {},
   "source": [
    "#### **Proof (1.):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6aa2c3",
   "metadata": {},
   "source": [
    "\n",
    "We use the elementary inequality\n",
    "$$\n",
    "\\log t \\le t - 1 \\quad \\text{for all } t > 0,\n",
    "$$\n",
    "with equality if and only if $t = 1$.\n",
    "\n",
    "This is equivalent to\n",
    "$$\n",
    "-\\log t \\ge 1 - t.\n",
    "$$\n",
    "Then we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "D_{KL}(p \\,\\|\\, q)\n",
    "&= \\sum_{x \\in \\mathcal{X}} p(x)\\,\\log\\!\\left( \\frac{p(x)}{q(x)} \\right),\\\\\n",
    "&= \\sum_{x \\in \\mathcal{X}} p(x)\\left[-\\log\\!\\left( \\frac{p(x)}{p(x)} \\right)\\right],\\\\\n",
    "&\\ge \\sum_{x \\in \\mathcal{X}} p(x)\\left(1- \\frac{q(x)}{p(x)} \\right),\\\\\n",
    "&= \\sum_{x \\in \\mathcal{X}} \\left(p(x)-q(x) \\right),\\\\\n",
    "&= \\sum_{x \\in \\mathcal{X}} p(x)-\\sum_{x \\in \\mathcal{X}}q(x),\\\\\n",
    "&= 1 -1,\\\\\n",
    "&= 0.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6c2e35",
   "metadata": {},
   "source": [
    "For the equality condition we have\n",
    "\n",
    "Equality holds if and only if\n",
    "$$\n",
    "-\\log\\frac{q(x)}{p(x)} = 1 - \\frac{q(x)}{p(x)}\n",
    "$$\n",
    "for all $x$ with $p(x) > 0$,\n",
    "which happens if and only if\n",
    "$$\n",
    "\\frac{q(x)}{p(x)} = 1\n",
    "\\quad \\Longleftrightarrow \\quad\n",
    "p(x) = q(x)\n",
    "$$\n",
    "for all $x$ with $p(x)>0$.\n",
    "\n",
    "Since both are probability distributions, this implies\n",
    "$$\n",
    "p(x) = q(x) \\quad \\text{for all } x \\in \\mathcal{X}.\n",
    "$$\n",
    "\n",
    "Conversely, if $p=q$, then\n",
    "$$\n",
    "D_{KL}(p\\|q) = \\sum_x P(x)\\log 1 = 0.\n",
    "$$\n",
    "\n",
    "Thus\n",
    "$$\n",
    "D_{KL}(p\\|q) \\ge 0,\n",
    "$$\n",
    "with equality if and only if $p = q$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ff006f",
   "metadata": {},
   "source": [
    "## Property: Relation with entropy and cross-entropy\n",
    "\n",
    "A key identity connects cross-entropy, entropy, and KL-divergence:\n",
    "\n",
    "$$\n",
    "H(p \\Vert q) \n",
    "=  D_{KL}(p \\,\\|\\, q)+ H(p).\n",
    "$$\n",
    "\n",
    "This decomposition shows that cross-entropy contains **two distinct effects**:\n",
    "\n",
    "1. An intrinsic part: the entropy of the true distribution $H(p)$\n",
    "2. A mismatch part: the divergence between the model and the truth $D_{KL}(p\\Vert q)$\n",
    "\n",
    "Interpretation\n",
    "\n",
    "* High cross-entropy $H(p \\Vert q)$ means that outcomes generated by the true distribution $p$ are (on average) highly unpredictable — either because  \n",
    "  - $p$ itself is highly random (high entropy) $H(p)$, or  \n",
    "  - the model $q$ is very different from the true distribution $p$, or  \n",
    "  - both.\n",
    "\n",
    "* Low cross-entropy $H(p \\Vert q)$ means that outcomes generated by $p$ are (on average) quite predictable using the model $q$, which happens when  \n",
    "  - $p$ is structured (low entropy) and  \n",
    "  - $q$ is close to $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f2c614",
   "metadata": {},
   "source": [
    "#### **Proof:**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H(p \\Vert q) \n",
    "&:= -\\sum_{x \\in \\mathcal{X}} p(x)\\log q(x),\\\\\n",
    "&= \\sum_{x \\in \\mathcal{X}} p(x)\\log \\left(\\frac{1}{q(x)}\\right)-H(p)+H(p),\\\\\n",
    "&= \\sum_{x \\in \\mathcal{X}} p(x)\\log \\left(\\frac{1}{q(x)}\\right)+\\sum_{x \\in \\mathcal{X}} p(x)\\log \\left(p(x)\\right)+H(p),\\\\\n",
    "&= \\sum_{x \\in \\mathcal{X}} p(x)\\log \\left(\\frac{p(x)}{q(x)}\\right)+H(p),\\\\\n",
    "&=  D_{KL}(p \\,\\|\\, q)+ H(p).\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764c45db",
   "metadata": {},
   "source": [
    "## Property: Relation with mutual information\n",
    "\n",
    "Then mutual information is a special case of KL divergence:\n",
    "\n",
    "$$\n",
    "I(X;Y)\n",
    "= D_{KL}\\!\\left( \\mathbb{P}_{(X,Y)} \\, \\| \\, \\mathbb{P}_X\\,\\mathbb{P}_Y \\right).\n",
    "$$\n",
    "\n",
    "This shows:\n",
    "\n",
    "- Mutual information measures how far the joint distribution is\n",
    "  from the distribution under independence\n",
    "- Hence it quantifies **statistical dependence**\n",
    "- And explains why\n",
    "$$\n",
    "I(X;Y) \\ge 0\n",
    "$$\n",
    "with equality if and only if $X$ and $Y$ are independent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4c3ce4",
   "metadata": {},
   "source": [
    "#### **Proof:** \n",
    "From the KL-divergence form property of the mutual informaiton we have \n",
    "$$I(Y;X) = \\sum_{x,y} p(x,y)\\log \\frac{p(x,y)}{p(x)p(y)}=D_{KL}\\!\\left( \\mathbb{P}_{(X,Y)} \\, \\| \\, \\mathbb{P}_X\\,\\mathbb{P}_Y \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4acb31",
   "metadata": {},
   "source": [
    "## Intuitive interpretation\n",
    "\n",
    "- Entropy $H(p)$ → uncertainty of a distribution\n",
    "- Mutual information $I(X;Y)$ → shared information between variables\n",
    "- KL divergence $D_{KL}(p\\|q)$ → how *wrong* $q$ is as a model for $p$\n",
    "\n",
    "KL divergence measures the expected additional information needed to represent\n",
    "samples from $p$ when assuming the distribution is $q$ instead of $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79b7439",
   "metadata": {},
   "source": [
    "| Concept | Definition |\n",
    "|--------|------------|\n",
    "| Self-Information | $I(x) = -\\log p(x)$ |\n",
    "| Entropy | $H(X) = -\\sum_{x \\in \\mathcal{X}} p(x)\\log p(x)$ |\n",
    "| Joint Entropy | $H(X,Y) = -\\sum_{x \\in \\mathcal{X}}\\sum_{y \\in \\mathcal{Y}} p(x,y)\\log p(x,y)$ |\n",
    "| Conditional Entropy | $H(X \\mid Y) = -\\sum_{y \\in \\mathcal{Y}} p(y)\\sum_{x \\in \\mathcal{X}} p(x \\mid y)\\log p(x \\mid y)$ |\n",
    "| Alternative form | $H(X \\mid Y) = H(X,Y) - H(Y)$ |\n",
    "| Mutual Information | $I(X;Y) = H(X) - H(X \\mid Y)$ |\n",
    "| Symmetry | $I(X;Y) = H(Y) - H(Y \\mid X)$ |\n",
    "| Equivalent form | $I(X;Y) = H(X) + H(Y) - H(X,Y)$ |\n",
    "| KL-divergence form | $I(X;Y) = \\sum_{x,y} p(x,y)\\log \\frac{p(x,y)}{p(x)p(y)}$ |\n",
    "| KL divergence | $D_{KL}(p\\|q) = \\sum_{x} p(x)\\log \\frac{p(x)}{q(x)}$ |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
