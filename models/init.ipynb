{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "549576a1",
   "metadata": {},
   "source": [
    "# Weight Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f2dca",
   "metadata": {},
   "source": [
    "## Variance Propagation in the Forward Pass\n",
    "Given a layer $y = Wx$ followed by an activation (non-linear function), what variance should the entries of $W$ have so that activations don’t blow up or vanish as we stack layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b80d835",
   "metadata": {},
   "source": [
    "We model one neuron in a fully connected layer as\n",
    "\n",
    "$$y = \\sum_{i=1}^{n} w_i x_i$$\n",
    "\n",
    "where\n",
    "* $n = \\text{fan\\_in}$,\n",
    "* $x_i$ are i.i.d., $\\mathbb{E}[x_i] = 0, \\operatorname{Var}(x_i) = v_x$,\n",
    "* $w_i$ are i.i.d., $\\mathbb{E}[w_i] = 0, \\operatorname{Var}(w_i) = v_w$,\n",
    "* $w_i$ independent of $x_i$.\n",
    "\n",
    "By independence and zero mean assumptions,\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(y)\n",
    "= \\operatorname{Var}\\Big(\\sum_{i=1}^n w_i x_i\\Big)\n",
    "= \\sum_{i=1}^n \\operatorname{Var}(w_i x_i)\n",
    "= \\sum_{i=1}^n \\operatorname{Var}(w_i)\\operatorname{Var}(x_i)\n",
    "= n \\, v_w \\, v_x.\n",
    "$$\n",
    "\n",
    "So\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(y) = \\text{fan\\_in} \\cdot v_w \\cdot v_x\n",
    "$$\n",
    "\n",
    "If we want $\\operatorname{Var}(y) = v_x$ we need $v_w =\\frac{1}{\\text{fan\\_in}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcab392",
   "metadata": {},
   "source": [
    "\n",
    "If we draw uniformly $w_i\\sim \\mathcal{U}(-b,b)$, then \n",
    "\n",
    "$$v_w=\\operatorname{Var}(w_i)=\\int_{-b}^bx^2 \\frac{1}{2b}dx=\\frac{b^2}{3}.$$\n",
    "\n",
    "Then $$b=\\sqrt{3v_w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb994f3",
   "metadata": {},
   "source": [
    "## Variance Propagation in the Backward Pass (Gradients)\n",
    "\n",
    "Given a layer $y = W x$, how should the variance of the weight entries $W$ be chosen so that the gradients $\\frac{\\partial L}{\\partial x}$ do not explode or vanish during backpropagation through layers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937d116a",
   "metadata": {},
   "source": [
    "We model the output of a fully connected layer as\n",
    "\n",
    "$$y_j = \\sum_{i=1}^{n} w_{j,i} x_i, \\quad j=1,2,\\dots,m$$\n",
    "\n",
    "where\n",
    "* $n = \\text{fan\\_in}$,\n",
    "* $m = \\text{fan\\_out}$,\n",
    "\n",
    "Denote by $L$ the loss function of the neural network, we assume:\n",
    "* $\\frac{\\partial L}{\\partial y_j}$ are i.i.d., $\\mathbb{E}[\\frac{\\partial L}{\\partial y_j}] = 0, \\operatorname{Var}(\\frac{\\partial L}{\\partial y_j}) = v_{\\partial y}$,\n",
    "* $w_{j,i}$ are i.i.d., $\\mathbb{E}[w_{j,i}] = 0, \\operatorname{Var}(w_{j,i}) = v_w$,\n",
    "* $w_{j,i}$ independent of $\\frac{\\partial L}{\\partial y_j}$.\n",
    "\n",
    "**Note:**  $\\frac{\\partial L}{\\partial y}$ are not independent because $\\frac{\\partial L}{\\partial y}$ depends on $w$ through higher layers. Nevertheless, we approximate them as independent to make the variance analysis tractable. This is standard in both the Glorot & Bengio (2010) and He et al. (2015) derivations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bffef5",
   "metadata": {},
   "source": [
    "Then, consider gradient backpropagation through this layer for the variable $x_\\ell$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial x_\\ell} & =\\sum_{j=1}^{m} \\frac{\\partial L}{\\partial y_j}\\frac{\\partial y_j}{\\partial x_\\ell}=\\sum_{j=1}^{m} w_{j,\\ell} \\frac{\\partial L}{\\partial y_j}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "By independence and zero mean assumptions,\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\operatorname{Var}\\left(\\frac{\\partial L}{\\partial x_\\ell}\\right)\n",
    "&= \\operatorname{Var}\\left(\\sum_{j=1}^{m} w_{j,\\ell} \\frac{\\partial L}{\\partial y_j}\\right),\\\\\n",
    "&= \\sum_{j=1}^{m} \\operatorname{Var}\\left(w_{j,\\ell} \\frac{\\partial L}{\\partial y_j}\\right),\\\\\n",
    "&= \\sum_{j=1}^{m} \\operatorname{Var}\\left(w_{j,\\ell}\\right)\\operatorname{Var}\\left( \\frac{\\partial L}{\\partial y_j}\\right),\\\\\n",
    "&= m \\, v_w \\, v_{\\partial y}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e638d0",
   "metadata": {},
   "source": [
    "So\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}\\left(\\frac{\\partial L}{\\partial x_\\ell}\\right) = \\text{fan\\_out} \\cdot v_w \\cdot v_{\\partial y}\n",
    "$$\n",
    "\n",
    "So, if we want $\\operatorname{Var}\\left(\\frac{\\partial L}{\\partial x_\\ell}\\right) = v_{\\partial y}$ we need $v_w =\\frac{1}{\\text{fan\\_out}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e5a57f",
   "metadata": {},
   "source": [
    "## Xavier/Glorot Initialization\n",
    "\n",
    "It’s impossible to perfectly satisfy both forward and backward pass values for $v_w$. So Xavier/Glorot Initialization makes compromise by averaging both\n",
    "$$v_w=\\frac{2}{\\text{fan\\_in}+\\text{fan\\_out}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c156cd8b",
   "metadata": {},
   "source": [
    "If we draw uniformly, from the prevous derivation, $w_{j,i}\\sim \\mathcal{U}(-b,b)$, then \n",
    "$$b=\\sqrt{3v_w}=\\sqrt{\\frac{6}{\\text{fan\\_in}+\\text{fan\\_out}}}$$\n",
    "\n",
    "If we draw normally $w_{j,i} \\sim \\mathcal{N}\\left(0,\\frac{2}{\\text{fan\\_in}+\\text{fan\\_out}}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10192f5",
   "metadata": {},
   "source": [
    "## Kaiming (He)\n",
    "\n",
    "For convinience, we define the gain such that $b=\\text{gain}\\sqrt{\\frac{3}{\\text{fan\\_in}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebab849",
   "metadata": {},
   "source": [
    "## LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe2c977",
   "metadata": {},
   "source": [
    "We assume:\n",
    "* $y$ is symetric: $P(y \\in A)=P(y \\in -A)$, for any borel real set $A \\in \\mathbb{R}$, \n",
    "and\n",
    "$$\n",
    "\\begin{align*}\n",
    "-A&:=\\{-x: x \\in A\\}.\n",
    "\\end{align*}\n",
    "$$\n",
    "* $p(y = 0) = 0$.\n",
    "\n",
    "\n",
    "Let $z = \\operatorname{LeakyReLU}(y)$. We want \n",
    "$$\\operatorname{Var}(z) = \\operatorname{Var}(\\operatorname{LeakyReLU}(y))= \\operatorname{Var}(y\\mathbf{1}_{\\{y>0\\}}+\\alpha y\\mathbf{1}_{\\{y\\leq 0\\}})   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66a3524",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\operatorname{Var}(z)&= \\mathbb{E}[z^2]- \\mathbb{E}[z]^2\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4f35ba",
   "metadata": {},
   "source": [
    "From the assumptions on $y$, we obtain\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[y^2\\mathbf{1}_{\\{y>0\\}}]&=\\int_{(0,\\infty)} \\tau^2dp_y(\\tau),\\\\\n",
    "&=\\frac{1}{2}\\left(\\int_{(0,\\infty)} \\tau^2dp_y(\\tau)+\\int_{(0,\\infty)} \\tau^2dp_y(-\\tau)\\right),\\\\\n",
    "&=\\frac{1}{2}\\left(\\int_{(0,\\infty)} \\tau^2dp_y(\\tau)+\\int_{(-\\infty,0)} (-\\tau)^2dp_y(\\tau)\\right),\\\\\n",
    "&=\\frac{1}{2}\\left(\\int_{-\\infty}^\\infty \\tau^2dp_y(\\tau)\\right),\\\\\n",
    "&=\\frac{1}{2}\\mathbb{E}[y^2],\\\\\n",
    "&=\\frac{1}{2}\\operatorname{Var}(y).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Notice that from the third equality we also have \n",
    "$$\\mathbb{E}[y^2\\mathbf{1}_{\\{y>0\\}}]=\\mathbb{E}[y^2\\mathbf{1}_{\\{y\\leq0\\}}]=\\frac{1}{2}\\operatorname{Var}(y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbcc0e9",
   "metadata": {},
   "source": [
    "Then we can deduce\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[z^2]&=\\mathbb{E}[y^2\\mathbf{1}_{\\{y>0\\}}+\\alpha^2 y^2\\mathbf{1}_{\\{y\\leq 0\\}}],\\\\\n",
    "&=\\mathbb{E}[y^2\\mathbf{1}_{\\{y>0\\}}]+\\alpha^2\\mathbb{E}[ y^2\\mathbf{1}_{\\{y\\leq 0\\}}],\\\\\n",
    "&=\\frac{1}{2}\\operatorname{Var}(y)+\\alpha^2\\frac{1}{2}\\operatorname{Var}(y),\\\\\n",
    "&=\\frac{1+\\alpha^2}{2}\\operatorname{Var}(y)\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfd5053",
   "metadata": {},
   "source": [
    "We simply ignore the second term $\\mathbb{E}[z]^2$, in practice this terms is sually neglectable. Theoretically espeaking we have very weak justifications, for example in the $\\text{ReLU}$ case $\\alpha=0$ we have:\n",
    "$$\n",
    "\\begin{align*}\n",
    "(\\mathbb{E}[z])^2 &= \\big(\\mathbb{E}[y \\mathbf{1}_{\\{y>0\\}}]\\big)^2,\\\\\n",
    "&\\le \\mathbb{E}[y^2] \\cdot \\mathbb{E}[\\mathbf{1}^2_{\\{y>0\\}}],\\\\\n",
    "&= \\mathbb{E}[y^2] \\cdot \\mathbb{E}[\\mathbf{1}_{\\{y>0\\}}],\\\\\n",
    "&= \\frac{1}{2} \\operatorname{Var}(y) \\cdot \\frac{1}{2}\\left(1 + p(y =0)\\right),\\\\\n",
    "&= \\frac{1}{4} \\operatorname{Var}(y),\n",
    "\\end{align*}\n",
    "$$\n",
    "where the inequality directly comes from Cauchy–Schwarz inequality and step 4 comes from the symetry of $y$ as follow\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[\\mathbf{1}_{\\{y>0\\}}]&=\\int_0^\\infty dp_y(\\tau)=p(y \\in [0,\\infty)),\\\\\n",
    "&=\\frac{1}{2}\\left(p(y \\in (-\\infty,0]) + p(y \\in [0,\\infty))\\right),\\\\\n",
    "&=\\frac{1}{2}\\left(1 + p(y =0)\\right).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Then we have the inequality\n",
    "$$\n",
    "\\frac14 \\operatorname{Var}(Y) \\;\\le\\; \\operatorname{Var}(\\text{ReLU}(Y)) \\;\\le\\; \\frac12 \\operatorname{Var}(Y)\n",
    "$$\n",
    "\n",
    "The Kaiming initialization takes the approximation \n",
    "$$\\operatorname{Var}[\\max(0,y)]\\approx \\frac12\\operatorname{Var}(y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf86121f",
   "metadata": {},
   "source": [
    "So we conclude\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\operatorname{Var}[z]\\approx \\frac{1+\\alpha^2}{2}\\operatorname{Var}(y) =\\frac{1+\\alpha^2}{2}\\text{fan\\_in} \\cdot v_w \\cdot v_x\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Then, to obtain $\\operatorname{Var}[z]\\approx v_x$ we need \n",
    "$v_w=\\frac{2}{(1+\\alpha^2)\\text{fan\\_in}}$. Then the value of the bound $b$ is\n",
    "\n",
    "$$b=\\sqrt{3v_w}=\\sqrt{\\frac{6}{(1+\\alpha^2)\\text{fan\\_in}}}$$\n",
    "\n",
    "**Note:** In this case we have $\\text{gain} = \\sqrt{\\frac{2}{1+\\alpha^2}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de18cfae",
   "metadata": {},
   "source": [
    "### Full Gain table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229c5846",
   "metadata": {},
   "source": [
    "| Nonlinearity / Init | Gain / Formula | Origin | Type | Notes |\n",
    "|----------------------|----------------|---------|------|--------|\n",
    "| **relu** | √2 | He et al. (2015) | ✅ Theoretical | Derived for rectifiers; compensates for halved variance. |\n",
    "| **leaky_relu(a)** | √(2 / (1 + a²)) | He et al. (2015), generalized | ✅ Theoretical | Generalization of He for nonzero negative slope. |\n",
    "| **tanh** | 5/3 | Torch/PyTorch convention | ⚠️ Heuristic | Empirically tuned for stable gradients; no closed form. |\n",
    "| **sigmoid** | 1.0 | Glorot & Bengio (2010) | ⚠️ Heuristic | Safe default; avoids saturation by keeping small weights. |\n",
    "| **linear / conv\\*** | 1.0 | Identity mapping | ✅ Trivial | No nonlinearity → no scaling required. |\n",
    "| **elu(α)** | ≈√2 | Clevert et al. (2015) | ⚠️ Heuristic | Negative branch saturates; ReLU-like behavior empirically fits √2. |\n",
    "| **selu** | LeCun Normal (σ² = 1 / fan_in) | Klambauer et al. (2017) | ✅ Theoretical | λ, α constants chosen for mean=0, var=1 fixed point; self-normalizing. |\n",
    "| **gelu** | ≈√2 | Hendrycks & Gimpel (2016) | ⚠️ Empirical | Variance ≈ 0.425 → gain ≈ 1.54 ≈ √2; smooth ReLU-like. |\n",
    "| **xavier / glorot** | σ² = 2 / (fan_in + fan_out) | Glorot & Bengio (2010) | ✅ Theoretical | Balances forward & backward variance for tanh/sigmoid. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "52458b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4997628",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "433a738d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.dim()=3\n",
      "tensor.size()=torch.Size([1, 2, 4])\n",
      "tensor.size(1)=2\n",
      "tensor.numel()=8\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([[[1,2,3,4],[4,5,6,2]]],dtype=torch.float32)\n",
    "print(f'{tensor.dim()=}')\n",
    "print(f'{tensor.size()=}')\n",
    "print(f'{tensor.size(1)=}')\n",
    "print(f'{tensor.numel()=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "beb82762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fan_in_and_fan_out(tensor: torch.Tensor):\n",
    "    dimensions = tensor.dim()\n",
    "    if dimensions < 2:\n",
    "        raise ValueError(\"Fan in/out can not be computed for tensor with fewer than 2 dimensions\")\n",
    "\n",
    "    num_input_fmaps = tensor.size(1)\n",
    "    num_output_fmaps = tensor.size(0)\n",
    "    receptive_field_size = 1\n",
    "    if tensor.dim() > 2:\n",
    "        # convolutional weights e.g. [out_channels, in_channels, kH, kW, ...]\n",
    "        receptive_field_size = tensor[0][0].numel()\n",
    "    fan_in = num_input_fmaps * receptive_field_size\n",
    "    fan_out = num_output_fmaps * receptive_field_size\n",
    "    return fan_in, fan_out\n",
    "\n",
    "def calculate_gain(nonlinearity: str, a: float = 0.0):\n",
    "    nonlinearity = nonlinearity.lower()\n",
    "    match nonlinearity:\n",
    "        case 'sigmoid'|'linear' | 'conv1d' | 'conv2d' | 'conv3d':\n",
    "            return 1.0\n",
    "        case 'tanh':\n",
    "            return 5.0/3  # example value\n",
    "        case 'relu':\n",
    "            return math.sqrt(2.0)\n",
    "        case 'leaky_relu':\n",
    "            return math.sqrt(2.0 / (1 + a*a))\n",
    "        case _:\n",
    "            raise ValueError(f\"Unsupported nonlinearity {nonlinearity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2d4c2bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_uniform_(tensor: torch.Tensor,\n",
    "                    gain: float = 1.0):\n",
    "    \"\"\"\n",
    "    Fills the input `tensor` with values drawn from U(-bound, bound)\n",
    "    according to Xavier/Glorot uniform initialization.\n",
    "\n",
    "    Reference:\n",
    "    Glorot & Bengio (2010), \"Understanding the difficulty of training deep feedforward neural networks\".\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Compute fan_in and fan_out\n",
    "    fan_in, fan_out = calculate_fan_in_and_fan_out(tensor)\n",
    "\n",
    "    # 2) Compute standard deviation and uniform bound\n",
    "    bound = gain * math.sqrt(6/(fan_in + fan_out))\n",
    "\n",
    "    # 3) Fill tensor\n",
    "    with torch.no_grad():\n",
    "        return tensor.uniform_(-bound, bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b4e0ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_normal_(tensor: torch.Tensor,\n",
    "                   gain: float = 1.0):\n",
    "    \"\"\"\n",
    "    Fills the input `tensor` with values drawn from N(0, std^2)\n",
    "    according to Xavier/Glorot normal initialization.\n",
    "\n",
    "    Reference:\n",
    "    Glorot & Bengio (2010), \"Understanding the difficulty of training deep feedforward neural networks\".\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Compute fan_in and fan_out\n",
    "    fan_in, fan_out = calculate_fan_in_and_fan_out(tensor)\n",
    "\n",
    "    # 2) Compute standard deviation\n",
    "    std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "\n",
    "    # 3) Fill tensor\n",
    "    with torch.no_grad():\n",
    "        return tensor.normal_(0.0, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "426cd102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_uniform_(tensor: torch.Tensor,\n",
    "                     a: float = 0.0,\n",
    "                     mode: str = 'fan_in',\n",
    "                     nonlinearity: str = 'leaky_relu'):\n",
    "    \"\"\"\n",
    "    Fills the input `tensor` with values drawn from U(-bound, bound)\n",
    "    according to Kaiming/He initialization.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Compute fan_in and fan_out\n",
    "    fan_in, fan_out = calculate_fan_in_and_fan_out(tensor)\n",
    "\n",
    "    # 2) Choose correct fan\n",
    "    if mode == 'fan_in':\n",
    "        fan = fan_in\n",
    "    elif mode == 'fan_out':\n",
    "        fan = fan_out\n",
    "    else:\n",
    "        raise ValueError(\"mode should be 'fan_in' or 'fan_out'\")\n",
    "\n",
    "    # 3) Compute gain and bound\n",
    "    gain = calculate_gain(nonlinearity, a)  \n",
    "    bound = math.sqrt(3.0/fan) * gain\n",
    "\n",
    "    # 5) Fill tensor\n",
    "    with torch.no_grad():\n",
    "        return tensor.uniform_(-bound, bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf0f436",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "deb55a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "gain = 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b6c914",
   "metadata": {},
   "source": [
    "#### Xavier Uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8c873957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After init (nn): tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "After init (custom): tensor([[-0.0195,  1.3937, -2.1383, -1.9120, -1.0007],\n",
      "        [ 0.6967, -0.0515,  2.0600, -0.2306,  0.6875],\n",
      "        [-0.7852, -0.5107, -2.4821, -1.7207, -1.0710]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.empty(3, 5)\n",
    "nn_tensor = torch.empty(3, 5)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "nn.init.xavier_uniform_(tensor=tensor, gain=gain)\n",
    "print(f'After init (nn): {nn_tensor}')\n",
    "\n",
    "torch.manual_seed(0)\n",
    "xavier_uniform_(tensor=tensor, gain=gain)\n",
    "print(f'After init (custom): {tensor}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c916ff5",
   "metadata": {},
   "source": [
    "#### Xavier Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2e6cf126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After init (nn): tensor([[ 2.3115, -0.4401, -3.2682,  0.8526, -1.6268],\n",
      "        [-2.0979,  0.6050,  1.2570, -1.0789, -0.6050],\n",
      "        [-0.8950,  0.2731, -1.2850,  1.6509, -1.6068]])\n",
      "After init (custom): tensor([[ 2.3115, -0.4401, -3.2682,  0.8526, -1.6268],\n",
      "        [-2.0979,  0.6050,  1.2570, -1.0789, -0.6050],\n",
      "        [-0.8950,  0.2731, -1.2850,  1.6509, -1.6068]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.empty(3, 5)\n",
    "nn_tensor = torch.empty(3, 5)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "nn.init.xavier_normal_(tensor=nn_tensor, gain=gain)\n",
    "print(f'After init (nn): {nn_tensor}')\n",
    "\n",
    "torch.manual_seed(0)\n",
    "xavier_normal_(tensor=tensor, gain=gain)\n",
    "print(f'After init (custom): {tensor}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56612024",
   "metadata": {},
   "source": [
    "#### Kaiming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "85de2ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinearity='sigmoid'\n",
    "a=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b06bdc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After init (nn): tensor([[-0.0075,  0.5364, -0.8230, -0.7359, -0.3852],\n",
      "        [ 0.2682, -0.0198,  0.7929, -0.0887,  0.2646],\n",
      "        [-0.3022, -0.1966, -0.9553, -0.6623, -0.4122]])\n",
      "After init (custom): tensor([[-0.0075,  0.5364, -0.8230, -0.7359, -0.3852],\n",
      "        [ 0.2682, -0.0198,  0.7929, -0.0887,  0.2646],\n",
      "        [-0.3022, -0.1966, -0.9553, -0.6623, -0.4122]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.empty(3, 5)\n",
    "nn_tensor = torch.empty(3, 5)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "nn.init.kaiming_uniform_(tensor=nn_tensor, a=a, mode='fan_out', nonlinearity=nonlinearity)\n",
    "print(f'After init (nn): {nn_tensor}')\n",
    "\n",
    "torch.manual_seed(0)\n",
    "kaiming_uniform_(tensor=tensor, a=a, mode='fan_out', nonlinearity=nonlinearity)\n",
    "print(f'After init (custom): {tensor}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
