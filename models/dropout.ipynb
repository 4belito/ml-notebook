{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bddc880",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "Dropout is a regularization technique used to prevent overfitting in neural networks.\n",
    "\n",
    "During training, dropout randomly “drops out” (sets to zero) a fraction of the neuron activations in a layer.\n",
    "This forces the network to not rely too heavily on any single neuron. It encourages redundancy and robustness in the learned features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeca6de9",
   "metadata": {},
   "source": [
    "## Definition: Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a500411e",
   "metadata": {},
   "source": [
    "Input\n",
    "* $x \\in \\mathbb{R}^{d_{in}}$ \n",
    "\n",
    "Parameter:\n",
    "* drop probability $p$\n",
    "\n",
    "Output\n",
    "* $\\text{Dropout}_{p}(x)=x$\n",
    "\n",
    "Training Output:\n",
    "* $\\text{TrainDropout}_{p}(x) = \\frac{1}{1-p}m*x$ \n",
    "\n",
    "where $m$ is a $d_{in}$-vector of i.i.d Bernoulli random variables \n",
    "$$m_i\\sim \\operatorname{Bernoulli}(1-p), \\quad i=1,2,\\dots,d_{in}.$$\n",
    "called mask.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe8e2ee",
   "metadata": {},
   "source": [
    "### Property: \n",
    "$$\\mathbb{E}[\\operatorname{TrainDropout}_p(x)]=\\operatorname{Id}(x)=\\operatorname{Dropout}_p(x).$$\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef3a8ff",
   "metadata": {},
   "source": [
    "Proof: Let's take $i=1,2,\\dots,d_{in}$:\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\mathbb{E}[\\operatorname{TrainDropout}_p(x)]_i&=\\mathbb{E}\\left[\\frac{1}{1-p}m_ix_i\\right],\\\\\n",
    "&=\\frac{1}{1-p}\\mathbb{E}\\left[m_i\\right]x_i,\\\\\n",
    "&=\\frac{1}{1-p}(p \\cdot 0+ 1 \\cdot (1-p))x_i,\\\\\n",
    "&=x_i.\n",
    "\\end{align*}\n",
    "$$\n",
    "Then $\\mathbb{E}[\\operatorname{TrainDropout}_p(x)]=x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3a2a89",
   "metadata": {},
   "source": [
    "**Note:** Dropout can be interpreted as training an implicit ensemble of many smaller subnetworks, each defined by a dropout mask $k\\in\\{0,1\\}^{d_{in}}$, where $k_i=1$ means the $i$-th neuron is kept and $k_i=0$ means the $i$-th neuron is dropped. These $k$-subnetworks share parameters and are not trained independently and not for the same amount of epoch s either. \n",
    "The probability of the $k$-subnetwork is trained in an epoach, is\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(m=k)&=\\prod_{i=1}^{d_{in}}(1-p)^{k_i}p^{1-k_i}=\\left(1-p\\right)^{\\sum_{i=1}^{d_{in}}k_i}p^{d_{in}-\\sum_{i=1}^{d_{in}}k_i},\\\\\n",
    "&=\\left(1-p\\right)^{n_{\\text{kept}}(k)}p^{d_{in}-n_{\\text{kept}}(k)},\\\\\n",
    "&=p^{d_{in}}\\left(\\frac{1}{p}-1\\right)^{n_{\\text{kept}}(k)},\n",
    "\\end{align*}\n",
    "$$\n",
    "where $n_{\\text{kept}}(k)=\\sum_{i=1}^{d_{in}}k_i$ is the number of neuron kept in the mask $k$.\n",
    "Then, the number of epoach the $k$-subnetwork is trained is:\n",
    "$$n(k)\\sim \\text{Bin}\\left(N,P(m=k)\\right)$$\n",
    "where $N$ is the total number of training epoachs.\n",
    "Then, the expected value of the number of epoachs the $k$-subnetwork is trained is: \n",
    "$$\\mathbb{E}[n(k)] = P(m=k)N=p^{d_{in}}\\left(\\frac{1}{p}-1\\right)^{n_{\\text{kept}}(k)}N.$$\n",
    "Notice that, depending on the values of the dropping probabilty $p$, we have\n",
    "$$p\\begin{cases}\n",
    "<\\frac{1}{2},& \\mathbb{E}[n(k)] \\text{ increases with } n_{\\text{kept}}(k),\\\\\n",
    "=\\frac{1}{2},& \\mathbb{E}[n(k)]=p^{d_{in}} \\text{ for all } k,\\\\\n",
    ">\\frac{1}{2},& \\mathbb{E}[n(k)] \\text{ decreases with } n_{\\text{kept}}(k).\n",
    "\\end{cases}\n",
    "$$\n",
    "Therefore, dropout can be seen as a weighted average trainning of a ensamble  of subnetworks, that, when $p<\\frac{1}{2}$ as usual, weighted more (train during more epoachs) the network masks that have more actvie neurons.\n",
    "\n",
    "In practice, no single configuration is visited many times for large $d_{\\text{in}}$ (for $M$ dropout layers we have $2^{\\sum_{i=1}^Md_{in,i}}$ subnetworks), instead, the expectation defines the effective contribution to training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38cda77",
   "metadata": {},
   "source": [
    "## Code: Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b687865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "6993bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(nn.Module):\n",
    "    def __init__(self, p: float = 0.5):\n",
    "        \"\"\"\n",
    "        p: drop probability (0 <= p < 1)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert 0.0 <= p < 1.0, \"p must be in [0, 1)\"\n",
    "        self.p = p\n",
    "        self.keep_prob = 1.0 - self.p\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # If we are in eval mode, dropout does nothing\n",
    "        if not self.training or self.p == 0.0:\n",
    "            return x\n",
    "        ## this match PyTorch's nn.Dropout random behavior\n",
    "        mask = torch.empty_like(x, dtype=torch.float32)\n",
    "        mask = mask.bernoulli_(self.keep_prob)\n",
    "        return x * mask / self.keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdd5961",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7cedd332",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = 4\n",
    "out_dim = 3\n",
    "p = 0.1\n",
    "\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "de75af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(batch_size, in_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "34cc201e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7992, -0.4482, -0.6629,  0.2023],\n",
       "        [-0.9519,  1.2229, -1.1902,  0.1363]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x/(1-p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5229139d",
   "metadata": {},
   "source": [
    "### Training output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "1703785b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000, -0.4482, -0.6629,  0.0000],\n",
       "        [-0.9519,  1.2229, -1.1902,  0.1363]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_drop = nn.Dropout(p=p)\n",
    "torch.manual_seed(0)\n",
    "nn_drop(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "abb95a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000, -0.4482, -0.6629,  0.0000],\n",
       "        [-0.9519,  1.2229, -1.1902,  0.1363]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop=Dropout(p=p)\n",
    "torch.manual_seed(0)\n",
    "drop(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da9c65b",
   "metadata": {},
   "source": [
    "### Evaluation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6d2d6833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7193, -0.4033, -0.5966,  0.1820],\n",
       "        [-0.8567,  1.1006, -1.0712,  0.1227]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ebf0fac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7193, -0.4033, -0.5966,  0.1820],\n",
       "        [-0.8567,  1.1006, -1.0712,  0.1227]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_drop.eval()\n",
    "nn_drop(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "691b0c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7193, -0.4033, -0.5966,  0.1820],\n",
       "        [-0.8567,  1.1006, -1.0712,  0.1227]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop.eval()\n",
    "drop(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21691cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
