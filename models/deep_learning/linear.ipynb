{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be540389",
   "metadata": {},
   "source": [
    "# Linear Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a501a77",
   "metadata": {},
   "source": [
    "## Definition: Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9c726e",
   "metadata": {},
   "source": [
    "Input\n",
    "* $x \\in \\mathbb{R}^{d_{in}}$ \n",
    "\n",
    "xeights\n",
    "* weight $W \\in \\mathbb{R}^{d_{out}\\times d_{in}}$ \n",
    "* bias $b \\in \\mathbb{R}^{d_{out}}$\n",
    "\n",
    "Output\n",
    "* $o \\in \\mathbb{R}^{d_{out}}$\n",
    "\n",
    "$$o = \\text{Linear}_{W,b}(x)=Wx+b.$$\n",
    "\n",
    "**Note:** In practice we also have a batch dimension and the inputs takes the form $X\\in \\mathbb{R}^{d_{b}\\times d_{in}}$ so the immplementation takes the form \n",
    "\n",
    "$$O = \\text{Linear}_{W,b}(X)=XW^{T}+1_{d_b}b^T,$$\n",
    "where\n",
    "$$1_{n}=\\begin{pmatrix}1\\\\1\\\\\\vdots\\\\ 1\n",
    "\\end{pmatrix}.$$\n",
    "denotes the $n$-dimensional vector of 1s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7160c4",
   "metadata": {},
   "source": [
    "## Property: Let $A\\in\\mathbb{R}^{m\\times n}$ and $w\\in \\mathbb{R}^n$, then\n",
    " $$\\frac{\\partial Aw}{\\partial w} = A$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c496365",
   "metadata": {},
   "source": [
    "**Proof:** Consider $A=[a_1|a_2|\\dots|a_m]^T$ and $f_i(w)=[Aw]_i=a^T_{i}w$. Then we have:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f_i}{\\partial w_\\ell}(w) &= \\frac{\\partial }{\\partial w_\\ell}\\left[\\sum_{j=1}^n a_{i,j}w_j\\right] ,\n",
    "\\\\\n",
    "&=\\sum_{j=1}^na_{i,j}\\frac{\\partial f}{\\partial w_\\ell}\\left[w_j\\right] ,\\\\\n",
    "&=\\sum_{j=1}^na_{i,j}\\begin{cases}\n",
    "1, & j=\\ell,\\\\\n",
    "0,&j\\neq \\ell,\\\\\n",
    "\\end{cases}\n",
    ",\\\\\n",
    "&= a_{i,l}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Then $\\frac{\\partial f_i}{\\partial w}= a_i^T$, and consequently $\\frac{\\partial Aw}{\\partial w} =A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0a10bb",
   "metadata": {},
   "source": [
    "## Property: Let $A\\in\\mathbb{R}^{n\\times n}$ and $w\\in \\mathbb{R}^n$, then\n",
    " $$\\frac{\\partial w^TAw}{\\partial w} = w^T(A+A^T)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73841034",
   "metadata": {},
   "source": [
    "**Proof:** Consider $f(w)=w^tAw$ and let's derive $f$ wrt $w_\\ell$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial w_\\ell}(w) &= \\frac{\\partial }{\\partial w_\\ell}\\left[\\sum_{i,j}w_ia_{i,j}w_j\\right] ,\n",
    "\\\\\n",
    "&=\\sum_{i,j=1}^na_{i,j}\\frac{\\partial }{\\partial w_\\ell}\\left[w_iw_j\\right] ,\\\\\n",
    "&=\\sum_{i,j=1}^na_{i,j}\\begin{cases}\n",
    "w_j, & i=\\ell,j\\neq \\ell,\\\\\n",
    "w_i, & i\\neq \\ell,j= \\ell,\\\\\n",
    "2w_{\\ell} & i=\\ell,j= \\ell,\\\\\n",
    "0& i\\neq\\ell,j\\neq \\ell,\\\\\n",
    "\\end{cases}\n",
    ",\\\\\n",
    "&= \\sum_{ j=1, j\\neq \\ell}^n a_{\\ell,j}w_j+\\sum_{ i=1, i\\neq \\ell}^n a_{i,\\ell}w_i +2a_{\\ell,\\ell}a_{\\ell,\\ell},\\\\\n",
    "&= \\sum_{ j=1}^n a_{\\ell,j}w_j+\\sum_{ i=1}^n a_{i,\\ell}w_i,\\\\\n",
    "&= [Aw]_{j}+[A^w]_{j},\\\\\n",
    "&= [Aw+A^Tw]_{j},\\\\\n",
    "&= [(A+A^T)w]_{j},\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So we get $\\frac{\\partial f}{\\partial w} =[(A+A^T)w ]^T=w^T(A^T+A)=w^T(A+A^T)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dedcf74",
   "metadata": {},
   "source": [
    "## Property: Let $A\\in\\mathbb{R}^{m\\times n}$ is a full rank matrix with $m>n$, $b\\in \\mathbb{R}^m$ and $w\\in \\mathbb{R}^n$. Then\n",
    "1. The matrix $A^T\\!A$ is invertible\n",
    "2. The function $f(w) = \\|Aw-b\\|_2$ reachs its unique minimum value at\n",
    "\n",
    "$$w_{\\text{min}} = (A^TA)^{-1}A^Tb $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4699dca5",
   "metadata": {},
   "source": [
    "**Proof (1.):** Since $A$ has full column rank ($m>n$), we have $ \\operatorname{rank}(A) = n.$\n",
    "\n",
    "By the Rank-Nullity Theorem,\n",
    "\n",
    "$$\n",
    "\\operatorname{rank}(A) + \\operatorname{nullity}(A) = n,\n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\n",
    "\\operatorname{nullity}(A) = 0.\n",
    "$$\n",
    "\n",
    "If $A^T A x = 0$, then\n",
    "\n",
    "$$\n",
    "0 = x^T A^T A x = (Ax)^T A x =\\|Ax\\|_2^2 \\Rightarrow Ax = 0,\n",
    "$$\n",
    "\n",
    "which implies\n",
    "\n",
    "$$\n",
    "x = 0.\n",
    "$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\n",
    "\\operatorname{nullity}(A^T A) = 0.\n",
    "$$\n",
    "\n",
    "Since $A^T A \\in \\mathbb{R}^{n \\times n}$ and has zero nullity, it is invertible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c813a02",
   "metadata": {},
   "source": [
    "**Proof (2.):** Notice that $f(w)=\\sqrt{(Aw-b)^T(Aw-b)}$ ing $\\sqrt{\\cdot}$ is monotone, it is enough to prove the result for the function $g(w)=(Aw-b)^T(Aw-b)$. Notice that\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "g(w)&=(Aw-b)^T(Aw-b),\\\\\n",
    "&=(w^TA^T-b^T)(Aw-b),\\\\\n",
    "&=w^TA^TAw-w^TA^Tb-b^TAw+b^Tb,\\\\\n",
    "&=w^TA^TAw-2b^TAw+b^Tb,\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da34014e",
   "metadata": {},
   "source": [
    "Then taking derivative wrt $w$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial g}{\\partial w}(w)&=2w^TA^TA-2b^TA,\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2359fdd",
   "metadata": {},
   "source": [
    "Since the first derivative only vanishis at\n",
    "$$\n",
    "\\begin{align*}\n",
    "0&=\\frac{\\partial g}{\\partial w}(w)\n",
    "&=2w^TA^TA-2b^TA,\\\\\n",
    "2b^TA&=2w^TA^TA,\\\\\n",
    "A^TAw&=A^Tb,\\\\\n",
    "w&=(A^TA)^{-1}A^Tb.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "the only positive extreme value is $w_{\\text{min}}=(A^TA)^{-1}A^Tb$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54f1d13",
   "metadata": {},
   "source": [
    "But $w_{\\text{min}}$ is a minimum because its Hessian matrix is positive definite\n",
    "$$H(g)=2A^TA$$\n",
    "so $w_{\\text{min}}=(A^TA)^{-1}A^Tb$  is the only minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f5d170",
   "metadata": {},
   "source": [
    "## Property: Exact Solution\n",
    "\n",
    "If $X=[x_1|x_2|\\dots|x_m]^T\\in\\mathbb{R}^{m\\times n}$ is the matrix of $m$ trainning inputs and $y = (y_1,y_2,\\dots,y_m)\\in \\mathbb{R}^m$ is the training data to the linear model $\\text{Linear}_{w,b}$. Then the optimal values for the learnable parameter $w$ and $b$ for the given data $(X,y)$ and measing error using using the $\\text{MSE}$ loss function \n",
    "$$\\text{mse}(\\hat{y},y)=\\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{y}_i)^2,$$\n",
    "this means:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "w*\\\\\n",
    "-\\\\\n",
    "b*\n",
    "\\end{pmatrix} = \\min_{w,b}\\{\\text{mse}\\left(\\text{Linear}_{w,b}(X),y\\right)\\}\n",
    "$$\n",
    "are\n",
    "$$\\begin{pmatrix}\n",
    "w*\\\\\n",
    "-\\\\\n",
    "b*\n",
    "\\end{pmatrix}=(\\hat{X}^T\\hat{X})^{-1} \\hat{X}^Ty,$$ \n",
    "\n",
    "where $\\hat{X}=[X|1_{m}]$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeab610",
   "metadata": {},
   "source": [
    "**Proof:** Notice that\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Linear}_{w,b}(X)=Xw+b1_m =\\hat{X}\n",
    "\\begin{pmatrix}\n",
    "w\\\\\n",
    "-\\\\\n",
    "b\n",
    "\\end{pmatrix}.\n",
    "\\end{align*}\n",
    "$$\n",
    "From here the result is direct from applyting the previous property for $f(w)=\\|\\text{Linear}_{w,b}(x)-y\\|_2$. Notice that\n",
    "$$\\text{mse}\\left(\\text{Linear}_{w,b}(x),y\\right)= \\frac{1}{n}\\|\\text{Linear}_{w,b}(x)-y\\|_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76697be6",
   "metadata": {},
   "source": [
    "## Code: Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e03cbd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from linear import Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed06ebe",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1626058",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "in_dim = 5\n",
    "out_dim = 3\n",
    "\n",
    "bias = False\n",
    "device = 'mps'\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2175352",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(batch_size, in_dim).to(device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b05252f",
   "metadata": {},
   "source": [
    "### Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6db9c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_linear = nn.Linear(in_dim, out_dim, bias=bias,device=device, dtype=dtype)\n",
    "linear = Linear(in_dim, out_dim, bias=bias, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c558f0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "for name, param in torch_linear.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd5a3ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "for name, param in linear.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f116e5c",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3367307",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10fd1f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch_linear = nn.Linear(in_dim, out_dim, bias=bias,device=device, dtype=dtype)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "linear = Linear(in_dim, out_dim, bias=bias, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d1705ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0488, -0.1656, -0.1285],\n",
       "        [ 0.2958,  0.4459,  0.5653],\n",
       "        [-0.4216,  0.1715, -0.2898],\n",
       "        [-1.2251,  0.9167, -0.0632]], device='mps:0', grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cda91ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0488, -0.1656, -0.1285],\n",
       "        [ 0.2958,  0.4459,  0.5653],\n",
       "        [-0.4216,  0.1715, -0.2898],\n",
       "        [-1.2251,  0.9167, -0.0632]], device='mps:0',\n",
       "       grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1496325c",
   "metadata": {},
   "source": [
    "## Check Exact Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c650089",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "in_dim = 5\n",
    "out_dim = 1\n",
    "\n",
    "bias = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8eaeaa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_linear = nn.Linear(in_dim, out_dim, bias=bias,device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55be7c29",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff9471bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(batch_size, in_dim).to(device=device, dtype=dtype)\n",
    "x.shape\n",
    "A = torch.tensor([[2, 3, 0, 1,4]],device=device,dtype=dtype).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e98b5e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 25.60531997680664\n",
      "Epoch 10, Loss: 15.397218704223633\n",
      "Epoch 20, Loss: 9.4021577835083\n",
      "Epoch 30, Loss: 5.876128196716309\n",
      "Epoch 40, Loss: 3.7972495555877686\n",
      "Epoch 50, Loss: 2.5667552947998047\n",
      "Epoch 60, Loss: 1.8337950706481934\n",
      "Epoch 70, Loss: 1.39277982711792\n",
      "Epoch 80, Loss: 1.1232341527938843\n",
      "Epoch 90, Loss: 0.9545536637306213\n",
      "Epoch 100, Loss: 0.8453550338745117\n",
      "Epoch 110, Loss: 0.7713768482208252\n",
      "Epoch 120, Loss: 0.718390703201294\n",
      "Epoch 130, Loss: 0.6780471801757812\n",
      "Epoch 140, Loss: 0.6454421281814575\n",
      "Epoch 150, Loss: 0.6176899075508118\n",
      "Epoch 160, Loss: 0.5930885076522827\n",
      "Epoch 170, Loss: 0.5706294775009155\n",
      "Epoch 180, Loss: 0.5497121214866638\n",
      "Epoch 190, Loss: 0.529973566532135\n",
      "Epoch 200, Loss: 0.5111919045448303\n",
      "Epoch 210, Loss: 0.4932273030281067\n",
      "Epoch 220, Loss: 0.4759887158870697\n",
      "Epoch 230, Loss: 0.4594144821166992\n",
      "Epoch 240, Loss: 0.44345924258232117\n",
      "Epoch 250, Loss: 0.428088903427124\n",
      "Epoch 260, Loss: 0.41327521204948425\n",
      "Epoch 270, Loss: 0.39899420738220215\n",
      "Epoch 280, Loss: 0.3852241337299347\n",
      "Epoch 290, Loss: 0.3719457983970642\n",
      "Epoch 300, Loss: 0.359140545129776\n",
      "Epoch 310, Loss: 0.3467910587787628\n",
      "Epoch 320, Loss: 0.3348807394504547\n",
      "Epoch 330, Loss: 0.32339397072792053\n",
      "Epoch 340, Loss: 0.3123157322406769\n",
      "Epoch 350, Loss: 0.30163127183914185\n",
      "Epoch 360, Loss: 0.29132652282714844\n",
      "Epoch 370, Loss: 0.2813880443572998\n",
      "Epoch 380, Loss: 0.271802693605423\n",
      "Epoch 390, Loss: 0.26255789399147034\n",
      "Epoch 400, Loss: 0.25364166498184204\n",
      "Epoch 410, Loss: 0.2450423389673233\n",
      "Epoch 420, Loss: 0.23674848675727844\n",
      "Epoch 430, Loss: 0.2287493795156479\n",
      "Epoch 440, Loss: 0.2210344523191452\n",
      "Epoch 450, Loss: 0.21359361708164215\n",
      "Epoch 460, Loss: 0.20641715824604034\n",
      "Epoch 470, Loss: 0.19949571788311005\n",
      "Epoch 480, Loss: 0.19282008707523346\n",
      "Epoch 490, Loss: 0.1863817572593689\n",
      "Epoch 500, Loss: 0.18017202615737915\n",
      "Epoch 510, Loss: 0.1741829216480255\n",
      "Epoch 520, Loss: 0.16840659081935883\n",
      "Epoch 530, Loss: 0.16283546388149261\n",
      "Epoch 540, Loss: 0.15746213495731354\n",
      "Epoch 550, Loss: 0.1522797793149948\n",
      "Epoch 560, Loss: 0.1472814530134201\n",
      "Epoch 570, Loss: 0.14246073365211487\n",
      "Epoch 580, Loss: 0.13781121373176575\n",
      "Epoch 590, Loss: 0.13332682847976685\n",
      "Epoch 600, Loss: 0.12900172173976898\n",
      "Epoch 610, Loss: 0.1248302236199379\n",
      "Epoch 620, Loss: 0.12080688029527664\n",
      "Epoch 630, Loss: 0.11692643910646439\n",
      "Epoch 640, Loss: 0.11318381875753403\n",
      "Epoch 650, Loss: 0.10957418382167816\n",
      "Epoch 660, Loss: 0.10609269142150879\n",
      "Epoch 670, Loss: 0.10273487865924835\n",
      "Epoch 680, Loss: 0.09949623048305511\n",
      "Epoch 690, Loss: 0.0963725671172142\n",
      "Epoch 700, Loss: 0.09335988014936447\n",
      "Epoch 710, Loss: 0.09045415371656418\n",
      "Epoch 720, Loss: 0.08765166252851486\n",
      "Epoch 730, Loss: 0.0849485695362091\n",
      "Epoch 740, Loss: 0.0823414996266365\n",
      "Epoch 750, Loss: 0.07982698082923889\n",
      "Epoch 760, Loss: 0.07740171998739243\n",
      "Epoch 770, Loss: 0.07506263256072998\n",
      "Epoch 780, Loss: 0.07280657440423965\n",
      "Epoch 790, Loss: 0.07063061743974686\n",
      "Epoch 800, Loss: 0.06853189319372177\n",
      "Epoch 810, Loss: 0.06650769710540771\n",
      "Epoch 820, Loss: 0.0645553395152092\n",
      "Epoch 830, Loss: 0.06267227977514267\n",
      "Epoch 840, Loss: 0.060856059193611145\n",
      "Epoch 850, Loss: 0.05910434573888779\n",
      "Epoch 860, Loss: 0.05741480365395546\n",
      "Epoch 870, Loss: 0.05578523874282837\n",
      "Epoch 880, Loss: 0.054213445633649826\n",
      "Epoch 890, Loss: 0.05269746482372284\n",
      "Epoch 900, Loss: 0.05123535171151161\n",
      "Epoch 910, Loss: 0.049825094640254974\n",
      "Epoch 920, Loss: 0.04846492037177086\n",
      "Epoch 930, Loss: 0.04715301841497421\n",
      "Epoch 940, Loss: 0.04588766768574715\n",
      "Epoch 950, Loss: 0.04466721788048744\n",
      "Epoch 960, Loss: 0.04349010810256004\n",
      "Epoch 970, Loss: 0.0423547625541687\n",
      "Epoch 980, Loss: 0.041259702295064926\n",
      "Epoch 990, Loss: 0.040203485637903214\n",
      "Epoch 1000, Loss: 0.039184778928756714\n",
      "Epoch 1010, Loss: 0.0382022000849247\n",
      "Epoch 1020, Loss: 0.03725452721118927\n",
      "Epoch 1030, Loss: 0.03634047508239746\n",
      "Epoch 1040, Loss: 0.03545882925391197\n",
      "Epoch 1050, Loss: 0.03460848331451416\n",
      "Epoch 1060, Loss: 0.033788297325372696\n",
      "Epoch 1070, Loss: 0.03299720585346222\n",
      "Epoch 1080, Loss: 0.03223419934511185\n",
      "Epoch 1090, Loss: 0.03149823844432831\n",
      "Epoch 1100, Loss: 0.03078841231763363\n",
      "Epoch 1110, Loss: 0.030103793367743492\n",
      "Epoch 1120, Loss: 0.029443396255373955\n",
      "Epoch 1130, Loss: 0.028806466609239578\n",
      "Epoch 1140, Loss: 0.02819213829934597\n",
      "Epoch 1150, Loss: 0.02759956382215023\n",
      "Epoch 1160, Loss: 0.027028068900108337\n",
      "Epoch 1170, Loss: 0.026476800441741943\n",
      "Epoch 1180, Loss: 0.025945110246539116\n",
      "Epoch 1190, Loss: 0.025432268157601357\n",
      "Epoch 1200, Loss: 0.024937622249126434\n",
      "Epoch 1210, Loss: 0.024460522457957268\n",
      "Epoch 1220, Loss: 0.02400035411119461\n",
      "Epoch 1230, Loss: 0.023556463420391083\n",
      "Epoch 1240, Loss: 0.02312835492193699\n",
      "Epoch 1250, Loss: 0.022715413942933083\n",
      "Epoch 1260, Loss: 0.022317135706543922\n",
      "Epoch 1270, Loss: 0.021932950243353844\n",
      "Epoch 1280, Loss: 0.021562375128269196\n",
      "Epoch 1290, Loss: 0.021204981952905655\n",
      "Epoch 1300, Loss: 0.020860251039266586\n",
      "Epoch 1310, Loss: 0.020527753978967667\n",
      "Epoch 1320, Loss: 0.020207032561302185\n",
      "Epoch 1330, Loss: 0.019897684454917908\n",
      "Epoch 1340, Loss: 0.01959933713078499\n",
      "Epoch 1350, Loss: 0.019311556592583656\n",
      "Epoch 1360, Loss: 0.019033990800380707\n",
      "Epoch 1370, Loss: 0.018766241148114204\n",
      "Epoch 1380, Loss: 0.018508000299334526\n",
      "Epoch 1390, Loss: 0.018258914351463318\n",
      "Epoch 1400, Loss: 0.01801864244043827\n",
      "Epoch 1410, Loss: 0.017786914482712746\n",
      "Epoch 1420, Loss: 0.017563385888934135\n",
      "Epoch 1430, Loss: 0.017347784712910652\n",
      "Epoch 1440, Loss: 0.017139829695224762\n",
      "Epoch 1450, Loss: 0.016939248889684677\n",
      "Epoch 1460, Loss: 0.01674576848745346\n",
      "Epoch 1470, Loss: 0.01655915379524231\n",
      "Epoch 1480, Loss: 0.016379158943891525\n",
      "Epoch 1490, Loss: 0.016205541789531708\n",
      "Epoch 1500, Loss: 0.016038071364164352\n",
      "Epoch 1510, Loss: 0.01587654836475849\n",
      "Epoch 1520, Loss: 0.015720760449767113\n",
      "Epoch 1530, Loss: 0.01557046826928854\n",
      "Epoch 1540, Loss: 0.01542550791054964\n",
      "Epoch 1550, Loss: 0.015285692177712917\n",
      "Epoch 1560, Loss: 0.0151508254930377\n",
      "Epoch 1570, Loss: 0.01502075046300888\n",
      "Epoch 1580, Loss: 0.014895278960466385\n",
      "Epoch 1590, Loss: 0.014774246141314507\n",
      "Epoch 1600, Loss: 0.014657503925263882\n",
      "Epoch 1610, Loss: 0.014544901438057423\n",
      "Epoch 1620, Loss: 0.014436294324696064\n",
      "Epoch 1630, Loss: 0.014331534504890442\n",
      "Epoch 1640, Loss: 0.01423047948628664\n",
      "Epoch 1650, Loss: 0.014133017510175705\n",
      "Epoch 1660, Loss: 0.014038996770977974\n",
      "Epoch 1670, Loss: 0.01394830271601677\n",
      "Epoch 1680, Loss: 0.013860831968486309\n",
      "Epoch 1690, Loss: 0.013776464387774467\n",
      "Epoch 1700, Loss: 0.013695070520043373\n",
      "Epoch 1710, Loss: 0.013616571202874184\n",
      "Epoch 1720, Loss: 0.013540856540203094\n",
      "Epoch 1730, Loss: 0.013467822223901749\n",
      "Epoch 1740, Loss: 0.01339737419039011\n",
      "Epoch 1750, Loss: 0.013329417444765568\n",
      "Epoch 1760, Loss: 0.013263873755931854\n",
      "Epoch 1770, Loss: 0.013200649991631508\n",
      "Epoch 1780, Loss: 0.013139670714735985\n",
      "Epoch 1790, Loss: 0.013080841861665249\n",
      "Epoch 1800, Loss: 0.0130241010338068\n",
      "Epoch 1810, Loss: 0.01296937931329012\n",
      "Epoch 1820, Loss: 0.01291659101843834\n",
      "Epoch 1830, Loss: 0.012865657918155193\n",
      "Epoch 1840, Loss: 0.012816550210118294\n",
      "Epoch 1850, Loss: 0.012769162654876709\n",
      "Epoch 1860, Loss: 0.012723473832011223\n",
      "Epoch 1870, Loss: 0.0126793859526515\n",
      "Epoch 1880, Loss: 0.012636861763894558\n",
      "Epoch 1890, Loss: 0.01259586401283741\n",
      "Epoch 1900, Loss: 0.01255629863590002\n",
      "Epoch 1910, Loss: 0.012518143281340599\n",
      "Epoch 1920, Loss: 0.012481333687901497\n",
      "Epoch 1930, Loss: 0.012445816770195961\n",
      "Epoch 1940, Loss: 0.012411561794579029\n",
      "Epoch 1950, Loss: 0.012378532439470291\n",
      "Epoch 1960, Loss: 0.012346667237579823\n",
      "Epoch 1970, Loss: 0.012315924279391766\n",
      "Epoch 1980, Loss: 0.012286273762583733\n",
      "Epoch 1990, Loss: 0.012257677502930164\n",
      "Epoch 2000, Loss: 0.012230086140334606\n",
      "Epoch 2010, Loss: 0.012203476391732693\n",
      "Epoch 2020, Loss: 0.012177807278931141\n",
      "Epoch 2030, Loss: 0.01215304434299469\n",
      "Epoch 2040, Loss: 0.012129166163504124\n",
      "Epoch 2050, Loss: 0.012106134556233883\n",
      "Epoch 2060, Loss: 0.012083915993571281\n",
      "Epoch 2070, Loss: 0.012062473222613335\n",
      "Epoch 2080, Loss: 0.012041804380714893\n",
      "Epoch 2090, Loss: 0.012021861039102077\n",
      "Epoch 2100, Loss: 0.012002614326775074\n",
      "Epoch 2110, Loss: 0.011984067969024181\n",
      "Epoch 2120, Loss: 0.01196616142988205\n",
      "Epoch 2130, Loss: 0.011948900297284126\n",
      "Epoch 2140, Loss: 0.011932243593037128\n",
      "Epoch 2150, Loss: 0.011916173622012138\n",
      "Epoch 2160, Loss: 0.011900680139660835\n",
      "Epoch 2170, Loss: 0.011885724030435085\n",
      "Epoch 2180, Loss: 0.011871308088302612\n",
      "Epoch 2190, Loss: 0.011857394129037857\n",
      "Epoch 2200, Loss: 0.011843981221318245\n",
      "Epoch 2210, Loss: 0.011831033043563366\n",
      "Epoch 2220, Loss: 0.011818554252386093\n",
      "Epoch 2230, Loss: 0.01180651132017374\n",
      "Epoch 2240, Loss: 0.01179490052163601\n",
      "Epoch 2250, Loss: 0.011783691123127937\n",
      "Epoch 2260, Loss: 0.011772883124649525\n",
      "Epoch 2270, Loss: 0.011762458831071854\n",
      "Epoch 2280, Loss: 0.011752400547266006\n",
      "Epoch 2290, Loss: 0.011742695234715939\n",
      "Epoch 2300, Loss: 0.011733333580195904\n",
      "Epoch 2310, Loss: 0.011724311858415604\n",
      "Epoch 2320, Loss: 0.011715603061020374\n",
      "Epoch 2330, Loss: 0.011707199737429619\n",
      "Epoch 2340, Loss: 0.01169909443706274\n",
      "Epoch 2350, Loss: 0.011691286228597164\n",
      "Epoch 2360, Loss: 0.011683751828968525\n",
      "Epoch 2370, Loss: 0.011676470749080181\n",
      "Epoch 2380, Loss: 0.011669456958770752\n",
      "Epoch 2390, Loss: 0.011662688106298447\n",
      "Epoch 2400, Loss: 0.011656160466372967\n",
      "Epoch 2410, Loss: 0.011649870313704014\n",
      "Epoch 2420, Loss: 0.011643779464066029\n",
      "Epoch 2430, Loss: 0.01163792610168457\n",
      "Epoch 2440, Loss: 0.01163227017968893\n",
      "Epoch 2450, Loss: 0.011626824736595154\n",
      "Epoch 2460, Loss: 0.011621570214629173\n",
      "Epoch 2470, Loss: 0.011616489849984646\n",
      "Epoch 2480, Loss: 0.01161159761250019\n",
      "Epoch 2490, Loss: 0.011606878601014614\n",
      "Epoch 2500, Loss: 0.011602329090237617\n",
      "Epoch 2510, Loss: 0.01159792859107256\n",
      "Epoch 2520, Loss: 0.01159369945526123\n",
      "Epoch 2530, Loss: 0.011589600704610348\n",
      "Epoch 2540, Loss: 0.011585661210119724\n",
      "Epoch 2550, Loss: 0.01158185675740242\n",
      "Epoch 2560, Loss: 0.01157818641513586\n",
      "Epoch 2570, Loss: 0.011574644595384598\n",
      "Epoch 2580, Loss: 0.011571232229471207\n",
      "Epoch 2590, Loss: 0.011567940004169941\n",
      "Epoch 2600, Loss: 0.011564766988158226\n",
      "Epoch 2610, Loss: 0.011561701074242592\n",
      "Epoch 2620, Loss: 0.011558742262423038\n",
      "Epoch 2630, Loss: 0.011555885896086693\n",
      "Epoch 2640, Loss: 0.011553131975233555\n",
      "Epoch 2650, Loss: 0.0115504814311862\n",
      "Epoch 2660, Loss: 0.011547924019396305\n",
      "Epoch 2670, Loss: 0.011545464396476746\n",
      "Epoch 2680, Loss: 0.011543083004653454\n",
      "Epoch 2690, Loss: 0.011540770530700684\n",
      "Epoch 2700, Loss: 0.011538553982973099\n",
      "Epoch 2710, Loss: 0.011536415666341782\n",
      "Epoch 2720, Loss: 0.01153435930609703\n",
      "Epoch 2730, Loss: 0.011532368138432503\n",
      "Epoch 2740, Loss: 0.011530444025993347\n",
      "Epoch 2750, Loss: 0.011528600938618183\n",
      "Epoch 2760, Loss: 0.011526819318532944\n",
      "Epoch 2770, Loss: 0.011525084264576435\n",
      "Epoch 2780, Loss: 0.011523422785103321\n",
      "Epoch 2790, Loss: 0.011521825566887856\n",
      "Epoch 2800, Loss: 0.01152027864009142\n",
      "Epoch 2810, Loss: 0.011518781073391438\n",
      "Epoch 2820, Loss: 0.011517343111336231\n",
      "Epoch 2830, Loss: 0.011515950784087181\n",
      "Epoch 2840, Loss: 0.01151461061090231\n",
      "Epoch 2850, Loss: 0.011513326317071915\n",
      "Epoch 2860, Loss: 0.011512074619531631\n",
      "Epoch 2870, Loss: 0.011510884389281273\n",
      "Epoch 2880, Loss: 0.011509723030030727\n",
      "Epoch 2890, Loss: 0.011508598923683167\n",
      "Epoch 2900, Loss: 0.011507511138916016\n",
      "Epoch 2910, Loss: 0.011506468988955021\n",
      "Epoch 2920, Loss: 0.011505473405122757\n",
      "Epoch 2930, Loss: 0.01150450948625803\n",
      "Epoch 2940, Loss: 0.011503569781780243\n",
      "Epoch 2950, Loss: 0.011502663604915142\n",
      "Epoch 2960, Loss: 0.011501790955662727\n",
      "Epoch 2970, Loss: 0.011500959284603596\n",
      "Epoch 2980, Loss: 0.011500148102641106\n",
      "Epoch 2990, Loss: 0.011499368585646152\n",
      "Epoch 3000, Loss: 0.01149861328303814\n",
      "Epoch 3010, Loss: 0.01149788685142994\n",
      "Epoch 3020, Loss: 0.011497187428176403\n",
      "Epoch 3030, Loss: 0.011496511287987232\n",
      "Epoch 3040, Loss: 0.011495858430862427\n",
      "Epoch 3050, Loss: 0.011495227925479412\n",
      "Epoch 3060, Loss: 0.011494615115225315\n",
      "Epoch 3070, Loss: 0.011494037695229053\n",
      "Epoch 3080, Loss: 0.01149346400052309\n",
      "Epoch 3090, Loss: 0.011492926627397537\n",
      "Epoch 3100, Loss: 0.011492405086755753\n",
      "Epoch 3110, Loss: 0.011491887271404266\n",
      "Epoch 3120, Loss: 0.01149140577763319\n",
      "Epoch 3130, Loss: 0.011490926146507263\n",
      "Epoch 3140, Loss: 0.011490473523736\n",
      "Epoch 3150, Loss: 0.011490042321383953\n",
      "Epoch 3160, Loss: 0.011489614844322205\n",
      "Epoch 3170, Loss: 0.0114892041310668\n",
      "Epoch 3180, Loss: 0.011488809250295162\n",
      "Epoch 3190, Loss: 0.011488422751426697\n",
      "Epoch 3200, Loss: 0.011488063260912895\n",
      "Epoch 3210, Loss: 0.011487705633044243\n",
      "Epoch 3220, Loss: 0.011487376876175404\n",
      "Epoch 3230, Loss: 0.011487037874758244\n",
      "Epoch 3240, Loss: 0.011486717499792576\n",
      "Epoch 3250, Loss: 0.011486408300697803\n",
      "Epoch 3260, Loss: 0.011486119590699673\n",
      "Epoch 3270, Loss: 0.01148582436144352\n",
      "Epoch 3280, Loss: 0.011485561728477478\n",
      "Epoch 3290, Loss: 0.011485292576253414\n",
      "Epoch 3300, Loss: 0.011485036462545395\n",
      "Epoch 3310, Loss: 0.011484785005450249\n",
      "Epoch 3320, Loss: 0.01148455124348402\n",
      "Epoch 3330, Loss: 0.011484316550195217\n",
      "Epoch 3340, Loss: 0.011484098620712757\n",
      "Epoch 3350, Loss: 0.011483881622552872\n",
      "Epoch 3360, Loss: 0.011483671143651009\n",
      "Epoch 3370, Loss: 0.01148347556591034\n",
      "Epoch 3380, Loss: 0.01148329209536314\n",
      "Epoch 3390, Loss: 0.011483092792332172\n",
      "Epoch 3400, Loss: 0.011482922360301018\n",
      "Epoch 3410, Loss: 0.01148274727165699\n",
      "Epoch 3420, Loss: 0.011482581496238708\n",
      "Epoch 3430, Loss: 0.011482419446110725\n",
      "Epoch 3440, Loss: 0.011482268571853638\n",
      "Epoch 3450, Loss: 0.011482109315693378\n",
      "Epoch 3460, Loss: 0.011481976136565208\n",
      "Epoch 3470, Loss: 0.011481826193630695\n",
      "Epoch 3480, Loss: 0.011481694877147675\n",
      "Epoch 3490, Loss: 0.01148157473653555\n",
      "Epoch 3500, Loss: 0.011481442488729954\n",
      "Epoch 3510, Loss: 0.011481324210762978\n",
      "Epoch 3520, Loss: 0.011481204070150852\n",
      "Epoch 3530, Loss: 0.011481094174087048\n",
      "Epoch 3540, Loss: 0.011480987071990967\n",
      "Epoch 3550, Loss: 0.011480881832540035\n",
      "Epoch 3560, Loss: 0.011480782181024551\n",
      "Epoch 3570, Loss: 0.01148068904876709\n",
      "Epoch 3580, Loss: 0.011480586603283882\n",
      "Epoch 3590, Loss: 0.01148049347102642\n",
      "Epoch 3600, Loss: 0.011480418033897877\n",
      "Epoch 3610, Loss: 0.01148032397031784\n",
      "Epoch 3620, Loss: 0.011480247601866722\n",
      "Epoch 3630, Loss: 0.011480169370770454\n",
      "Epoch 3640, Loss: 0.011480099521577358\n",
      "Epoch 3650, Loss: 0.01148002129048109\n",
      "Epoch 3660, Loss: 0.011479949578642845\n",
      "Epoch 3670, Loss: 0.01147988811135292\n",
      "Epoch 3680, Loss: 0.011479814536869526\n",
      "Epoch 3690, Loss: 0.01147975493222475\n",
      "Epoch 3700, Loss: 0.011479695327579975\n",
      "Epoch 3710, Loss: 0.011479631066322327\n",
      "Epoch 3720, Loss: 0.011479579843580723\n",
      "Epoch 3730, Loss: 0.011479523032903671\n",
      "Epoch 3740, Loss: 0.01147947646677494\n",
      "Epoch 3750, Loss: 0.01147941779345274\n",
      "Epoch 3760, Loss: 0.011479374952614307\n",
      "Epoch 3770, Loss: 0.011479323729872704\n",
      "Epoch 3780, Loss: 0.011479279957711697\n",
      "Epoch 3790, Loss: 0.01147923432290554\n",
      "Epoch 3800, Loss: 0.011479197070002556\n",
      "Epoch 3810, Loss: 0.011479145847260952\n",
      "Epoch 3820, Loss: 0.011479117907583714\n",
      "Epoch 3830, Loss: 0.011479067616164684\n",
      "Epoch 3840, Loss: 0.011479043401777744\n",
      "Epoch 3850, Loss: 0.011479008942842484\n",
      "Epoch 3860, Loss: 0.011478960514068604\n",
      "Epoch 3870, Loss: 0.011478930711746216\n",
      "Epoch 3880, Loss: 0.011478905566036701\n",
      "Epoch 3890, Loss: 0.01147887296974659\n",
      "Epoch 3900, Loss: 0.011478839442133904\n",
      "Epoch 3910, Loss: 0.011478808708488941\n",
      "Epoch 3920, Loss: 0.011478783562779427\n",
      "Epoch 3930, Loss: 0.011478753760457039\n",
      "Epoch 3940, Loss: 0.011478730477392673\n",
      "Epoch 3950, Loss: 0.011478707194328308\n",
      "Epoch 3960, Loss: 0.011478676460683346\n",
      "Epoch 3970, Loss: 0.011478659696877003\n",
      "Epoch 3980, Loss: 0.01147863082587719\n",
      "Epoch 3990, Loss: 0.011478612199425697\n",
      "Epoch 4000, Loss: 0.011478595435619354\n",
      "Epoch 4010, Loss: 0.011478574015200138\n",
      "Epoch 4020, Loss: 0.011478564701974392\n",
      "Epoch 4030, Loss: 0.011478533037006855\n",
      "Epoch 4040, Loss: 0.011478518135845661\n",
      "Epoch 4050, Loss: 0.011478502303361893\n",
      "Epoch 4060, Loss: 0.01147848553955555\n",
      "Epoch 4070, Loss: 0.011478468775749207\n",
      "Epoch 4080, Loss: 0.011478452011942863\n",
      "Epoch 4090, Loss: 0.01147843711078167\n",
      "Epoch 4100, Loss: 0.011478425934910774\n",
      "Epoch 4110, Loss: 0.011478406377136707\n",
      "Epoch 4120, Loss: 0.011478391475975513\n",
      "Epoch 4130, Loss: 0.011478383094072342\n",
      "Epoch 4140, Loss: 0.011478369124233723\n",
      "Epoch 4150, Loss: 0.011478360742330551\n",
      "Epoch 4160, Loss: 0.011478343047201633\n",
      "Epoch 4170, Loss: 0.011478330940008163\n",
      "Epoch 4180, Loss: 0.011478323489427567\n",
      "Epoch 4190, Loss: 0.011478318832814693\n",
      "Epoch 4200, Loss: 0.011478300206363201\n",
      "Epoch 4210, Loss: 0.011478297412395477\n",
      "Epoch 4220, Loss: 0.011478292755782604\n",
      "Epoch 4230, Loss: 0.011478272266685963\n",
      "Epoch 4240, Loss: 0.011478263884782791\n",
      "Epoch 4250, Loss: 0.011478257365524769\n",
      "Epoch 4260, Loss: 0.011478254571557045\n",
      "Epoch 4270, Loss: 0.011478240601718426\n",
      "Epoch 4280, Loss: 0.011478233151137829\n",
      "Epoch 4290, Loss: 0.011478228494524956\n",
      "Epoch 4300, Loss: 0.011478212662041187\n",
      "Epoch 4310, Loss: 0.01147820707410574\n",
      "Epoch 4320, Loss: 0.01147820707410574\n",
      "Epoch 4330, Loss: 0.011478208936750889\n",
      "Epoch 4340, Loss: 0.01147819496691227\n",
      "Epoch 4350, Loss: 0.011478185653686523\n",
      "Epoch 4360, Loss: 0.011478187516331673\n",
      "Epoch 4370, Loss: 0.0114781828597188\n",
      "Epoch 4380, Loss: 0.011478173546493053\n",
      "Epoch 4390, Loss: 0.011478167958557606\n",
      "Epoch 4400, Loss: 0.011478166095912457\n",
      "Epoch 4410, Loss: 0.011478161439299583\n",
      "Epoch 4420, Loss: 0.011478153988718987\n",
      "Epoch 4430, Loss: 0.01147814467549324\n",
      "Epoch 4440, Loss: 0.011478143744170666\n",
      "Epoch 4450, Loss: 0.011478142812848091\n",
      "Epoch 4460, Loss: 0.011478130705654621\n",
      "Epoch 4470, Loss: 0.011478126980364323\n",
      "Epoch 4480, Loss: 0.01147812232375145\n",
      "Epoch 4490, Loss: 0.011478126049041748\n",
      "Epoch 4500, Loss: 0.011478111147880554\n",
      "Epoch 4510, Loss: 0.011478117667138577\n",
      "Epoch 4520, Loss: 0.011478113941848278\n",
      "Epoch 4530, Loss: 0.011478111147880554\n",
      "Epoch 4540, Loss: 0.011478113941848278\n",
      "Epoch 4550, Loss: 0.011478100903332233\n",
      "Epoch 4560, Loss: 0.011478109285235405\n",
      "Epoch 4570, Loss: 0.011478097178041935\n",
      "Epoch 4580, Loss: 0.011478097178041935\n",
      "Epoch 4590, Loss: 0.01147809810936451\n",
      "Epoch 4600, Loss: 0.011478092521429062\n",
      "Epoch 4610, Loss: 0.011478094384074211\n",
      "Epoch 4620, Loss: 0.01147808413952589\n",
      "Epoch 4630, Loss: 0.011478079482913017\n",
      "Epoch 4640, Loss: 0.01147808600217104\n",
      "Epoch 4650, Loss: 0.011478080414235592\n",
      "Epoch 4660, Loss: 0.011478080414235592\n",
      "Epoch 4670, Loss: 0.011478078551590443\n",
      "Epoch 4680, Loss: 0.011478072963654995\n",
      "Epoch 4690, Loss: 0.011478080414235592\n",
      "Epoch 4700, Loss: 0.011478067375719547\n",
      "Epoch 4710, Loss: 0.011478068307042122\n",
      "Epoch 4720, Loss: 0.011478074826300144\n",
      "Epoch 4730, Loss: 0.011478063650429249\n",
      "Epoch 4740, Loss: 0.011478066444396973\n",
      "Epoch 4750, Loss: 0.011478072963654995\n",
      "Epoch 4760, Loss: 0.011478062719106674\n",
      "Epoch 4770, Loss: 0.011478067375719547\n",
      "Epoch 4780, Loss: 0.011478063650429249\n",
      "Epoch 4790, Loss: 0.011478066444396973\n",
      "Epoch 4800, Loss: 0.011478053405880928\n",
      "Epoch 4810, Loss: 0.011478055268526077\n",
      "Epoch 4820, Loss: 0.011478053405880928\n",
      "Epoch 4830, Loss: 0.011478051543235779\n",
      "Epoch 4840, Loss: 0.011478055268526077\n",
      "Epoch 4850, Loss: 0.011478054337203503\n",
      "Epoch 4860, Loss: 0.011478050611913204\n",
      "Epoch 4870, Loss: 0.011478051543235779\n",
      "Epoch 4880, Loss: 0.011478046886622906\n",
      "Epoch 4890, Loss: 0.011478053405880928\n",
      "Epoch 4900, Loss: 0.01147804968059063\n",
      "Epoch 4910, Loss: 0.01147804968059063\n",
      "Epoch 4920, Loss: 0.011478056199848652\n",
      "Epoch 4930, Loss: 0.01147804968059063\n",
      "Epoch 4940, Loss: 0.011478042230010033\n",
      "Epoch 4950, Loss: 0.011478042230010033\n",
      "Epoch 4960, Loss: 0.011478045023977757\n",
      "Epoch 4970, Loss: 0.011478040367364883\n",
      "Epoch 4980, Loss: 0.011478045023977757\n",
      "Epoch 4990, Loss: 0.011478046886622906\n",
      "Epoch 5000, Loss: 0.011478046886622906\n",
      "Epoch 5010, Loss: 0.011478040367364883\n",
      "Epoch 5020, Loss: 0.011478041298687458\n",
      "Epoch 5030, Loss: 0.01147803757339716\n",
      "Epoch 5040, Loss: 0.011478034779429436\n",
      "Epoch 5050, Loss: 0.011478029191493988\n",
      "Epoch 5060, Loss: 0.011478045955300331\n",
      "Epoch 5070, Loss: 0.011478041298687458\n",
      "Epoch 5080, Loss: 0.01147803757339716\n",
      "Epoch 5090, Loss: 0.011478030122816563\n",
      "Epoch 5100, Loss: 0.011478034779429436\n",
      "Epoch 5110, Loss: 0.011478040367364883\n",
      "Epoch 5120, Loss: 0.011478028260171413\n",
      "Epoch 5130, Loss: 0.01147803757339716\n",
      "Epoch 5140, Loss: 0.011478030122816563\n",
      "Epoch 5150, Loss: 0.01147803757339716\n",
      "Epoch 5160, Loss: 0.011478036642074585\n",
      "Epoch 5170, Loss: 0.011478034779429436\n",
      "Epoch 5180, Loss: 0.011478034779429436\n",
      "Epoch 5190, Loss: 0.011478043161332607\n",
      "Epoch 5200, Loss: 0.011478045023977757\n",
      "Epoch 5210, Loss: 0.011478038504719734\n",
      "Epoch 5220, Loss: 0.011478032916784286\n",
      "Epoch 5230, Loss: 0.01147802546620369\n",
      "Epoch 5240, Loss: 0.011478030122816563\n",
      "Epoch 5250, Loss: 0.011478042230010033\n",
      "Epoch 5260, Loss: 0.011478028260171413\n",
      "Epoch 5270, Loss: 0.011478026397526264\n",
      "Epoch 5280, Loss: 0.011478032916784286\n",
      "Epoch 5290, Loss: 0.01147802360355854\n",
      "Epoch 5300, Loss: 0.011478032916784286\n",
      "Epoch 5310, Loss: 0.011478034779429436\n",
      "Epoch 5320, Loss: 0.011478032916784286\n",
      "Epoch 5330, Loss: 0.011478032916784286\n",
      "Epoch 5340, Loss: 0.011478030122816563\n",
      "Epoch 5350, Loss: 0.011478033848106861\n",
      "Epoch 5360, Loss: 0.011478031985461712\n",
      "Epoch 5370, Loss: 0.011478036642074585\n",
      "Epoch 5380, Loss: 0.011478032916784286\n",
      "Epoch 5390, Loss: 0.011478032916784286\n",
      "Epoch 5400, Loss: 0.011478033848106861\n",
      "Epoch 5410, Loss: 0.011478031985461712\n",
      "Epoch 5420, Loss: 0.011478032916784286\n",
      "Epoch 5430, Loss: 0.011478026397526264\n",
      "Epoch 5440, Loss: 0.011478026397526264\n",
      "Epoch 5450, Loss: 0.01147802360355854\n",
      "Epoch 5460, Loss: 0.01147802360355854\n",
      "Epoch 5470, Loss: 0.011478031985461712\n",
      "Epoch 5480, Loss: 0.011478030122816563\n",
      "Epoch 5490, Loss: 0.011478024534881115\n",
      "Epoch 5500, Loss: 0.011478028260171413\n",
      "Epoch 5510, Loss: 0.011478018946945667\n",
      "Epoch 5520, Loss: 0.011478021740913391\n",
      "Epoch 5530, Loss: 0.011478030122816563\n",
      "Epoch 5540, Loss: 0.011478029191493988\n",
      "Epoch 5550, Loss: 0.011478031985461712\n",
      "Epoch 5560, Loss: 0.011478031985461712\n",
      "Epoch 5570, Loss: 0.011478032916784286\n",
      "Epoch 5580, Loss: 0.011478034779429436\n",
      "Epoch 5590, Loss: 0.01147802546620369\n",
      "Epoch 5600, Loss: 0.01147802546620369\n",
      "Epoch 5610, Loss: 0.01147802546620369\n",
      "Epoch 5620, Loss: 0.011478028260171413\n",
      "Epoch 5630, Loss: 0.011478028260171413\n",
      "Epoch 5640, Loss: 0.011478032916784286\n",
      "Epoch 5650, Loss: 0.011478032916784286\n",
      "Epoch 5660, Loss: 0.011478026397526264\n",
      "Epoch 5670, Loss: 0.01147802546620369\n",
      "Epoch 5680, Loss: 0.011478028260171413\n",
      "Epoch 5690, Loss: 0.011478024534881115\n",
      "Epoch 5700, Loss: 0.01147802546620369\n",
      "Epoch 5710, Loss: 0.01147802546620369\n",
      "Epoch 5720, Loss: 0.011478024534881115\n",
      "Epoch 5730, Loss: 0.01147802546620369\n",
      "Epoch 5740, Loss: 0.011478028260171413\n",
      "Epoch 5750, Loss: 0.01147802546620369\n",
      "Epoch 5760, Loss: 0.011478021740913391\n",
      "Epoch 5770, Loss: 0.01147802360355854\n",
      "Epoch 5780, Loss: 0.011478028260171413\n",
      "Epoch 5790, Loss: 0.01147802360355854\n",
      "Epoch 5800, Loss: 0.01147802546620369\n",
      "Epoch 5810, Loss: 0.011478030122816563\n",
      "Epoch 5820, Loss: 0.011478017084300518\n",
      "Epoch 5830, Loss: 0.011478028260171413\n",
      "Epoch 5840, Loss: 0.01147802546620369\n",
      "Epoch 5850, Loss: 0.011478020809590816\n",
      "Epoch 5860, Loss: 0.011478032916784286\n",
      "Epoch 5870, Loss: 0.011478028260171413\n",
      "Epoch 5880, Loss: 0.011478030122816563\n",
      "Epoch 5890, Loss: 0.011478024534881115\n",
      "Epoch 5900, Loss: 0.011478030122816563\n",
      "Epoch 5910, Loss: 0.01147802546620369\n",
      "Epoch 5920, Loss: 0.01147802360355854\n",
      "Epoch 5930, Loss: 0.011478029191493988\n",
      "Epoch 5940, Loss: 0.011478030122816563\n",
      "Epoch 5950, Loss: 0.011478031985461712\n",
      "Epoch 5960, Loss: 0.01147802546620369\n",
      "Epoch 5970, Loss: 0.011478030122816563\n",
      "Epoch 5980, Loss: 0.011478032916784286\n",
      "Epoch 5990, Loss: 0.01147802546620369\n",
      "Epoch 6000, Loss: 0.011478030122816563\n",
      "Epoch 6010, Loss: 0.01147802360355854\n",
      "Epoch 6020, Loss: 0.011478030122816563\n",
      "Epoch 6030, Loss: 0.011478028260171413\n",
      "Epoch 6040, Loss: 0.01147802546620369\n",
      "Epoch 6050, Loss: 0.011478028260171413\n",
      "Epoch 6060, Loss: 0.011478026397526264\n",
      "Epoch 6070, Loss: 0.011478031985461712\n",
      "Epoch 6080, Loss: 0.011478026397526264\n",
      "Epoch 6090, Loss: 0.011478034779429436\n",
      "Epoch 6100, Loss: 0.011478032916784286\n",
      "Epoch 6110, Loss: 0.01147802546620369\n",
      "Epoch 6120, Loss: 0.011478021740913391\n",
      "Epoch 6130, Loss: 0.011478024534881115\n",
      "Epoch 6140, Loss: 0.01147802546620369\n",
      "Epoch 6150, Loss: 0.011478028260171413\n",
      "Epoch 6160, Loss: 0.011478024534881115\n",
      "Epoch 6170, Loss: 0.011478021740913391\n",
      "Epoch 6180, Loss: 0.011478026397526264\n",
      "Epoch 6190, Loss: 0.011478033848106861\n",
      "Epoch 6200, Loss: 0.011478028260171413\n",
      "Epoch 6210, Loss: 0.011478030122816563\n",
      "Epoch 6220, Loss: 0.01147802546620369\n",
      "Epoch 6230, Loss: 0.011478030122816563\n",
      "Epoch 6240, Loss: 0.011478030122816563\n",
      "Epoch 6250, Loss: 0.011478028260171413\n",
      "Epoch 6260, Loss: 0.01147802360355854\n",
      "Epoch 6270, Loss: 0.011478030122816563\n",
      "Epoch 6280, Loss: 0.011478021740913391\n",
      "Epoch 6290, Loss: 0.011478026397526264\n",
      "Epoch 6300, Loss: 0.011478030122816563\n",
      "Epoch 6310, Loss: 0.011478028260171413\n",
      "Epoch 6320, Loss: 0.011478028260171413\n",
      "Epoch 6330, Loss: 0.01147802360355854\n",
      "Epoch 6340, Loss: 0.011478034779429436\n",
      "Epoch 6350, Loss: 0.011478032916784286\n",
      "Epoch 6360, Loss: 0.01147803757339716\n",
      "Epoch 6370, Loss: 0.011478029191493988\n",
      "Epoch 6380, Loss: 0.011478033848106861\n",
      "Epoch 6390, Loss: 0.01147802546620369\n",
      "Epoch 6400, Loss: 0.011478029191493988\n",
      "Epoch 6410, Loss: 0.011478028260171413\n",
      "Epoch 6420, Loss: 0.011478030122816563\n",
      "Epoch 6430, Loss: 0.011478028260171413\n",
      "Epoch 6440, Loss: 0.011478028260171413\n",
      "Epoch 6450, Loss: 0.011478030122816563\n",
      "Epoch 6460, Loss: 0.01147802546620369\n",
      "Epoch 6470, Loss: 0.011478028260171413\n",
      "Epoch 6480, Loss: 0.011478028260171413\n",
      "Epoch 6490, Loss: 0.011478032916784286\n",
      "Epoch 6500, Loss: 0.01147803757339716\n",
      "Epoch 6510, Loss: 0.011478033848106861\n",
      "Epoch 6520, Loss: 0.011478031985461712\n",
      "Epoch 6530, Loss: 0.01147802360355854\n",
      "Epoch 6540, Loss: 0.011478032916784286\n",
      "Epoch 6550, Loss: 0.011478033848106861\n",
      "Epoch 6560, Loss: 0.011478028260171413\n",
      "Epoch 6570, Loss: 0.01147802546620369\n",
      "Epoch 6580, Loss: 0.011478028260171413\n",
      "Epoch 6590, Loss: 0.011478021740913391\n",
      "Epoch 6600, Loss: 0.011478029191493988\n",
      "Epoch 6610, Loss: 0.011478031985461712\n",
      "Epoch 6620, Loss: 0.011478030122816563\n",
      "Epoch 6630, Loss: 0.011478033848106861\n",
      "Epoch 6640, Loss: 0.011478034779429436\n",
      "Epoch 6650, Loss: 0.011478028260171413\n",
      "Epoch 6660, Loss: 0.011478030122816563\n",
      "Epoch 6670, Loss: 0.011478030122816563\n",
      "Epoch 6680, Loss: 0.011478030122816563\n",
      "Epoch 6690, Loss: 0.011478029191493988\n",
      "Epoch 6700, Loss: 0.011478032916784286\n",
      "Epoch 6710, Loss: 0.011478028260171413\n",
      "Epoch 6720, Loss: 0.011478030122816563\n",
      "Epoch 6730, Loss: 0.011478028260171413\n",
      "Epoch 6740, Loss: 0.011478028260171413\n",
      "Epoch 6750, Loss: 0.011478030122816563\n",
      "Epoch 6760, Loss: 0.011478030122816563\n",
      "Epoch 6770, Loss: 0.011478029191493988\n",
      "Epoch 6780, Loss: 0.011478026397526264\n",
      "Epoch 6790, Loss: 0.011478029191493988\n",
      "Epoch 6800, Loss: 0.011478026397526264\n",
      "Epoch 6810, Loss: 0.011478020809590816\n",
      "Epoch 6820, Loss: 0.011478024534881115\n",
      "Epoch 6830, Loss: 0.01147802360355854\n",
      "Epoch 6840, Loss: 0.011478028260171413\n",
      "Epoch 6850, Loss: 0.01147802546620369\n",
      "Epoch 6860, Loss: 0.01147802360355854\n",
      "Epoch 6870, Loss: 0.01147802546620369\n",
      "Epoch 6880, Loss: 0.011478028260171413\n",
      "Epoch 6890, Loss: 0.011478028260171413\n",
      "Epoch 6900, Loss: 0.011478026397526264\n",
      "Epoch 6910, Loss: 0.01147802546620369\n",
      "Epoch 6920, Loss: 0.011478029191493988\n",
      "Epoch 6930, Loss: 0.011478032916784286\n",
      "Epoch 6940, Loss: 0.011478026397526264\n",
      "Epoch 6950, Loss: 0.011478028260171413\n",
      "Epoch 6960, Loss: 0.011478031985461712\n",
      "Epoch 6970, Loss: 0.011478032916784286\n",
      "Epoch 6980, Loss: 0.011478032916784286\n",
      "Epoch 6990, Loss: 0.011478032916784286\n",
      "Epoch 7000, Loss: 0.011478024534881115\n",
      "Epoch 7010, Loss: 0.01147802546620369\n",
      "Epoch 7020, Loss: 0.011478031985461712\n",
      "Epoch 7030, Loss: 0.011478030122816563\n",
      "Epoch 7040, Loss: 0.011478029191493988\n",
      "Epoch 7050, Loss: 0.011478028260171413\n",
      "Epoch 7060, Loss: 0.011478026397526264\n",
      "Epoch 7070, Loss: 0.011478028260171413\n",
      "Epoch 7080, Loss: 0.011478031985461712\n",
      "Epoch 7090, Loss: 0.011478030122816563\n",
      "Epoch 7100, Loss: 0.011478028260171413\n",
      "Epoch 7110, Loss: 0.011478024534881115\n",
      "Epoch 7120, Loss: 0.011478028260171413\n",
      "Epoch 7130, Loss: 0.011478026397526264\n",
      "Epoch 7140, Loss: 0.011478028260171413\n",
      "Epoch 7150, Loss: 0.011478030122816563\n",
      "Epoch 7160, Loss: 0.01147802546620369\n",
      "Epoch 7170, Loss: 0.011478028260171413\n",
      "Epoch 7180, Loss: 0.011478026397526264\n",
      "Epoch 7190, Loss: 0.01147803757339716\n",
      "Epoch 7200, Loss: 0.01147803757339716\n",
      "Epoch 7210, Loss: 0.011478034779429436\n",
      "Epoch 7220, Loss: 0.011478034779429436\n",
      "Epoch 7230, Loss: 0.011478034779429436\n",
      "Epoch 7240, Loss: 0.011478032916784286\n",
      "Epoch 7250, Loss: 0.01147802546620369\n",
      "Epoch 7260, Loss: 0.011478024534881115\n",
      "Epoch 7270, Loss: 0.01147802546620369\n",
      "Epoch 7280, Loss: 0.01147802546620369\n",
      "Epoch 7290, Loss: 0.011478026397526264\n",
      "Epoch 7300, Loss: 0.011478026397526264\n",
      "Epoch 7310, Loss: 0.011478028260171413\n",
      "Epoch 7320, Loss: 0.011478026397526264\n",
      "Epoch 7330, Loss: 0.01147802546620369\n",
      "Epoch 7340, Loss: 0.011478028260171413\n",
      "Epoch 7350, Loss: 0.011478028260171413\n",
      "Epoch 7360, Loss: 0.011478028260171413\n",
      "Epoch 7370, Loss: 0.011478028260171413\n",
      "Epoch 7380, Loss: 0.011478028260171413\n",
      "Epoch 7390, Loss: 0.011478031985461712\n",
      "Epoch 7400, Loss: 0.011478030122816563\n",
      "Epoch 7410, Loss: 0.011478031985461712\n",
      "Epoch 7420, Loss: 0.011478030122816563\n",
      "Epoch 7430, Loss: 0.011478031985461712\n",
      "Epoch 7440, Loss: 0.011478031985461712\n",
      "Epoch 7450, Loss: 0.011478030122816563\n",
      "Epoch 7460, Loss: 0.011478028260171413\n",
      "Epoch 7470, Loss: 0.011478026397526264\n",
      "Epoch 7480, Loss: 0.011478026397526264\n",
      "Epoch 7490, Loss: 0.011478026397526264\n",
      "Epoch 7500, Loss: 0.01147802546620369\n",
      "Epoch 7510, Loss: 0.01147802546620369\n",
      "Epoch 7520, Loss: 0.011478026397526264\n",
      "Epoch 7530, Loss: 0.011478024534881115\n",
      "Epoch 7540, Loss: 0.011478024534881115\n",
      "Epoch 7550, Loss: 0.01147802360355854\n",
      "Epoch 7560, Loss: 0.01147802360355854\n",
      "Epoch 7570, Loss: 0.01147802360355854\n",
      "Epoch 7580, Loss: 0.01147802360355854\n",
      "Epoch 7590, Loss: 0.01147802546620369\n",
      "Epoch 7600, Loss: 0.01147802546620369\n",
      "Epoch 7610, Loss: 0.01147802546620369\n",
      "Epoch 7620, Loss: 0.01147802546620369\n",
      "Epoch 7630, Loss: 0.011478021740913391\n",
      "Epoch 7640, Loss: 0.011478021740913391\n",
      "Epoch 7650, Loss: 0.011478020809590816\n",
      "Epoch 7660, Loss: 0.011478019878268242\n",
      "Epoch 7670, Loss: 0.011478019878268242\n",
      "Epoch 7680, Loss: 0.011478018946945667\n",
      "Epoch 7690, Loss: 0.011478018946945667\n",
      "Epoch 7700, Loss: 0.011478019878268242\n",
      "Epoch 7710, Loss: 0.011478019878268242\n",
      "Epoch 7720, Loss: 0.011478018946945667\n",
      "Epoch 7730, Loss: 0.011478019878268242\n",
      "Epoch 7740, Loss: 0.011478019878268242\n",
      "Epoch 7750, Loss: 0.011478019878268242\n",
      "Epoch 7760, Loss: 0.011478019878268242\n",
      "Epoch 7770, Loss: 0.011478019878268242\n",
      "Epoch 7780, Loss: 0.011478019878268242\n",
      "Epoch 7790, Loss: 0.011478019878268242\n",
      "Epoch 7800, Loss: 0.011478019878268242\n",
      "Epoch 7810, Loss: 0.011478020809590816\n",
      "Epoch 7820, Loss: 0.011478020809590816\n",
      "Epoch 7830, Loss: 0.011478021740913391\n",
      "Epoch 7840, Loss: 0.01147802546620369\n",
      "Epoch 7850, Loss: 0.01147802546620369\n",
      "Epoch 7860, Loss: 0.01147802360355854\n",
      "Epoch 7870, Loss: 0.01147802360355854\n",
      "Epoch 7880, Loss: 0.01147802360355854\n",
      "Epoch 7890, Loss: 0.01147802360355854\n",
      "Epoch 7900, Loss: 0.01147802360355854\n",
      "Epoch 7910, Loss: 0.01147802360355854\n",
      "Epoch 7920, Loss: 0.01147802360355854\n",
      "Epoch 7930, Loss: 0.01147802360355854\n",
      "Epoch 7940, Loss: 0.01147802360355854\n",
      "Epoch 7950, Loss: 0.01147802360355854\n",
      "Epoch 7960, Loss: 0.01147802360355854\n",
      "Epoch 7970, Loss: 0.01147802360355854\n",
      "Epoch 7980, Loss: 0.01147802360355854\n",
      "Epoch 7990, Loss: 0.01147802360355854\n",
      "Epoch 8000, Loss: 0.01147802360355854\n",
      "Epoch 8010, Loss: 0.01147802360355854\n",
      "Epoch 8020, Loss: 0.01147802360355854\n",
      "Epoch 8030, Loss: 0.01147802360355854\n",
      "Epoch 8040, Loss: 0.01147802360355854\n",
      "Epoch 8050, Loss: 0.01147802360355854\n",
      "Epoch 8060, Loss: 0.01147802360355854\n",
      "Epoch 8070, Loss: 0.01147802360355854\n",
      "Epoch 8080, Loss: 0.01147802360355854\n",
      "Epoch 8090, Loss: 0.01147802360355854\n",
      "Epoch 8100, Loss: 0.01147802360355854\n",
      "Epoch 8110, Loss: 0.01147802360355854\n",
      "Epoch 8120, Loss: 0.01147802360355854\n",
      "Epoch 8130, Loss: 0.01147802360355854\n",
      "Epoch 8140, Loss: 0.01147802360355854\n",
      "Epoch 8150, Loss: 0.01147802360355854\n",
      "Epoch 8160, Loss: 0.01147802360355854\n",
      "Epoch 8170, Loss: 0.01147802360355854\n",
      "Epoch 8180, Loss: 0.01147802360355854\n",
      "Epoch 8190, Loss: 0.01147802360355854\n",
      "Epoch 8200, Loss: 0.01147802360355854\n",
      "Epoch 8210, Loss: 0.01147802360355854\n",
      "Epoch 8220, Loss: 0.01147802360355854\n",
      "Epoch 8230, Loss: 0.01147802360355854\n",
      "Epoch 8240, Loss: 0.01147802360355854\n",
      "Epoch 8250, Loss: 0.01147802360355854\n",
      "Epoch 8260, Loss: 0.01147802360355854\n",
      "Epoch 8270, Loss: 0.01147802360355854\n",
      "Epoch 8280, Loss: 0.01147802360355854\n",
      "Epoch 8290, Loss: 0.01147802360355854\n",
      "Epoch 8300, Loss: 0.01147802360355854\n",
      "Epoch 8310, Loss: 0.01147802360355854\n",
      "Epoch 8320, Loss: 0.01147802360355854\n",
      "Epoch 8330, Loss: 0.01147802360355854\n",
      "Epoch 8340, Loss: 0.01147802360355854\n",
      "Epoch 8350, Loss: 0.01147802360355854\n",
      "Epoch 8360, Loss: 0.01147802360355854\n",
      "Epoch 8370, Loss: 0.01147802360355854\n",
      "Epoch 8380, Loss: 0.01147802360355854\n",
      "Epoch 8390, Loss: 0.01147802360355854\n",
      "Epoch 8400, Loss: 0.01147802360355854\n",
      "Epoch 8410, Loss: 0.01147802360355854\n",
      "Epoch 8420, Loss: 0.01147802360355854\n",
      "Epoch 8430, Loss: 0.01147802360355854\n",
      "Epoch 8440, Loss: 0.01147802360355854\n",
      "Epoch 8450, Loss: 0.01147802360355854\n",
      "Epoch 8460, Loss: 0.01147802360355854\n",
      "Epoch 8470, Loss: 0.01147802360355854\n",
      "Epoch 8480, Loss: 0.01147802360355854\n",
      "Epoch 8490, Loss: 0.01147802360355854\n",
      "Epoch 8500, Loss: 0.01147802360355854\n",
      "Epoch 8510, Loss: 0.01147802360355854\n",
      "Epoch 8520, Loss: 0.01147802360355854\n",
      "Epoch 8530, Loss: 0.01147802360355854\n",
      "Epoch 8540, Loss: 0.01147802360355854\n",
      "Epoch 8550, Loss: 0.01147802360355854\n",
      "Epoch 8560, Loss: 0.01147802360355854\n",
      "Epoch 8570, Loss: 0.01147802360355854\n",
      "Epoch 8580, Loss: 0.01147802360355854\n",
      "Epoch 8590, Loss: 0.01147802360355854\n",
      "Epoch 8600, Loss: 0.01147802360355854\n",
      "Epoch 8610, Loss: 0.01147802360355854\n",
      "Epoch 8620, Loss: 0.01147802360355854\n",
      "Epoch 8630, Loss: 0.01147802360355854\n",
      "Epoch 8640, Loss: 0.01147802360355854\n",
      "Epoch 8650, Loss: 0.01147802360355854\n",
      "Epoch 8660, Loss: 0.01147802360355854\n",
      "Epoch 8670, Loss: 0.01147802360355854\n",
      "Epoch 8680, Loss: 0.01147802360355854\n",
      "Epoch 8690, Loss: 0.01147802360355854\n",
      "Epoch 8700, Loss: 0.01147802360355854\n",
      "Epoch 8710, Loss: 0.01147802360355854\n",
      "Epoch 8720, Loss: 0.01147802360355854\n",
      "Epoch 8730, Loss: 0.01147802360355854\n",
      "Epoch 8740, Loss: 0.01147802360355854\n",
      "Epoch 8750, Loss: 0.01147802360355854\n",
      "Epoch 8760, Loss: 0.01147802360355854\n",
      "Epoch 8770, Loss: 0.01147802360355854\n",
      "Epoch 8780, Loss: 0.01147802360355854\n",
      "Epoch 8790, Loss: 0.01147802360355854\n",
      "Epoch 8800, Loss: 0.01147802360355854\n",
      "Epoch 8810, Loss: 0.01147802360355854\n",
      "Epoch 8820, Loss: 0.01147802360355854\n",
      "Epoch 8830, Loss: 0.01147802360355854\n",
      "Epoch 8840, Loss: 0.01147802360355854\n",
      "Epoch 8850, Loss: 0.01147802360355854\n",
      "Epoch 8860, Loss: 0.01147802360355854\n",
      "Epoch 8870, Loss: 0.01147802360355854\n",
      "Epoch 8880, Loss: 0.01147802360355854\n",
      "Epoch 8890, Loss: 0.01147802360355854\n",
      "Epoch 8900, Loss: 0.01147802360355854\n",
      "Epoch 8910, Loss: 0.01147802360355854\n",
      "Epoch 8920, Loss: 0.01147802360355854\n",
      "Epoch 8930, Loss: 0.01147802360355854\n",
      "Epoch 8940, Loss: 0.01147802360355854\n",
      "Epoch 8950, Loss: 0.01147802360355854\n",
      "Epoch 8960, Loss: 0.01147802360355854\n",
      "Epoch 8970, Loss: 0.01147802360355854\n",
      "Epoch 8980, Loss: 0.01147802360355854\n",
      "Epoch 8990, Loss: 0.01147802360355854\n",
      "Epoch 9000, Loss: 0.01147802360355854\n",
      "Epoch 9010, Loss: 0.01147802360355854\n",
      "Epoch 9020, Loss: 0.01147802360355854\n",
      "Epoch 9030, Loss: 0.01147802360355854\n",
      "Epoch 9040, Loss: 0.01147802360355854\n",
      "Epoch 9050, Loss: 0.01147802360355854\n",
      "Epoch 9060, Loss: 0.01147802360355854\n",
      "Epoch 9070, Loss: 0.01147802360355854\n",
      "Epoch 9080, Loss: 0.01147802360355854\n",
      "Epoch 9090, Loss: 0.01147802360355854\n",
      "Epoch 9100, Loss: 0.01147802360355854\n",
      "Epoch 9110, Loss: 0.01147802360355854\n",
      "Epoch 9120, Loss: 0.01147802360355854\n",
      "Epoch 9130, Loss: 0.01147802360355854\n",
      "Epoch 9140, Loss: 0.01147802360355854\n",
      "Epoch 9150, Loss: 0.01147802360355854\n",
      "Epoch 9160, Loss: 0.01147802360355854\n",
      "Epoch 9170, Loss: 0.01147802360355854\n",
      "Epoch 9180, Loss: 0.01147802360355854\n",
      "Epoch 9190, Loss: 0.01147802360355854\n",
      "Epoch 9200, Loss: 0.01147802360355854\n",
      "Epoch 9210, Loss: 0.01147802360355854\n",
      "Epoch 9220, Loss: 0.01147802360355854\n",
      "Epoch 9230, Loss: 0.01147802360355854\n",
      "Epoch 9240, Loss: 0.01147802360355854\n",
      "Epoch 9250, Loss: 0.01147802360355854\n",
      "Epoch 9260, Loss: 0.01147802360355854\n",
      "Epoch 9270, Loss: 0.01147802360355854\n",
      "Epoch 9280, Loss: 0.01147802360355854\n",
      "Epoch 9290, Loss: 0.01147802360355854\n",
      "Epoch 9300, Loss: 0.01147802360355854\n",
      "Epoch 9310, Loss: 0.01147802360355854\n",
      "Epoch 9320, Loss: 0.01147802360355854\n",
      "Epoch 9330, Loss: 0.01147802360355854\n",
      "Epoch 9340, Loss: 0.01147802360355854\n",
      "Epoch 9350, Loss: 0.01147802360355854\n",
      "Epoch 9360, Loss: 0.01147802360355854\n",
      "Epoch 9370, Loss: 0.01147802360355854\n",
      "Epoch 9380, Loss: 0.01147802360355854\n",
      "Epoch 9390, Loss: 0.01147802360355854\n",
      "Epoch 9400, Loss: 0.01147802360355854\n",
      "Epoch 9410, Loss: 0.01147802360355854\n",
      "Epoch 9420, Loss: 0.01147802360355854\n",
      "Epoch 9430, Loss: 0.01147802360355854\n",
      "Epoch 9440, Loss: 0.01147802360355854\n",
      "Epoch 9450, Loss: 0.01147802360355854\n",
      "Epoch 9460, Loss: 0.01147802360355854\n",
      "Epoch 9470, Loss: 0.01147802360355854\n",
      "Epoch 9480, Loss: 0.01147802360355854\n",
      "Epoch 9490, Loss: 0.01147802360355854\n",
      "Epoch 9500, Loss: 0.01147802360355854\n",
      "Epoch 9510, Loss: 0.01147802360355854\n",
      "Epoch 9520, Loss: 0.01147802360355854\n",
      "Epoch 9530, Loss: 0.01147802360355854\n",
      "Epoch 9540, Loss: 0.01147802360355854\n",
      "Epoch 9550, Loss: 0.01147802360355854\n",
      "Epoch 9560, Loss: 0.01147802360355854\n",
      "Epoch 9570, Loss: 0.01147802360355854\n",
      "Epoch 9580, Loss: 0.01147802360355854\n",
      "Epoch 9590, Loss: 0.01147802360355854\n",
      "Epoch 9600, Loss: 0.01147802360355854\n",
      "Epoch 9610, Loss: 0.01147802360355854\n",
      "Epoch 9620, Loss: 0.01147802360355854\n",
      "Epoch 9630, Loss: 0.01147802360355854\n",
      "Epoch 9640, Loss: 0.01147802360355854\n",
      "Epoch 9650, Loss: 0.01147802360355854\n",
      "Epoch 9660, Loss: 0.01147802360355854\n",
      "Epoch 9670, Loss: 0.01147802360355854\n",
      "Epoch 9680, Loss: 0.01147802360355854\n",
      "Epoch 9690, Loss: 0.01147802360355854\n",
      "Epoch 9700, Loss: 0.01147802360355854\n",
      "Epoch 9710, Loss: 0.01147802360355854\n",
      "Epoch 9720, Loss: 0.01147802360355854\n",
      "Epoch 9730, Loss: 0.01147802360355854\n",
      "Epoch 9740, Loss: 0.01147802360355854\n",
      "Epoch 9750, Loss: 0.01147802360355854\n",
      "Epoch 9760, Loss: 0.01147802360355854\n",
      "Epoch 9770, Loss: 0.01147802360355854\n",
      "Epoch 9780, Loss: 0.01147802360355854\n",
      "Epoch 9790, Loss: 0.01147802360355854\n",
      "Epoch 9800, Loss: 0.01147802360355854\n",
      "Epoch 9810, Loss: 0.01147802360355854\n",
      "Epoch 9820, Loss: 0.01147802360355854\n",
      "Epoch 9830, Loss: 0.01147802360355854\n",
      "Epoch 9840, Loss: 0.01147802360355854\n",
      "Epoch 9850, Loss: 0.01147802360355854\n",
      "Epoch 9860, Loss: 0.01147802360355854\n",
      "Epoch 9870, Loss: 0.01147802360355854\n",
      "Epoch 9880, Loss: 0.01147802360355854\n",
      "Epoch 9890, Loss: 0.01147802360355854\n",
      "Epoch 9900, Loss: 0.01147802360355854\n",
      "Epoch 9910, Loss: 0.01147802360355854\n",
      "Epoch 9920, Loss: 0.01147802360355854\n",
      "Epoch 9930, Loss: 0.01147802360355854\n",
      "Epoch 9940, Loss: 0.01147802360355854\n",
      "Epoch 9950, Loss: 0.01147802360355854\n",
      "Epoch 9960, Loss: 0.01147802360355854\n",
      "Epoch 9970, Loss: 0.01147802360355854\n",
      "Epoch 9980, Loss: 0.01147802360355854\n",
      "Epoch 9990, Loss: 0.01147802360355854\n"
     ]
    }
   ],
   "source": [
    "y_true = (x@A+ 0.1* torch.randn(batch_size, out_dim).to(device=device, dtype=dtype))\n",
    "optim = torch.optim.SGD(torch_linear.parameters(), lr=0.01)\n",
    "x.shape\n",
    "\n",
    "for epoch in range(10_000):\n",
    "    y_pred = torch_linear(x)\n",
    "\n",
    "\n",
    "    loss = nn.MSELoss()(y_pred, y_true)\n",
    "\n",
    "    torch_linear.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a22b31e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.9613e+00, 3.0499e+00, 3.9282e-03, 9.8797e-01, 3.9679e+00]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_linear.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4a82dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.9613e+00],\n",
       "        [3.0499e+00],\n",
       "        [3.8978e-03],\n",
       "        [9.8793e-01],\n",
       "        [3.9679e+00]], device='mps:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x.T @ x).inverse() @ x.T @ y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32cc885",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-notebook",
   "language": "python",
   "name": "ml-notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
