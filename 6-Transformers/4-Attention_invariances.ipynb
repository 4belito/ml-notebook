{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6be66df7",
   "metadata": {},
   "source": [
    "# Attention Permutation Invariances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e371ca",
   "metadata": {},
   "source": [
    "## Key-Value Permutation Invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beda509",
   "metadata": {},
   "source": [
    "If $\\pi_r(M)$ denotes an arbitrary permutation over the rows of a matrix $M$, then\n",
    "$$\\text{Attention}(Q,\\pi_r(K),\\pi_r(V))=\\text{Attention}(Q,K,V) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df38027",
   "metadata": {},
   "source": [
    "Proof:\n",
    "\n",
    "Notice that $\\pi_r(M)$ can be written as $\\pi_r(M) = R_{\\pi}M$ for some permutation matrix $R_{\\pi}$. A permutation matrix is a square binary matrix that has exactly one entry of 1 in each row and each column with all other entries 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2f17d3",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Attention}(Q,\\pi_r(K),\\pi_r(V)) & = \\text{Attention}(Q,R_{\\pi}K,R_{\\pi}V),\\\\\n",
    "& = \\text{softmax}\\left(\\frac{Q(R_{\\pi}K)^T}{\\sqrt{d_k}}\\right)R_{\\pi}V,\\\\\n",
    "& = \\text{softmax}\\left(\\frac{QK^TR_{\\pi}^{T}}{\\sqrt{d_k}}\\right)R_{\\pi}V,\\\\\n",
    "& = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}R_{\\pi}^{T}\\right)R_{\\pi}V,\\\\\n",
    "& = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)R_{\\pi}^{T}R_{\\pi}V,\\\\\n",
    "& = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V,\\\\\n",
    "& = \\text{Attention}(Q,K,V),\n",
    "\\end{align*}\n",
    "$$\n",
    "where in the fifth equality we used that postmultiply by a permutation matrix is a column permutation and softmax is permutation equivariant. Notice that for any permutation $\\pi$ we have \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{softmax}\\left(\\pi(x)\\right)& = \\text{softmax}\\left(x_{\\pi(1)},x_{\\pi(2)},\\dots,x_{\\pi(n)}\\right),\\\\\n",
    "&=\\frac{\\left(e^{x_{\\pi(1)}},e^{x_{\\pi(2)}},\\dots,e^{x_{\\pi(n)}}\\right)}{\\sum_{i=1}^n e^{x_{\\pi(i)}}},\\\\\n",
    "&=\\frac{\\left(e^{x_{\\pi(1)}},e^{x_{\\pi(2)}},\\dots,e^{x_{\\pi(n)}}\\right)}{\\sum_{i=1}^n e^{x_{i}}},\\\\\n",
    "&=\\frac{\\pi\\left(e^{x_{1}},e^{x_{2}},\\dots,e^{x_{n}}\\right)}{\\sum_{i=1}^n e^{x_{i}}},\\\\\n",
    "&=\\pi\\left(\\frac{\\left(e^{x_{1}},e^{x_{2}},\\dots,e^{x_{n}}\\right)}{\\sum_{i=1}^n e^{x_{i}}}\\right),\\\\\n",
    "&=\\pi\\left(\\text{softmax}(x)\\right),\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "and $\\text{softmax}$ is applied to each row. So for $A = [a_1|a_2|\\dots|a_n]^T = \\frac{QK^T}{\\sqrt{d_k}}$, $f=\\text{softmax}$, and a column permutation $\\pi_c(A) = [\\pi(a_1)|\\pi(a_2)|\\dots|\\pi(a_n)]^T = AC_{\\pi}$, we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(AC_{\\pi}) & = f([\\pi(a_1)|\\pi(a_2)|\\dots|\\pi(a_n)]^T),\\\\\n",
    " & = ([f(\\pi(a_1))|f(\\pi(a_2))|\\dots|f(\\pi(a_n))]^T),\\\\\n",
    "& = ([\\pi(f(a_1))|\\pi(f(a_2))|\\dots|\\pi(f(a_n))]^T),\\\\\n",
    "& = \\pi_c\\left([f(a_1)|f(a_2)|\\dots|f(a_n)]^T\\right),\\\\\n",
    "& = \\pi_c\\left(f(A)\\right),\\\\\n",
    "& =f(A)C_{\\pi}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dd40a7",
   "metadata": {},
   "source": [
    "## Permutation Equivariance of Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f3a2f6",
   "metadata": {},
   "source": [
    "If $\\pi_r$ and $\\pi_r$ denote arbitrary permutations over the rows of a matrix, then\n",
    "$$\\text{Attention}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))=\\pi_r\\left(\\text{Attention}(Q,K,V)\\right). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed7bf39",
   "metadata": {},
   "source": [
    "Proof:\n",
    "Consider  $\\pi_r(M) = R_{\\pi}M$ for some permutation matrix $R_{\\pi}$. From the previous result we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Attention}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V)) & = \\text{Attention}(\\pi_r(Q),K,V),\\\\\n",
    "&= \\text{Attention}(R_{\\pi}Q,K,V),\\\\\n",
    "& = \\text{softmax}\\left(\\frac{(R_{\\pi}Q)K^T}{\\sqrt{d_k}}\\right)V,\\\\\n",
    "& = \\text{softmax}\\left(R_{\\pi}\\frac{QK^T}{\\sqrt{d_k}}\\right)V,\\\\\n",
    "& = R_{\\pi}\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V,\\\\\n",
    "& = \\pi_r\\left(\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\\right),\\\\\n",
    "& = \\pi_r\\left(\\text{Attention}(Q,K,V) \\right),\n",
    "\\end{align*}\n",
    "$$\n",
    "where in the fifth equality we have used that the softmax funtion is applied individually for each row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8ac724",
   "metadata": {},
   "source": [
    "## Permutation Equivariance of Attention with weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb95af4",
   "metadata": {},
   "source": [
    "If Attention with weights has no bias (linear projections instead of affine projections), the Attention is permutation equivariant. If $\\pi_r$ and $\\sigma_r$ denote arbitrary permutations over the rows of a matrix, then\n",
    "$$\\text{Attention}_{\\mathcal{W}}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))=\\pi_r\\left(\\text{Attention}_{\\mathcal{W}}(Q,K,V)\\right) $$\n",
    "where \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{W} &= \\{W_q,b_q,W_k,b_k,W_v,b_v\\},\\\\\n",
    "b_q &= 0_{\\mathbb{R}^{d_k}},\\\\\n",
    "b_k &= 0_{\\mathbb{R}^{d_k}},\\\\\n",
    "b_v &= 0_{\\mathbb{R}^{d_v}},\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc062a9",
   "metadata": {},
   "source": [
    "Proof: Consider  $\\pi_r(M) = R_{\\pi}M$ and  $\\sigma_r(M) = R_{\\sigma}M$for some permutation matrix $R_{\\pi}$ and $R_{\\sigma}$ respectively. From the previous result we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Attention}_{\\mathcal{W}}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))\n",
    "&=\\text{Attention}(\\pi_r(Q)W_q,\\sigma_r(K)W_k,\\sigma_r(V)W_v),\\\\\n",
    "&=\\text{Attention}((R_{\\pi}Q)W_q,(R_{\\sigma}K)W_k,(R_{\\sigma}V)W_v),\\\\\n",
    "&=\\text{Attention}(R_{\\pi}(QW_q),R_{\\sigma}(KW_k),R_{\\sigma}(VW_v)),\\\\\n",
    "&=R_{\\pi}\\text{Attention}(QW_q,KW_k,VW_v),\\\\\n",
    "&=R_{\\pi}\\text{Attention}_{\\mathcal{W}}(Q,K,V),\\\\\n",
    "&=\\pi_r\\left(\\text{Attention}_{\\mathcal{W}}(Q,K,V)\\right).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514d147c",
   "metadata": {},
   "source": [
    "## Permutation Equivariance of Self-Attention (with weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfd3879",
   "metadata": {},
   "source": [
    "If Self-Attention with weights has no bias (linear projections instead of affine projections), the SelfAttention is permutation equivariant. If $\\pi_r(M)$ denotes an arbitrary permutation over the rows of a matrix $M$, then\n",
    "$$\\text{SelfAttention}_{\\mathcal{W}}(\\pi_r(X))=\\pi_r\\left(\\text{SelfAttention}_{\\mathcal{W}}(X)\\right) $$\n",
    "where \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{W} &= \\{W_q,b_q,W_k,b_k,W_v,b_v\\},\\\\\n",
    "b_q &= 0_{\\mathbb{R}^{d_k}},\\\\\n",
    "b_k &= 0_{\\mathbb{R}^{d_k}},\\\\\n",
    "b_v &= 0_{\\mathbb{R}^{d_v}},\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94ee979",
   "metadata": {},
   "source": [
    "Proof: From the previous result we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{SelfAttention}_{\\mathcal{W}}(\\pi_r(X))\n",
    "&=\\text{Attention}_{\\mathcal{W}}(\\pi_r(X),\\pi_r(X),\\pi_r(X)),\\\\\n",
    "&=\\pi_r\\left(\\text{Attention}_{\\mathcal{W}}(X,X,X)\\right),\\\\\n",
    "&=\\pi_r\\left(\\text{SelfAttention}_{\\mathcal{W}}(X)\\right).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72c025e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Permutation Equivariance of Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4b5e03",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "If Multi-Head Attention has no bias (linear projections instead of affine projections), the Multi-Head Attention is permutation equivariant. If $\\pi_r$ and $\\sigma_r$ denote arbitrary permutations over the rows of a matrix, then\n",
    "$$\\text{MultiHeadAttention}_{_{\\{\\mathcal{W}_{i}\\}_{i=1}^h\\cup \\{W_o,b_o\\}}}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))=\\pi_r\\left(\\text{MultiHeadAttention}_{_{\\{\\mathcal{W}_{i}\\}_{i=1}^h\\cup \\{W_o,b_o\\}}}(Q,K,V)\\right) $$\n",
    "where \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{W_i} &= \\{W_{q,i},b_{q,i},W_{k,i},b_{k,i},W_{v,i},b_{v,i}\\},\\quad i = 1,2,\\dots,h,\\\\\n",
    "b_{q,i} &= 0_{\\mathbb{R}^{d_k}},\\quad i = 1,2,\\dots,h,\\\\\n",
    "b_{k,i} &= 0_{\\mathbb{R}^{d_k}},\\quad  i = 1,2,\\dots,h,\\\\\n",
    "b_{v,i} &= 0_{\\mathbb{R}^{d_v}},\\quad i = 1,2,\\dots,h,\\\\\n",
    "b_{0} &= 0_{\\mathbb{R}^{d_o}}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b6a2bc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Proof:  Consider  $\\pi_r(M) = R_{\\pi}M$ and $\\sigma_r(M) = R_{\\sigma}M$ for permutation matrices $R_{\\pi}$ and $R_{\\sigma}$ respectively. From the Attention equivariance we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{MultiHeadAttention}_{_{\\{\\mathcal{W}_{i}\\}_{i=1}^h\\cup \\{W_o,b_o\\}}}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\text{Attention}_{\\mathcal{W}_{1}}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))|\n",
    "\\text{Attention}_{\\mathcal{W}_{2}}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))|\n",
    "\\dots|\n",
    "\\text{Attention}_{\\mathcal{W}_{h}}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))\n",
    "\\end{pmatrix}W_o,\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\pi_r\\left(\\text{Attention}_{\\mathcal{W}_{1}}(Q,K,V)\\right)|\n",
    "\\pi_r\\left(\\text{Attention}_{\\mathcal{W}_{2}}(Q,K,V)\\right)|\n",
    "\\dots|\n",
    "\\pi_r\\left(\\text{Attention}_{\\mathcal{W}_{h}}(Q,K,V)\\right)\n",
    "\\end{pmatrix}W_o,\\\\\n",
    "&=\\begin{pmatrix}\n",
    "R_{\\pi}\\text{Attention}_{\\mathcal{W}_{1}}(Q,K,V)|\n",
    "R_{\\pi}\\text{Attention}_{\\mathcal{W}_{2}}(Q,K,V)|\n",
    "\\dots|\n",
    "R_{\\pi}\\text{Attention}_{\\mathcal{W}_{h}}(Q,K,V)\n",
    "\\end{pmatrix}W_o,\\\\\n",
    "&=R_{\\pi}\\begin{pmatrix}\n",
    "\\text{Attention}_{\\mathcal{W}_{1}}(Q,K,V)|\n",
    "\\text{Attention}_{\\mathcal{W}_{2}}(Q,K,V)|\n",
    "\\dots|\n",
    "\\text{Attention}_{\\mathcal{W}_{h}}(Q,K,V)\n",
    "\\end{pmatrix}W_o,\\\\\n",
    "&=R_{\\pi}\\text{MultiHeadAttention}_{_{\\{\\mathcal{W}_{i}\\}_{i=1}^h\\cup \\{W_o,b_o\\}}}(Q,K,V),\\\\\n",
    "&=\\pi_r\\left(\\text{MultiHeadAttention}_{_{\\{\\mathcal{W}_{i}\\}_{i=1}^h\\cup \\{W_o,b_o\\}}}(Q,K,V)\\right).\n",
    "\\end{align*}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
