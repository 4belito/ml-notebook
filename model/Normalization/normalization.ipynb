{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61f435b4",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "[link](https://ut.philkr.net/deeplearning/residuals_and_normalizations/normalizations_in_pytorch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25c6130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b84dab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNoBIas(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLPNoBIas, self).__init__()\n",
    "        layers = [nn.Flatten()]\n",
    "        in_dim = input_dim\n",
    "        for h_dim in hidden_dim:\n",
    "            layers.append(nn.Linear(in_dim, h_dim, bias=False))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_dim = h_dim\n",
    "        layers.append(nn.Linear(in_dim, output_dim, bias=False))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b6234d",
   "metadata": {},
   "source": [
    "## Vanashing activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11b95735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_norm = 13.235665321350098\n",
      "out_norm = 5.109479904174805\n",
      "out_norm = 2.2417287826538086\n",
      "out_norm = 0.8878068923950195\n",
      "out_norm = 0.3714502453804016\n",
      "out_norm = 0.1554199755191803\n",
      "out_norm = 0.05945925787091255\n",
      "out_norm = 0.024814074859023094\n",
      "out_norm = 0.009948276914656162\n",
      "out_norm = 0.00434709619730711\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 28, 28)\n",
    "for i in range(10):\n",
    "    net = MLPNoBIas(input_dim=28*28, hidden_dim=i*[512], output_dim=100)\n",
    "    out_norm = net(x).norm().item()\n",
    "    print(f'{out_norm = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eda7ab3",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "174e2109",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBNPre(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        layers = [nn.Flatten()]\n",
    "        in_dim = input_dim\n",
    "        for h_dim in hidden_dim:\n",
    "            layers.append(nn.BatchNorm1d(in_dim,affine=False)) # bias learned by LinearLayer\n",
    "            layers.append(nn.Linear(in_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_dim = h_dim\n",
    "        layers.append(nn.Linear(in_dim, output_dim, bias=False))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0055bd9e",
   "metadata": {},
   "source": [
    "**Note:** We should always learn the bias before the ReLU. Otherwise, the ReLU will set half of the activations to zero, which limits the expressiveness of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9e3c62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_norm = 12.1582612991333\n",
      "out_norm = 5.250041961669922\n",
      "out_norm = 5.146596908569336\n",
      "out_norm = 5.144865989685059\n",
      "out_norm = 5.4364705085754395\n",
      "out_norm = 5.265069484710693\n",
      "out_norm = 5.387322425842285\n",
      "out_norm = 5.271307468414307\n",
      "out_norm = 5.063490867614746\n",
      "out_norm = 5.191336154937744\n",
      "out_norm = 5.492993354797363\n",
      "out_norm = 5.130956172943115\n",
      "out_norm = 5.244418144226074\n",
      "out_norm = 5.2606587409973145\n",
      "out_norm = 5.013553142547607\n",
      "out_norm = 5.314823627471924\n",
      "out_norm = 5.065823078155518\n",
      "out_norm = 5.035289764404297\n",
      "out_norm = 5.108409404754639\n",
      "out_norm = 5.232163429260254\n",
      "out_norm = 5.492886543273926\n",
      "out_norm = 5.6458210945129395\n",
      "out_norm = 5.216387748718262\n",
      "out_norm = 5.3886566162109375\n",
      "out_norm = 5.405171871185303\n",
      "out_norm = 5.305245876312256\n",
      "out_norm = 5.2440714836120605\n",
      "out_norm = 5.205931186676025\n",
      "out_norm = 5.564276218414307\n",
      "out_norm = 5.314305782318115\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 28, 28)\n",
    "for i in range(30):\n",
    "    net = MLPBNPre(input_dim=28*28, hidden_dim=i*[512], output_dim=100)\n",
    "    out_norm = net(x).norm().item()\n",
    "    print(f'{out_norm = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4acb9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBNPos(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Flatten())\n",
    "        in_dim = input_dim\n",
    "        for h_dim in hidden_dim:\n",
    "            layers.append(nn.Linear(in_dim, h_dim, bias=False))\n",
    "            layers.append(nn.BatchNorm1d(h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_dim = h_dim\n",
    "        layers.append(nn.Linear(in_dim, output_dim, bias=False))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4b8b77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_norm = 14.170045852661133\n",
      "out_norm = 8.724059104919434\n",
      "out_norm = 8.843928337097168\n",
      "out_norm = 9.060585021972656\n",
      "out_norm = 8.993568420410156\n",
      "out_norm = 8.748834609985352\n",
      "out_norm = 8.898076057434082\n",
      "out_norm = 9.477777481079102\n",
      "out_norm = 8.1068754196167\n",
      "out_norm = 9.740224838256836\n",
      "out_norm = 8.90272045135498\n",
      "out_norm = 9.097138404846191\n",
      "out_norm = 9.12496280670166\n",
      "out_norm = 8.948826789855957\n",
      "out_norm = 8.199789047241211\n",
      "out_norm = 8.707714080810547\n",
      "out_norm = 8.901817321777344\n",
      "out_norm = 9.005202293395996\n",
      "out_norm = 8.998388290405273\n",
      "out_norm = 9.329340934753418\n",
      "out_norm = 9.193537712097168\n",
      "out_norm = 9.306073188781738\n",
      "out_norm = 8.786357879638672\n",
      "out_norm = 9.39171314239502\n",
      "out_norm = 9.443123817443848\n",
      "out_norm = 9.122742652893066\n",
      "out_norm = 9.19918155670166\n",
      "out_norm = 8.831351280212402\n",
      "out_norm = 9.312227249145508\n",
      "out_norm = 9.15225601196289\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 28, 28)\n",
    "for i in range(30):\n",
    "    net = MLPBNPos(input_dim=28*28, hidden_dim=i*[512], output_dim=100)\n",
    "    out_norm = net(x).norm().item()\n",
    "    print(f'{out_norm = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d19ae99",
   "metadata": {},
   "source": [
    "## Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3532f02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBNPre(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        layers = [nn.Flatten()]\n",
    "        in_dim = input_dim\n",
    "        for h_dim in hidden_dim:\n",
    "            # BatchNorm matches current input dimension\n",
    "            layers.append(nn.LayerNorm(in_dim, elementwise_affine=False)) # bias learned by LinearLayer\n",
    "            layers.append(nn.Linear(in_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_dim = h_dim\n",
    "        layers.append(nn.Linear(in_dim, output_dim, bias=False))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bd3bfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_norm = 13.84544849395752\n",
      "out_norm = 5.214603900909424\n",
      "out_norm = 5.113377094268799\n",
      "out_norm = 5.348901748657227\n",
      "out_norm = 4.925804138183594\n",
      "out_norm = 5.6150617599487305\n",
      "out_norm = 5.550417900085449\n",
      "out_norm = 5.169072151184082\n",
      "out_norm = 5.2616376876831055\n",
      "out_norm = 4.960304260253906\n",
      "out_norm = 5.183526039123535\n",
      "out_norm = 5.386806488037109\n",
      "out_norm = 5.098806858062744\n",
      "out_norm = 5.266600608825684\n",
      "out_norm = 5.357975006103516\n",
      "out_norm = 5.1884307861328125\n",
      "out_norm = 5.391501426696777\n",
      "out_norm = 5.506072521209717\n",
      "out_norm = 5.424647331237793\n",
      "out_norm = 5.29259729385376\n",
      "out_norm = 5.61790132522583\n",
      "out_norm = 5.114840507507324\n",
      "out_norm = 5.161526203155518\n",
      "out_norm = 5.420399188995361\n",
      "out_norm = 5.113193988800049\n",
      "out_norm = 5.6424784660339355\n",
      "out_norm = 5.430173873901367\n",
      "out_norm = 5.270264625549316\n",
      "out_norm = 4.936638832092285\n",
      "out_norm = 5.396730899810791\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 28, 28)\n",
    "for i in range(30):\n",
    "    net = MLPBNPre(input_dim=28*28, hidden_dim=i*[512], output_dim=100)\n",
    "    out_norm = net(x).norm().item()\n",
    "    print(f'{out_norm = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d614241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPLNPos(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Flatten())\n",
    "        in_dim = input_dim\n",
    "        for h_dim in hidden_dim:\n",
    "            layers.append(nn.Linear(in_dim, h_dim, bias=False)) # Bias learned by LayerNorm\n",
    "            layers.append(nn.LayerNorm(h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_dim = h_dim\n",
    "        layers.append(nn.Linear(in_dim, output_dim, bias=False))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ba87855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_norm = 13.674946784973145\n",
      "out_norm = 9.108837127685547\n",
      "out_norm = 8.960722923278809\n",
      "out_norm = 8.448254585266113\n",
      "out_norm = 9.74319076538086\n",
      "out_norm = 8.173799514770508\n",
      "out_norm = 9.221722602844238\n",
      "out_norm = 9.078075408935547\n",
      "out_norm = 8.76062297821045\n",
      "out_norm = 8.310615539550781\n",
      "out_norm = 10.636826515197754\n",
      "out_norm = 9.499361991882324\n",
      "out_norm = 8.590950965881348\n",
      "out_norm = 10.054750442504883\n",
      "out_norm = 8.244994163513184\n",
      "out_norm = 9.755012512207031\n",
      "out_norm = 8.789600372314453\n",
      "out_norm = 9.30587387084961\n",
      "out_norm = 10.1161527633667\n",
      "out_norm = 8.145820617675781\n",
      "out_norm = 8.818049430847168\n",
      "out_norm = 8.688199043273926\n",
      "out_norm = 9.163427352905273\n",
      "out_norm = 8.954636573791504\n",
      "out_norm = 8.770222663879395\n",
      "out_norm = 8.566849708557129\n",
      "out_norm = 8.197632789611816\n",
      "out_norm = 9.962224006652832\n",
      "out_norm = 9.507499694824219\n",
      "out_norm = 9.523523330688477\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 28, 28)\n",
    "for i in range(30):\n",
    "    net = MLPLNPos(input_dim=28*28, hidden_dim=i*[512], output_dim=100)\n",
    "    out_norm = net(x).norm().item()\n",
    "    print(f'{out_norm = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f3a732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
