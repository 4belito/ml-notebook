{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "154968d9",
   "metadata": {},
   "source": [
    "# Multi-Head Attention\n",
    "[link](https://ut.philkr.net/deeplearning/transformers/multihead_attention_in_pytorch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b5e26e",
   "metadata": {},
   "source": [
    "## Definition: Attention\n",
    "\n",
    "For a set of key-value pairs $\\{(k_i,v_i)\\}_{i=1}^N \\in \\mathbb{R}^{d_k\\times d_v}$ and another set of queries $\\{q_j\\}_{j=1}^M \\in \\mathbb{R}^{d_k}$, atention returns the \"expected\" value $o_j \\in \\mathbb{R}^{d_v}$ for each querry $q_j, \\ j=1,2,\\dots,M$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ee4d4",
   "metadata": {},
   "source": [
    "Inputs\n",
    "* a set of queries $Q = [q_1|q_2|\\dots|q_M]^T \\in \\mathbb{R}^{M \\times d_k}$\n",
    "* a set of keys $K = [k_1|k_2|\\dots|k_N]^T \\in \\mathbb{R}^{N \\times d_k}$\n",
    "* a set of values $V = [v_1|v_2|\\dots|v_N]^T \\in \\mathbb{R}^{N \\times d_v}$\n",
    "\n",
    "Output: $O= [o_1|o_2|\\dots|o_M]^T \\in\\mathbb{R}^{M \\times d_v}$\n",
    "$$ \\text{Attention}(Q,K,V) = O  = \\alpha V, \\quad \\text{ where } \\quad \\alpha = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) \\in \\mathbb{R}^{M\\times N}$$\n",
    "$$ o_{i}  =  \\sum_{j=1}^N\\alpha_{i,j}v_j\\, \\quad \\text{ where } \\quad  \\alpha_{i,j}=\\frac{e^{\\frac{q_i^{T}k_j}{\\sqrt{d_k}}}}{\\sum_{l=1}^Ne^{\\frac{q_i^{T}k_l}{\\sqrt{d_k}}}}$$\n",
    "\n",
    "\n",
    "where $\\text{softmax}$ is applied to per row, so each row of $\\alpha$ sums to one.  If we denote by $v(q_i)$ the random variable \"value of the querry $q_i$\", for $i=1,2,\\dots,M$, then the induced probability of $v(q_i)$ is \n",
    "$$p(v(q_i) = v_j)=\\alpha_{i,j}, \\quad j=1,2,\\dots,N.$$\n",
    "\n",
    "Notice that $\\sum_{j=1}^N\\alpha_{i,j}=1, \\ i =1,2,\\dots,M$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283e2ee1",
   "metadata": {},
   "source": [
    "**Note:** The value $p(v(q_i) = v_j)=\\alpha_{i,j}$ is ussually interpreted as how much attention the querry $q_i$ pays to value $v_j$. So the \"attention\" of $q_i$ is partitioned along the values $v_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297bb21b",
   "metadata": {},
   "source": [
    "### Property: Key-Value Permutation Invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90689ed",
   "metadata": {},
   "source": [
    "If $\\pi_r(M)$ denotes an arbitrary permutation over the rows of a matrix $M$, then\n",
    "$$\\text{Attention}(Q,\\pi_r(K),\\pi_r(V))=\\text{Attention}(Q,K,V) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b0ff17",
   "metadata": {},
   "source": [
    "Proof:\n",
    "\n",
    "Notice that $\\pi_r(M)$ can be written as $\\pi_r(M) = R_{\\pi}M$ for some permutation matrix $R_{\\pi}$. A permutation matrix is a square binary matrix that has exactly one entry of 1 in each row and each column with all other entries 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7860acf8",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Attention}(Q,\\pi_r(K),\\pi_r(V)) & = \\text{Attention}(Q,R_{\\pi}K,R_{\\pi}V),\\\\\n",
    "& = \\text{softmax}\\left(\\frac{Q(R_{\\pi}K)^T}{\\sqrt{d_k}}\\right)R_{\\pi}V,\\\\\n",
    "& = \\text{softmax}\\left(\\frac{QK^TR_{\\pi}^{T}}{\\sqrt{d_k}}\\right)R_{\\pi}V,\\\\\n",
    "& = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}R_{\\pi}^{T}\\right)R_{\\pi}V,\\\\\n",
    "& = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)R_{\\pi}^{T}R_{\\pi}V,\\\\\n",
    "& = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V,\\\\\n",
    "& = \\text{Attention}(Q,K,V),\n",
    "\\end{align*}\n",
    "$$\n",
    "where in the fifth equality we used that postmultiply by a permutation matrix is a column permutation and softmax is permutation equivariant. Notice that for any permutation $\\pi$ we have \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{softmax}\\left(\\pi(x)\\right)& = \\text{softmax}\\left(x_{\\pi(1)},x_{\\pi(2)},\\dots,x_{\\pi(n)}\\right),\\\\\n",
    "&=\\frac{\\left(e^{x_{\\pi(1)}},e^{x_{\\pi(2)}},\\dots,e^{x_{\\pi(n)}}\\right)}{\\sum_{i=1}^n e^{x_{\\pi(i)}}},\\\\\n",
    "&=\\frac{\\left(e^{x_{\\pi(1)}},e^{x_{\\pi(2)}},\\dots,e^{x_{\\pi(n)}}\\right)}{\\sum_{i=1}^n e^{x_{i}}},\\\\\n",
    "&=\\frac{\\pi\\left(e^{x_{1}},e^{x_{2}},\\dots,e^{x_{n}}\\right)}{\\sum_{i=1}^n e^{x_{i}}},\\\\\n",
    "&=\\pi\\left(\\frac{\\left(e^{x_{1}},e^{x_{2}},\\dots,e^{x_{n}}\\right)}{\\sum_{i=1}^n e^{x_{i}}}\\right),\\\\\n",
    "&=\\pi\\left(\\text{softmax}(x)\\right),\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "and $\\text{softmax}$ is applied to each row. So for $A = [a_1|a_2|\\dots|a_n]^T = \\frac{QK^T}{\\sqrt{d_k}}$, $f=\\text{softmax}$, and a column permutation $\\pi_c(A) = [\\pi(a_1)|\\pi(a_2)|\\dots|\\pi(a_n)]^T = AC_{\\pi}$, we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(AC_{\\pi}) & = f([\\pi(a_1)|\\pi(a_2)|\\dots|\\pi(a_n)]^T),\\\\\n",
    " & = ([f(\\pi(a_1))|f(\\pi(a_2))|\\dots|f(\\pi(a_n))]^T),\\\\\n",
    "& = ([\\pi(f(a_1))|\\pi(f(a_2))|\\dots|\\pi(f(a_n))]^T),\\\\\n",
    "& = \\pi_c\\left([f(a_1)|f(a_2)|\\dots|f(a_n)]^T\\right),\\\\\n",
    "& = \\pi_c\\left(f(A)\\right),\\\\\n",
    "& =f(A)C_{\\pi}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae671a5f",
   "metadata": {},
   "source": [
    "### Property: Attention Permutation Equivariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49db8091",
   "metadata": {},
   "source": [
    "If $\\pi_r$ and $\\sigma_r$ denote arbitrary permutations over the rows of a matrix, then\n",
    "$$\\text{Attention}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))=\\pi_r\\left(\\text{Attention}(Q,K,V)\\right). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63084601",
   "metadata": {},
   "source": [
    "Proof:\n",
    "Consider  $\\pi_r(M) = R_{\\pi}M$ for some permutation matrix $R_{\\pi}$. From the previous result we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Attention}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V)) & = \\text{Attention}(\\pi_r(Q),K,V),\\\\\n",
    "&= \\text{Attention}(R_{\\pi}Q,K,V),\\\\\n",
    "& = \\text{softmax}\\left(\\frac{(R_{\\pi}Q)K^T}{\\sqrt{d_k}}\\right)V,\\\\\n",
    "& = \\text{softmax}\\left(R_{\\pi}\\frac{QK^T}{\\sqrt{d_k}}\\right)V,\\\\\n",
    "& = R_{\\pi}\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V,\\\\\n",
    "& = \\pi_r\\left(\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\\right),\\\\\n",
    "& = \\pi_r\\left(\\text{Attention}(Q,K,V) \\right),\n",
    "\\end{align*}\n",
    "$$\n",
    "where in the fifth equality we have used that the softmax funtion is applied individually for each row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92708176",
   "metadata": {},
   "source": [
    "## Definition: Attention (with weights)\n",
    "Inputs\n",
    "* a set of queries $Q = [q_1|q_2|\\dots|q_M]^T \\in \\mathbb{R}^{M \\times c_q}, \\quad M\\in\\mathbb{N}$\n",
    "* a set of keys $K = [k_1|k_2|\\dots|k_N]^T \\in \\mathbb{R}^{N \\times c_k}, \\quad N\\in\\mathbb{N}$\n",
    "* a set of values $V = [v_1|v_2|\\dots|v_N]^T \\in \\mathbb{R}^{N \\times c_v}, \\quad N\\in\\mathbb{N}$.\n",
    "\n",
    "Weights\n",
    "* a set of querry weights $W_q \\in \\mathbb{R}^{c_q\\times d_k}$ and bias $b_q\\in \\mathbb{R}^{d_k}$\n",
    "* a set of key weights $W_k \\in \\mathbb{R}^{c_k \\times d_k}$ and bias $b_k\\in \\mathbb{R}^{d_k}$\n",
    "* a set of value weights $W_v \\in \\mathbb{R}^{c_v \\times d_v}$ and bias $b_q\\in \\mathbb{R}^{d_v}$\n",
    "\n",
    "Output: \n",
    "* Output $O= [o_1|o_2|\\dots|o_N]^T \\in\\mathbb{R}^{M\\times d_v}$\n",
    "\\begin{align*}\n",
    "O =\\text{Attention}_{\\mathcal{W}}(Q,K,V)=\\text{Attention}(QW_q+B_q,KW_k+B_k,VW_v+B_v)\n",
    "\\end{align*}\n",
    "where\n",
    "\\begin{align*}\n",
    "B_q &= [b_q|b_q|\\dots|b_q]^T \\in \\mathbb{R}^{M \\times d_k},\\\\ \n",
    "B_k &= [b_k|b_k|\\dots|b_k]^T \\in \\mathbb{R}^{N \\times d_k},\\\\\n",
    "B_v &= [b_v|b_v|\\dots|b_v]^T \\in \\mathbb{R}^{N \\times d_v},\\\\\n",
    "\\mathcal{W} &= \\{W_q,b_q,W_k,b_k,W_v,b_v\\}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dbe492",
   "metadata": {},
   "source": [
    "**Note:** The same $\\operatorname{Attention}$ function can be applied to inputs of different sequence lengths. The model parameters are not tied to specific positions. In this sense, $\\operatorname{Attention}$ is **position-agnostic**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea74e686",
   "metadata": {},
   "source": [
    "### Property: Attention (with weights) Permutation Equivariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6f0ad6",
   "metadata": {},
   "source": [
    "If Attention with weights has no bias (linear projections instead of affine projections), the Attention is permutation equivariant. If $\\pi_r$ and $\\sigma_r$ denote arbitrary permutations over the rows of a matrix, then\n",
    "$$\\text{Attention}_{\\mathcal{W}}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))=\\pi_r\\left(\\text{Attention}_{\\mathcal{W}}(Q,K,V)\\right) $$\n",
    "where \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{W} &= \\{W_q,b_q,W_k,b_k,W_v,b_v\\},\\\\\n",
    "b_q &= 0_{\\mathbb{R}^{d_k}},\\\\\n",
    "b_k &= 0_{\\mathbb{R}^{d_k}},\\\\\n",
    "b_v &= 0_{\\mathbb{R}^{d_v}},\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28a8da0",
   "metadata": {},
   "source": [
    "Proof: Consider  $\\pi_r(M) = R_{\\pi}M$ and  $\\sigma_r(M) = R_{\\sigma}M$ for some permutation matrix $R_{\\pi}$ and $R_{\\sigma}$ respectively. From the previous result we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Attention}_{\\mathcal{W}}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))\n",
    "&=\\text{Attention}(\\pi_r(Q)W_q,\\sigma_r(K)W_k,\\sigma_r(V)W_v),\\\\\n",
    "&=\\text{Attention}((R_{\\pi}Q)W_q,(R_{\\sigma}K)W_k,(R_{\\sigma}V)W_v),\\\\\n",
    "&=\\text{Attention}(R_{\\pi}(QW_q),R_{\\sigma}(KW_k),R_{\\sigma}(VW_v)),\\\\\n",
    "&=R_{\\pi}\\text{Attention}(QW_q,KW_k,VW_v),\\\\\n",
    "&=R_{\\pi}\\text{Attention}_{\\mathcal{W}}(Q,K,V),\\\\\n",
    "&=\\pi_r\\left(\\text{Attention}_{\\mathcal{W}}(Q,K,V)\\right).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d632046",
   "metadata": {},
   "source": [
    "## Definition: Self-Attention\n",
    "Inputs\n",
    "* $X \\in \\mathbb{R}^{N \\times c}, \\quad N\\in \\mathbb{N}$ \n",
    "\n",
    "Weights\n",
    "* a set of querry weights $W_q \\in \\mathbb{R}^{c\\times d_k}$ and bias $b_q\\in \\mathbb{R}^{d_k}$\n",
    "* a set of key weights $W_k \\in \\mathbb{R}^{c \\times d_k}$ and bias $b_k\\in \\mathbb{R}^{d_k}$\n",
    "* a set of value weights $W_v \\in \\mathbb{R}^{c \\times d_v}$ and bias $b_q\\in \\mathbb{R}^{d_v}$\n",
    "\n",
    "Output: \n",
    "* Output $O= [o_1|o_2|\\dots|o_N]^T \\in\\mathbb{R}^{N\\times d_v}$\n",
    "\\begin{align*}\n",
    "O =\\text{SelfAttention}_{\\mathcal{W}}(X)=\\text{Attention}_{\\mathcal{W}}(X,X,X),\n",
    "\\end{align*}\n",
    "where\n",
    "\\begin{align*}\n",
    "\\mathcal{W} &= \\{W_q,b_q,W_k,b_k,W_v,b_v\\}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d49465",
   "metadata": {},
   "source": [
    "### Property: Self-Attention Permutation Equivariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741cee4b",
   "metadata": {},
   "source": [
    "If Self-Attention has no bias (linear projections instead of affine projections), the Self-Attention is permutation equivariant. If $\\pi_r$ denotes as arbitrary permutation over the rows of a matrix, then\n",
    "$$\\text{SelfAttention}_{\\mathcal{W}}(\\pi_r(X))=\\pi_r\\left(\\text{SelfAttention}_{\\mathcal{W}}(X)\\right) $$\n",
    "where \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{W} &= \\{W_q,b_q,W_k,b_k,W_v,b_v\\},\\\\\n",
    "b_q &= 0_{\\mathbb{R}^{d_k}},\\\\\n",
    "b_k &= 0_{\\mathbb{R}^{d_k}},\\\\\n",
    "b_v &= 0_{\\mathbb{R}^{d_v}},\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f719472",
   "metadata": {},
   "source": [
    "Proof: From the previous result we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{SelfAttention}_{\\mathcal{W}}(\\pi_r(X))\n",
    "&=\\text{Attention}_{\\mathcal{W}}(\\pi_r(X),\\pi_r(X),\\pi_r(X)),\\\\\n",
    "&=\\pi_r\\left(\\text{Attention}_{\\mathcal{W}}(X,X,X)\\right),\\\\\n",
    "&=\\pi_r\\left(\\text{SelfAttention}_{\\mathcal{W}}(X)\\right).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdcbb8f",
   "metadata": {},
   "source": [
    "## Definition: Multi-Head Attention\n",
    "\n",
    "Inputs\n",
    "* a set of queries $Q = [q_1|q_2|\\dots|q_M]^T \\in \\mathbb{R}^{M \\times c_q}, \\quad M\\in\\mathbb{N}$\n",
    "* a set of keys $K = [k_1|k_2|\\dots|k_N]^T \\in \\mathbb{R}^{N \\times c_k}, \\quad N\\in\\mathbb{N}$\n",
    "* a set of values $V = [v_1|v_2|\\dots|v_N]^T \\in \\mathbb{R}^{N \\times c_v}, \\quad N\\in\\mathbb{N}$\n",
    "\n",
    "Weights\n",
    "* $W_{q,i} \\in \\mathbb{R}^{c_q \\times d_k}$ and $b_{q,i}\\in \\mathbb{R}^{d_k}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{k,i} \\in \\mathbb{R}^{c_k \\times d_k}$ and $b_{k,i}\\in \\mathbb{R}^{d_k}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{v,i} \\in \\mathbb{R}^{c_v \\times d_v}$ and $b_{v,i}\\in \\mathbb{R}^{d_v}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{o} \\in \\mathbb{R}^{d_vh \\times d_o }$ and $b_{o}\\in \\mathbb{R}^{d_0}$.\n",
    "\n",
    "Output: \n",
    "* Output $O= [o_1|o_2|\\dots|o_M]^T \\in\\mathbb{R}^{M\\times d_o}$\n",
    "\\begin{align*}\n",
    "O  &= \\text{MultiHeadAttention}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(Q,K,V),\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\text{Attention}_{\\mathcal{W}_{1}}(Q,K,V)|\n",
    "\\text{Attention}_{\\mathcal{W}_{2}}(Q,K,V)|\n",
    "\\dots|\n",
    "\\text{Attention}_{\\mathcal{W}_{h}}(Q,K,V)\n",
    "\\end{pmatrix}W_o + B_o\n",
    "\\end{align*}\n",
    "where\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "B_o  &= [b_o|b_o|\\dots|b_o]^T \\in \\mathbb{R}^{M \\times d_o},\\\\ \n",
    "\\mathcal{W}_0 &= \\{W_{o},b_{o}\\},\\\\\n",
    "\\mathcal{W}_i &= \\{W_{q,i},b_{q,i},W_{k,i},b_{k,i},W_{v,i},b_{v,i}\\}, \\quad i = 1,2,\\dots,h.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a11aa8",
   "metadata": {},
   "source": [
    "Usually we have:\n",
    " * $d_h:=d_k=d_v$ (head dimensions)\n",
    " * $d := c_q = c_k = c_v = d_h\\cdot h = d_o$ (model dimension)  \n",
    "**Note:** In this case, internally we have:\n",
    "    * $W_q=[W_{q,1}|W_{q,2}|\\dots|W_{q,h}]\\in\\mathbb{R}^{d\\times d}$  \n",
    "    * $W_k=[W_{k,1}|W_{k,2}|\\dots|W_{k,h}]\\in\\mathbb{R}^{d\\times d}$  \n",
    "    * $W_v=[W_{v,1}|W_{v,2}|\\dots|W_{v,h}]\\in\\mathbb{R}^{d\\times d}$  \n",
    "\n",
    "**Note:** The same $\\operatorname{MultiHeadAttention}$ function can be applied to inputs of different sequence lengths. The model parameters are not tied to specific positions. In this sense, $\\operatorname{MultiHeadAttention}$ is **position-agnostic**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6759e72",
   "metadata": {},
   "source": [
    "### Property: Multi-Head Attention Permutation Equivariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fb3c96",
   "metadata": {},
   "source": [
    "If Multi-Head Attention has no bias (linear projections instead of affine projections), the Multi-Head Attention is permutation equivariant. If $\\pi_r$ and $\\sigma_r$ denote arbitrary permutations over the rows of a matrix, then\n",
    "$$\\text{MultiHeadAttention}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))=\\pi_r\\left(\\text{MultiHeadAttention}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(Q,K,V)\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e562f03e",
   "metadata": {},
   "source": [
    "Proof:  Consider  $\\pi_r(M) = R_{\\pi}M$ and $\\sigma_r(M) = R_{\\sigma}M$ for permutation matrices $R_{\\pi}$ and $R_{\\sigma}$ respectively. From the Attention equivariance we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{MultiHeadAttention}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\text{Attention}_{\\mathcal{W}_{1}}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))|\n",
    "\\text{Attention}_{\\mathcal{W}_{2}}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))|\n",
    "\\dots|\n",
    "\\text{Attention}_{\\mathcal{W}_{h}}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))\n",
    "\\end{pmatrix}W_o,\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\pi_r\\left(\\text{Attention}_{\\mathcal{W}_{1}}(Q,K,V)\\right)|\n",
    "\\pi_r\\left(\\text{Attention}_{\\mathcal{W}_{2}}(Q,K,V)\\right)|\n",
    "\\dots|\n",
    "\\pi_r\\left(\\text{Attention}_{\\mathcal{W}_{h}}(Q,K,V)\\right)\n",
    "\\end{pmatrix}W_o,\\\\\n",
    "&=\\begin{pmatrix}\n",
    "R_{\\pi}\\text{Attention}_{\\mathcal{W}_{1}}(Q,K,V)|\n",
    "R_{\\pi}\\text{Attention}_{\\mathcal{W}_{2}}(Q,K,V)|\n",
    "\\dots|\n",
    "R_{\\pi}\\text{Attention}_{\\mathcal{W}_{h}}(Q,K,V)\n",
    "\\end{pmatrix}W_o,\\\\\n",
    "&=R_{\\pi}\\begin{pmatrix}\n",
    "\\text{Attention}_{\\mathcal{W}_{1}}(Q,K,V)|\n",
    "\\text{Attention}_{\\mathcal{W}_{2}}(Q,K,V)|\n",
    "\\dots|\n",
    "\\text{Attention}_{\\mathcal{W}_{h}}(Q,K,V)\n",
    "\\end{pmatrix}W_o,\\\\\n",
    "&=R_{\\pi}\\text{MultiHeadAttention}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(Q,K,V),\\\\\n",
    "&=\\pi_r\\left(\\text{MultiHeadAttention}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(Q,K,V)\\right).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021f71a",
   "metadata": {},
   "source": [
    "## Definition: Multi-Head Attention (torch implementation)\n",
    "\n",
    "Torch implementation corresponds to the particular (usual) case\n",
    " * $d_h:=d_k=d_v$ (head dimensions)\n",
    " * $d := c_q = d_h\\cdot h = d_o$ (model dimension)  \n",
    "\n",
    "Denote $d_h = d|h$ ($h$ must divide $d$). Then we have the simplified defintion\n",
    "\n",
    "Inputs\n",
    "* a set of queries $Q = [q_1|q_2|\\dots|q_M]^T \\in \\mathbb{R}^{M \\times d}$\n",
    "* a set of keys $K = [k_1|k_2|\\dots|k_N]^T \\in \\mathbb{R}^{N \\times c_k}$\n",
    "* a set of values $V = [v_1|v_2|\\dots|v_N]^T \\in \\mathbb{R}^{N \\times c_v}$\n",
    "\n",
    "Weights\n",
    "* $W_{q,i} \\in \\mathbb{R}^{d \\times d_h}$ and $b_{q,i}\\in \\mathbb{R}^{d_h}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{k,i} \\in \\mathbb{R}^{c_k \\times d_h}$ and $b_{k,i}\\in \\mathbb{R}^{d_h}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{v,i} \\in \\mathbb{R}^{c_v \\times d_h}$ and $b_{v,i}\\in \\mathbb{R}^{d_h}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{o} \\in \\mathbb{R}^{d \\times d }$ and $b_{o}\\in \\mathbb{R}^{d}$.\n",
    "\n",
    "\n",
    "Output: \n",
    "* Output $O= [o_1|o_2|\\dots|o_M]^T \\in\\mathbb{R}^{M\\times d}$\n",
    "\\begin{align*}\n",
    "O  &= \\text{MultiHeadAttention}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(Q,K,V),\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\text{Attention}_{\\mathcal{W}_1}(Q,K,V)|\n",
    "\\text{Attention}_{\\mathcal{W}_2}(Q,K,V)|\n",
    "\\dots|\n",
    "\\text{Attention}_{\\mathcal{W}_h}(Q,K,V)\n",
    "\\end{pmatrix}W_o + B_o\n",
    "\\end{align*}\n",
    "where:\n",
    "    * $W_q=[W_{q,1}|W_{q,2}|\\dots|W_{q,h}]\\in\\mathbb{R}^{d\\times d}$  \n",
    "    * $W_k=[W_{k,1}|W_{k,2}|\\dots|W_{k,h}]\\in\\mathbb{R}^{d\\times d}$  \n",
    "    * $W_v=[W_{v,1}|W_{v,2}|\\dots|W_{v,h}]\\in\\mathbb{R}^{d\\times d}$  \n",
    "    * $b_q=[b^T_{q,1}|b^T_{q,2}|\\dots|b^T_{q,h}]^T\\in\\mathbb{R}^{d}$  \n",
    "    * $b_k=[b^T_{k,1}|b^T_{k,2}|\\dots|b^T_{k,h}]^T\\in\\mathbb{R}^{d}$ \n",
    "    * $b_v=[b^T_{v,1}|b^T_{v,2}|\\dots|b^T_{v,h}]^T\\in\\mathbb{R}^{d}$  \n",
    "    * $B_o= [b_o|b_o|\\dots|b_o]^T \\in \\mathbb{R}^{M \\times d}$\n",
    "    * $\\mathcal{W}_0 = \\{W_{o},b_{o}\\}$\n",
    "    * $\\mathcal{W}_i = \\{W_{q,i},b_{q,i},W_{k,i},b_{k,i},W_{v,i},b_{v,i}\\}, \\quad i = 1,2,\\dots,h$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc23134",
   "metadata": {},
   "source": [
    "## Definition: Multi-Head Self-Attention\n",
    "\n",
    "This what we mean when we write $\\text{MultiHeadAttention}(X)$.\n",
    "\n",
    "Inputs\n",
    "* $X \\in \\mathbb{R}^{N \\times c}, \\quad N \\in \\mathbb{N}.$ \n",
    "\n",
    "Weights\n",
    "* $W_{q,i} \\in \\mathbb{R}^{c \\times d_k}$ and $b_{q,i}\\in \\mathbb{R}^{d_k}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{k,i} \\in \\mathbb{R}^{c \\times d_k}$ and $b_{k,i}\\in \\mathbb{R}^{d_k}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{v,i} \\in \\mathbb{R}^{c \\times d_v}$ and $b_{v,i}\\in \\mathbb{R}^{d_v}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{o} \\in \\mathbb{R}^{d_vh \\times d_o }$ and $b_{o}\\in \\mathbb{R}^{d_0}$.\n",
    "\n",
    "Output: \n",
    "* Output $O= [o_1|o_2|\\dots|o_M]^T \\in\\mathbb{R}^{M\\times d_o}$\n",
    "\\begin{align*}\n",
    "O = \\text{MultiHeadSelfAttention}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(X)= \\text{MultiHeadAttention}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(X,X,X).\n",
    "\\end{align*}\n",
    "where\n",
    "\\begin{align*}\n",
    "\\mathcal{W}_0 &= \\{W_{o},b_{o}\\},\\\\\n",
    "\\mathcal{W}_i &= \\{W_{q,i},b_{q,i},W_{k,i},b_{k,i},W_{v,i},b_{v,i}\\}, \\quad i = 1,2,\\dots,h.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369a7e28",
   "metadata": {},
   "source": [
    "**Note:** The same $\\operatorname{MultiHeadSelfAttention}$ function can be applied to inputs of different sequence lengths. The model parameters are not tied to specific positions. In this sense, $\\operatorname{MultiHeadHeadAttention}$ is **position-agnostic**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b398a204",
   "metadata": {},
   "source": [
    "### Property: Multi-Head Self-Attention Permutation Equivariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0764344d",
   "metadata": {},
   "source": [
    "If Multi-Head-Self-Attention has no bias (linear projections instead of affine projections), the Multi-Head-Self-Attention is permutation equivariant. If $\\pi_r(M)$ denotes an arbitrary permutation over the rows of a matrix $M$, then\n",
    "$$\\text{MultiHeadSelfAttention}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(\\pi_r(X))=\\pi_r\\left(\\text{MultiHeadSelfAttention}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(X)\\right),$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce029db0",
   "metadata": {},
   "source": [
    "Proof: From the previous result we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{MultiHeadSelfAttention}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(\\pi_r(X))\n",
    "&=\\text{MultiHeadAttention}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(\\pi_r(X),\\pi_r(X),\\pi_r(X)),\\\\\n",
    "&=\\pi_r\\left(\\text{MultiHeadAttention}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(X,X,X)\\right),\\\\\n",
    "&=\\pi_r\\left(\\text{MultiHeadSelfAttention}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(X)\\right).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a6d386",
   "metadata": {},
   "source": [
    "## Code: Multi-Head Attention\n",
    "\n",
    "The code for the remaining models are easily deduced from $\\text{MultiHeadAttention}$:\n",
    "* $\\text{Attention}_{\\mathcal{W}}(Q,K,V) = \\text{MultiHeadAttention}_{\\{\\mathcal{W},\\operatorname{Id},0\\}}(Q,K,V)$ (one head)\n",
    "* $\\text{SelfAttention}_{\\mathcal{W}}(X) = \\text{Attention}_{\\mathcal{W}}(X,X,X)$\n",
    "* $\\text{MultiHeadSelfAttention}_{\\{\\mathcal{W}_{i}\\}_{i=1}^h\\cup \\{W_o,b_o\\}}(X) = \\text{MultiHeadAttention}_{\\{\\mathcal{W}_{i}\\}_{i=1}^h\\cup \\{W_o,b_o\\}}(X,X,X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ab3980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52e46913",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention module with (general) parameters\n",
    "    cq, ck, cv: input dimensions for Q, K, V\n",
    "    dk, dv: dimensions for each head's Q, K and V\n",
    "    do: output dimension\n",
    "    h: number of heads.\n",
    "    \n",
    "    The initialization of the weights differs from PyTorchâ€™s `nn.MultiheadAttention`.\n",
    "    Here we use standard `nn.Linear` initialization (Xavier uniform for weights and\n",
    "    zeros for biases) for clarity and simplicity. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, cq, ck, cv, dk, dv, do, h, bias=True, add_bias_kv=False, device=None, dtype=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert dk % h == 0, \"dk must be divisible by h\"\n",
    "        self.cq = cq\n",
    "        self.ck = ck\n",
    "        self.cv = cv\n",
    "        self.dk = dk\n",
    "        self.dv = dv\n",
    "        self.do = do\n",
    "        self.h = h\n",
    "        self.add_bias_kv = add_bias_kv\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        # Q -> QW_q+B_q\n",
    "        self.q_proj = torch.nn.Linear(cq, dk * h, bias, self.device, self.dtype)\n",
    "        # K -> KW_k+B_k\n",
    "        self.k_proj = torch.nn.Linear(ck, dk * h, bias, self.device, self.dtype)\n",
    "        # V -> VW_v+B_v\n",
    "        self.v_proj = torch.nn.Linear(cv, dv * h, bias, self.device, self.dtype)\n",
    "\n",
    "        self.out_proj = torch.nn.Linear(dv * h, do, bias, self.device, self.dtype)\n",
    "        if self.add_bias_kv:\n",
    "            self.bias_k = torch.nn.Parameter(\n",
    "                torch.zeros(1, 1, dk * h, device=self.device, dtype=self.dtype)\n",
    "            )\n",
    "            self.bias_v = torch.nn.Parameter(\n",
    "                torch.zeros(1, 1, dv * h, device=self.device, dtype=self.dtype)\n",
    "            )\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        \"\"\"Forward pass of the MHA module.\"\"\"\n",
    "        # Linear projections\n",
    "        proj_q = self.q_proj(Q)  # Q=QW_q+B_q\n",
    "        proj_k = self.k_proj(K)  # K=KW_k+B_k\n",
    "        proj_v = self.v_proj(V)  # V=VW_v+B_v\n",
    "        if self.add_bias_kv:\n",
    "            # append bias to the key and value sequences\n",
    "            batch_size = proj_k.shape[0]\n",
    "            proj_k = torch.cat([proj_k, self.bias_k.repeat(batch_size, 1, 1)], dim=1)\n",
    "            proj_v = torch.cat([proj_v, self.bias_v.repeat(batch_size, 1, 1)], dim=1)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        r_q = einops.rearrange(proj_q, \"b m (h dk) -> b h m dk\", h=self.h)\n",
    "        r_k = einops.rearrange(proj_k, \"b n (h dk) -> b h n dk\", h=self.h)\n",
    "        r_v = einops.rearrange(proj_v, \"b n (h dv) -> b h n dv\", h=self.h)\n",
    "\n",
    "        # QK^T\n",
    "        scores = torch.einsum(\"bhmd, bhnd -> bhmn\", r_q, r_k)\n",
    "\n",
    "        # softmax(QK^T/sqrt(dk))\n",
    "\n",
    "        attn = torch.nn.functional.softmax(scores / (self.dk**0.5), dim=-1)\n",
    "\n",
    "        # softmax(QK^T/sqrt(dk))V\n",
    "        o = torch.einsum(\"bhmn, bhnv -> bhmv\", attn, r_v)\n",
    "\n",
    "        # Reshape back\n",
    "        r_o = einops.rearrange(o, \"b h m dv -> b m (h dv)\")\n",
    "\n",
    "        # Final linear projection\n",
    "        proj_o = self.out_proj(r_o)\n",
    "        return proj_o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fac18d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dim = 10\n",
    "M = 5  # sequence length of q\n",
    "N = 3  # sequence length of k,v\n",
    "d = 16 # embedding/model dimension\n",
    "ck = 32  # key dimension \n",
    "cv = 64  # value dimension\n",
    "h = 2  # number of heads\n",
    "\n",
    "bias = True\n",
    "add_bias_kv = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e18b6f",
   "metadata": {},
   "source": [
    "#### Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fed376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_attn = torch.nn.MultiheadAttention(embed_dim=d,kdim=ck,vdim=cv,\n",
    "                                        num_heads=h,batch_first=True,\n",
    "                                        bias=bias,add_bias_kv=add_bias_kv)\n",
    "\n",
    "assert d % h == 0, \"d must be divisible by h\"\n",
    "dh = d // h\n",
    "attn = MultiHeadAttention(cq = d, ck = ck, cv=cv, dk = dh ,dv=dh,do=d,h=h,bias=bias,add_bias_kv=add_bias_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3405b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch MHA Weights:\n",
      "q_proj_weight - torch.Size([16, 16])\n",
      "k_proj_weight - torch.Size([16, 32])\n",
      "v_proj_weight - torch.Size([16, 64])\n",
      "in_proj_bias - torch.Size([48])\n",
      "out_proj.weight - torch.Size([16, 16])\n",
      "out_proj.bias - torch.Size([16])\n",
      "\n",
      "Our MHA Weights:\n",
      "q_proj.weight - torch.Size([16, 16])\n",
      "q_proj.bias - torch.Size([16])\n",
      "k_proj.weight - torch.Size([16, 32])\n",
      "k_proj.bias - torch.Size([16])\n",
      "v_proj.weight - torch.Size([16, 64])\n",
      "v_proj.bias - torch.Size([16])\n",
      "out_proj.weight - torch.Size([16, 16])\n",
      "out_proj.bias - torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch MHA Weights:\")\n",
    "for name, w in nn_attn.named_parameters():\n",
    "    print(f\"{name} - {w.shape}\")\n",
    "\n",
    "print(\"\\nOur MHA Weights:\")\n",
    "for name, w in attn.named_parameters():\n",
    "    print(f\"{name} - {w.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cee4a8",
   "metadata": {},
   "source": [
    "#### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72f74493",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand(batch_dim,M,d)\n",
    "k = torch.rand(batch_dim,N,ck) \n",
    "v = torch.rand(batch_dim,N,cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c520f7",
   "metadata": {},
   "source": [
    "#### Load weights\n",
    "\n",
    "Weights are created differently, so lets load the nn wieghts on our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53d3e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # 1) copy weights (shapes already match)\n",
    "    attn.q_proj.weight.copy_(nn_attn.q_proj_weight)   # (d, d)\n",
    "    attn.k_proj.weight.copy_(nn_attn.k_proj_weight)   # (d, ck)\n",
    "    attn.v_proj.weight.copy_(nn_attn.v_proj_weight)   # (d, cv)\n",
    "\n",
    "    # 2) split the packed bias: (3d,) -> (d,) + (d,) + (d,)\n",
    "    b = nn_attn.in_proj_bias      # shape (48,)\n",
    "    if bias:\n",
    "        attn.q_proj.bias.copy_(b[0:d])        # 0:d\n",
    "        attn.k_proj.bias.copy_(b[d:2*d])      # d:2d\n",
    "        attn.v_proj.bias.copy_(b[2*d:3*d])    # 2d:3d\n",
    "    \n",
    "    if add_bias_kv:\n",
    "        attn.bias_k.copy_(nn_attn.bias_k.squeeze(0))\n",
    "        attn.bias_v.copy_(nn_attn.bias_v.squeeze(0))\n",
    "\n",
    "    # 3) output projection\n",
    "    attn.out_proj.weight.copy_(nn_attn.out_proj.weight)\n",
    "    if bias:\n",
    "        attn.out_proj.bias.copy_(nn_attn.out_proj.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "336887f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = attn(q, k, v)\n",
    "nn_out,_ = nn_attn(q,k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46ab0dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5569e-01,  1.0676e-01, -6.1087e-01, -7.9775e-02,  2.3798e-01,\n",
       "          -3.2078e-01,  6.9330e-01,  3.1864e-01, -2.5643e-01, -1.8558e-01,\n",
       "          -3.6022e-01,  6.1788e-02, -1.6606e-01, -6.5259e-01, -2.4641e-01,\n",
       "          -2.1432e-01],\n",
       "         [ 1.5143e-01,  9.0589e-02, -5.9957e-01, -8.0171e-02,  2.5650e-01,\n",
       "          -3.3008e-01,  6.7403e-01,  3.2564e-01, -2.6820e-01, -2.0156e-01,\n",
       "          -3.5230e-01,  7.8036e-02, -1.5919e-01, -6.6133e-01, -2.3463e-01,\n",
       "          -2.2829e-01],\n",
       "         [ 1.5039e-01,  1.1415e-01, -6.1351e-01, -9.4964e-02,  2.2960e-01,\n",
       "          -3.0630e-01,  7.0023e-01,  3.0765e-01, -2.4476e-01, -1.7589e-01,\n",
       "          -3.6727e-01,  5.1043e-02, -1.6824e-01, -6.5180e-01, -2.6089e-01,\n",
       "          -1.9861e-01],\n",
       "         [ 1.4563e-01,  8.0383e-02, -5.9320e-01, -7.2906e-02,  2.5868e-01,\n",
       "          -3.3843e-01,  6.6956e-01,  3.2497e-01, -2.6531e-01, -1.9871e-01,\n",
       "          -3.5460e-01,  7.0480e-02, -1.5454e-01, -6.6729e-01, -2.4935e-01,\n",
       "          -2.3204e-01],\n",
       "         [ 1.5815e-01,  1.0866e-01, -6.1099e-01, -8.9951e-02,  2.4548e-01,\n",
       "          -3.1543e-01,  6.8748e-01,  3.2161e-01, -2.6459e-01, -1.9648e-01,\n",
       "          -3.5435e-01,  7.7361e-02, -1.6708e-01, -6.5166e-01, -2.2686e-01,\n",
       "          -2.1630e-01]],\n",
       "\n",
       "        [[-1.0755e-01,  4.1454e-01, -6.3624e-01, -3.3375e-02,  1.8323e-01,\n",
       "          -1.5138e-01,  5.3609e-01,  2.6428e-01,  1.9110e-01,  3.0038e-02,\n",
       "          -2.3250e-01,  1.6557e-01, -1.2224e-01, -4.2554e-01,  2.9454e-03,\n",
       "           1.7571e-01],\n",
       "         [-3.0431e-02,  4.2130e-01, -6.4025e-01,  2.9888e-02,  1.7236e-01,\n",
       "          -1.5740e-01,  5.7150e-01,  3.3994e-01,  1.5041e-01,  4.5864e-03,\n",
       "          -2.4793e-01,  1.6210e-01, -1.0065e-01, -4.5894e-01,  9.6412e-03,\n",
       "           1.8278e-01],\n",
       "         [-6.7467e-02,  4.1512e-01, -6.2924e-01, -3.4687e-03,  1.9077e-01,\n",
       "          -1.5311e-01,  5.6329e-01,  2.9964e-01,  1.5962e-01,  1.3474e-02,\n",
       "          -2.6885e-01,  1.6469e-01, -1.1038e-01, -4.3612e-01, -1.3232e-02,\n",
       "           1.7507e-01],\n",
       "         [-6.5544e-02,  4.2129e-01, -6.4101e-01, -5.1028e-03,  1.7473e-01,\n",
       "          -1.5577e-01,  5.5381e-01,  3.0647e-01,  1.7412e-01,  2.1444e-02,\n",
       "          -2.3303e-01,  1.6138e-01, -1.0896e-01, -4.4761e-01,  1.1921e-02,\n",
       "           1.7973e-01],\n",
       "         [-7.9209e-02,  4.1517e-01, -6.4129e-01, -2.5239e-03,  1.7360e-01,\n",
       "          -1.5424e-01,  5.4459e-01,  2.9306e-01,  1.7739e-01,  1.6627e-02,\n",
       "          -2.2869e-01,  1.6594e-01, -1.1605e-01, -4.3872e-01,  1.2074e-02,\n",
       "           1.8005e-01]],\n",
       "\n",
       "        [[-1.7637e-01,  3.7893e-01, -6.4285e-01,  8.8977e-02,  2.1773e-01,\n",
       "          -3.7075e-01,  4.2502e-01,  4.0290e-01,  2.4373e-01,  2.2985e-01,\n",
       "          -1.1624e-01,  1.7978e-01,  8.5094e-02, -4.9852e-01, -8.5519e-03,\n",
       "           6.4910e-02],\n",
       "         [-1.7171e-01,  3.8134e-01, -6.3780e-01,  8.4797e-02,  2.1544e-01,\n",
       "          -3.6828e-01,  4.2754e-01,  4.0093e-01,  2.3952e-01,  2.2675e-01,\n",
       "          -1.2363e-01,  1.7481e-01,  8.4071e-02, -5.0000e-01, -8.1927e-03,\n",
       "           6.3953e-02],\n",
       "         [-1.5576e-01,  3.8997e-01, -6.2353e-01,  7.8024e-02,  2.1217e-01,\n",
       "          -3.5574e-01,  4.3837e-01,  4.0466e-01,  2.3126e-01,  2.2298e-01,\n",
       "          -1.4295e-01,  1.6493e-01,  8.7384e-02, -4.9639e-01, -4.8659e-04,\n",
       "           6.1785e-02],\n",
       "         [-1.9072e-01,  3.6594e-01, -6.5356e-01,  7.4995e-02,  2.2052e-01,\n",
       "          -3.7510e-01,  4.1922e-01,  3.9642e-01,  2.4510e-01,  2.3005e-01,\n",
       "          -9.4143e-02,  1.9247e-01,  7.5314e-02, -5.0906e-01, -2.9790e-02,\n",
       "           7.1791e-02],\n",
       "         [-1.8295e-01,  3.7037e-01, -6.4640e-01,  7.7618e-02,  2.1613e-01,\n",
       "          -3.7265e-01,  4.2232e-01,  3.9250e-01,  2.3898e-01,  2.2594e-01,\n",
       "          -1.0647e-01,  1.7887e-01,  7.6559e-02, -5.0460e-01, -2.3219e-02,\n",
       "           6.5768e-02]],\n",
       "\n",
       "        [[-1.5804e-02,  4.0639e-01, -6.2397e-01, -6.8415e-02,  4.3120e-01,\n",
       "          -3.1672e-01,  5.3788e-01,  4.0259e-01, -9.6298e-02, -4.4946e-02,\n",
       "          -3.4755e-01,  4.8829e-01, -4.6331e-02, -3.7721e-01, -1.5251e-02,\n",
       "          -6.9416e-03],\n",
       "         [-9.8515e-03,  4.0402e-01, -6.2506e-01, -6.2212e-02,  4.3620e-01,\n",
       "          -3.1905e-01,  5.3650e-01,  4.1492e-01, -9.5741e-02, -5.2940e-02,\n",
       "          -3.5076e-01,  4.9293e-01, -4.7339e-02, -3.7968e-01, -7.8422e-03,\n",
       "          -1.2251e-02],\n",
       "         [ 1.9694e-03,  4.1878e-01, -6.3442e-01, -5.4956e-02,  4.3427e-01,\n",
       "          -3.1843e-01,  5.5183e-01,  4.2544e-01, -1.1211e-01, -4.9770e-02,\n",
       "          -3.4628e-01,  4.9507e-01, -4.2335e-02, -3.8203e-01, -2.1615e-03,\n",
       "          -4.0791e-03],\n",
       "         [-2.6342e-02,  3.7208e-01, -6.0890e-01, -6.7363e-02,  4.4603e-01,\n",
       "          -3.2902e-01,  5.0037e-01,  4.0687e-01, -6.4345e-02, -7.2974e-02,\n",
       "          -3.6566e-01,  4.9190e-01, -6.0422e-02, -3.8094e-01, -1.1478e-02,\n",
       "          -3.7452e-02],\n",
       "         [-2.0662e-02,  4.0294e-01, -6.2575e-01, -7.0731e-02,  4.2982e-01,\n",
       "          -3.1915e-01,  5.4060e-01,  3.9478e-01, -9.5959e-02, -3.8345e-02,\n",
       "          -3.3997e-01,  4.8766e-01, -4.8864e-02, -3.7428e-01, -2.4151e-02,\n",
       "          -6.3199e-03]],\n",
       "\n",
       "        [[-9.3732e-02,  2.6940e-01, -6.1991e-01, -1.8148e-01,  1.9933e-01,\n",
       "          -2.8054e-01,  5.2727e-01,  1.6899e-01,  3.2907e-02,  1.3375e-01,\n",
       "          -8.9230e-02,  1.9845e-01, -9.9860e-02, -3.5171e-01, -2.6157e-02,\n",
       "          -6.8701e-02],\n",
       "         [-1.0542e-01,  2.7008e-01, -6.2093e-01, -1.8157e-01,  1.9980e-01,\n",
       "          -2.8427e-01,  5.2418e-01,  1.6124e-01,  3.5152e-02,  1.3872e-01,\n",
       "          -7.8307e-02,  1.9642e-01, -1.0581e-01, -3.5261e-01, -2.3102e-02,\n",
       "          -6.7594e-02],\n",
       "         [-8.5494e-02,  2.7150e-01, -6.1438e-01, -1.8060e-01,  1.8928e-01,\n",
       "          -2.8320e-01,  5.3982e-01,  1.6129e-01,  1.9864e-02,  1.4108e-01,\n",
       "          -8.4133e-02,  1.7596e-01, -1.0926e-01, -3.5951e-01, -1.7315e-02,\n",
       "          -7.8598e-02],\n",
       "         [-9.0593e-02,  2.7492e-01, -6.1214e-01, -1.6832e-01,  1.9753e-01,\n",
       "          -2.8573e-01,  5.2505e-01,  1.7568e-01,  2.6585e-02,  1.4143e-01,\n",
       "          -8.0581e-02,  2.0273e-01, -1.0491e-01, -3.4753e-01, -1.8444e-02,\n",
       "          -7.0437e-02],\n",
       "         [-1.1733e-01,  2.7472e-01, -6.1569e-01, -1.6897e-01,  2.0376e-01,\n",
       "          -2.8899e-01,  5.1086e-01,  1.6599e-01,  3.9299e-02,  1.4643e-01,\n",
       "          -6.3525e-02,  2.1023e-01, -1.0886e-01, -3.4540e-01, -1.8081e-02,\n",
       "          -6.0870e-02]],\n",
       "\n",
       "        [[-7.2085e-03,  2.0242e-01, -7.8770e-01,  1.2784e-01,  3.2346e-01,\n",
       "          -3.5007e-01,  6.9527e-01,  2.9035e-01, -8.3508e-02, -2.1216e-02,\n",
       "          -5.3083e-02,  2.4749e-01, -1.3935e-01, -3.5437e-01, -1.7906e-01,\n",
       "           8.6124e-05],\n",
       "         [ 1.0481e-03,  2.1006e-01, -7.9695e-01,  1.2910e-01,  3.2321e-01,\n",
       "          -3.6175e-01,  6.9569e-01,  2.9414e-01, -8.2257e-02, -1.7251e-02,\n",
       "          -5.3389e-02,  2.4558e-01, -1.3297e-01, -3.5106e-01, -1.8339e-01,\n",
       "          -1.6467e-03],\n",
       "         [ 7.4018e-04,  1.8282e-01, -7.8020e-01,  1.3757e-01,  3.3110e-01,\n",
       "          -3.5018e-01,  7.0214e-01,  3.0136e-01, -7.9005e-02, -1.8109e-02,\n",
       "          -4.0341e-02,  2.3935e-01, -1.3158e-01, -3.4990e-01, -1.7139e-01,\n",
       "          -1.3397e-02],\n",
       "         [-5.5519e-03,  2.0073e-01, -7.8461e-01,  1.2010e-01,  3.2423e-01,\n",
       "          -3.5215e-01,  6.9169e-01,  2.9834e-01, -6.8430e-02, -1.4718e-02,\n",
       "          -5.2356e-02,  2.5055e-01, -1.2882e-01, -3.6046e-01, -1.8014e-01,\n",
       "           3.1321e-03],\n",
       "         [ 4.3791e-03,  2.1046e-01, -7.9667e-01,  1.2964e-01,  3.2813e-01,\n",
       "          -3.5514e-01,  7.0459e-01,  3.0496e-01, -9.5018e-02, -1.6961e-02,\n",
       "          -4.9919e-02,  2.5654e-01, -1.3717e-01, -3.5144e-01, -1.7438e-01,\n",
       "          -4.6613e-03]],\n",
       "\n",
       "        [[ 6.8886e-02,  4.6638e-01, -4.4757e-01, -2.1027e-01,  2.1403e-01,\n",
       "           5.8211e-03,  6.4324e-01,  3.8431e-01,  3.5674e-04,  5.8941e-02,\n",
       "          -3.6511e-01,  2.1518e-01, -4.2056e-02, -4.8192e-01,  1.1955e-01,\n",
       "           1.0815e-01],\n",
       "         [ 7.8134e-02,  4.6253e-01, -4.2579e-01, -2.2055e-01,  2.1525e-01,\n",
       "           1.8791e-02,  6.5156e-01,  4.0071e-01, -2.3604e-02,  5.3636e-02,\n",
       "          -3.6182e-01,  2.2424e-01, -4.9091e-02, -5.0174e-01,  1.3873e-01,\n",
       "           1.0544e-01],\n",
       "         [ 7.3603e-02,  4.4893e-01, -4.2886e-01, -2.1693e-01,  2.1262e-01,\n",
       "           2.2715e-03,  6.4119e-01,  3.8208e-01, -1.4983e-02,  4.7915e-02,\n",
       "          -3.6447e-01,  2.0732e-01, -4.6507e-02, -4.9505e-01,  1.1956e-01,\n",
       "           9.4868e-02],\n",
       "         [ 7.7387e-02,  4.3828e-01, -4.0683e-01, -2.1802e-01,  2.1616e-01,\n",
       "           1.9521e-02,  6.5042e-01,  3.9005e-01, -3.5605e-02,  4.6837e-02,\n",
       "          -3.4786e-01,  2.0397e-01, -5.6017e-02, -5.0271e-01,  1.4490e-01,\n",
       "           8.8348e-02],\n",
       "         [ 8.3835e-02,  4.9262e-01, -4.6292e-01, -2.2573e-01,  2.2061e-01,\n",
       "           1.4205e-02,  6.5013e-01,  4.0415e-01, -6.7415e-03,  5.5933e-02,\n",
       "          -3.7783e-01,  2.4275e-01, -3.5167e-02, -4.8235e-01,  1.2916e-01,\n",
       "           1.2124e-01]],\n",
       "\n",
       "        [[ 7.5170e-03,  2.4746e-01, -5.8548e-01, -2.1807e-02,  2.5714e-01,\n",
       "          -3.0188e-01,  6.9924e-01,  3.2532e-01, -2.4531e-02,  2.8298e-02,\n",
       "          -1.5980e-01,  2.7199e-01, -1.6699e-01, -4.2600e-01,  9.3501e-02,\n",
       "          -1.1685e-01],\n",
       "         [ 2.2948e-02,  2.3550e-01, -5.6282e-01, -4.2856e-02,  2.4474e-01,\n",
       "          -2.9440e-01,  6.9169e-01,  3.3333e-01, -4.3801e-02,  1.1148e-02,\n",
       "          -1.6870e-01,  2.7341e-01, -1.7011e-01, -4.5179e-01,  8.0561e-02,\n",
       "          -1.1976e-01],\n",
       "         [-4.0726e-03,  2.5814e-01, -6.0310e-01, -1.7693e-02,  2.6401e-01,\n",
       "          -3.1742e-01,  7.0996e-01,  2.9851e-01, -2.3831e-02,  3.9345e-02,\n",
       "          -1.5529e-01,  2.5224e-01, -1.6444e-01, -4.0880e-01,  9.0226e-02,\n",
       "          -1.1927e-01],\n",
       "         [ 8.9744e-03,  2.5386e-01, -5.8966e-01, -2.5341e-02,  2.5622e-01,\n",
       "          -3.1179e-01,  7.0709e-01,  3.1476e-01, -3.5247e-02,  2.6307e-02,\n",
       "          -1.6246e-01,  2.5987e-01, -1.6549e-01, -4.2918e-01,  8.8560e-02,\n",
       "          -1.1839e-01],\n",
       "         [ 6.1242e-03,  2.3614e-01, -5.7934e-01, -3.3974e-02,  2.6690e-01,\n",
       "          -3.0515e-01,  6.9985e-01,  3.0969e-01, -2.9959e-02,  2.1379e-02,\n",
       "          -1.6152e-01,  2.5594e-01, -1.6852e-01, -4.1696e-01,  8.6254e-02,\n",
       "          -1.3090e-01]],\n",
       "\n",
       "        [[ 1.2848e-01,  4.2649e-01, -6.1849e-01, -6.1139e-02,  2.7327e-01,\n",
       "          -2.6265e-01,  8.7039e-01,  2.2752e-01, -1.3141e-01,  4.6065e-02,\n",
       "          -2.2038e-01,  1.5056e-01, -2.6300e-01, -4.0572e-01,  8.1474e-02,\n",
       "          -4.8831e-02],\n",
       "         [ 1.3643e-01,  4.2589e-01, -6.2183e-01, -7.0965e-02,  2.7341e-01,\n",
       "          -2.6114e-01,  8.7271e-01,  2.3256e-01, -1.3057e-01,  4.2218e-02,\n",
       "          -2.2986e-01,  1.5230e-01, -2.5850e-01, -4.0720e-01,  7.6876e-02,\n",
       "          -5.2210e-02],\n",
       "         [ 1.4243e-01,  4.3593e-01, -6.4489e-01, -4.3961e-02,  2.7963e-01,\n",
       "          -2.8666e-01,  8.7107e-01,  2.5759e-01, -1.0203e-01,  5.7728e-02,\n",
       "          -2.2847e-01,  1.6548e-01, -2.4045e-01, -4.0569e-01,  8.3537e-02,\n",
       "          -5.4524e-02],\n",
       "         [ 1.4982e-01,  4.2815e-01, -6.2880e-01, -8.3315e-02,  2.8137e-01,\n",
       "          -2.6578e-01,  8.8166e-01,  2.4246e-01, -1.2689e-01,  3.4010e-02,\n",
       "          -2.4107e-01,  1.6056e-01, -2.5542e-01, -4.1119e-01,  7.6085e-02,\n",
       "          -5.4920e-02],\n",
       "         [ 1.3140e-01,  4.0751e-01, -5.9964e-01, -7.8638e-02,  2.5299e-01,\n",
       "          -2.4258e-01,  8.6677e-01,  2.0539e-01, -1.5541e-01,  3.2079e-02,\n",
       "          -2.3078e-01,  1.2500e-01, -2.7164e-01, -4.0280e-01,  6.2119e-02,\n",
       "          -6.0626e-02]],\n",
       "\n",
       "        [[ 1.5139e-02,  2.8492e-01, -6.3906e-01, -4.9558e-02,  2.8537e-01,\n",
       "          -3.6692e-01,  5.7312e-01,  3.7895e-01, -4.2300e-03,  7.7914e-02,\n",
       "          -1.7886e-01,  2.9132e-01, -1.0923e-01, -4.5762e-01,  8.0786e-02,\n",
       "          -1.6336e-01],\n",
       "         [ 3.9781e-03,  2.6261e-01, -6.2566e-01, -5.0836e-02,  3.0184e-01,\n",
       "          -3.5376e-01,  5.7409e-01,  3.8379e-01,  6.2645e-04,  5.9987e-02,\n",
       "          -1.8324e-01,  2.9253e-01, -1.0804e-01, -4.5527e-01,  1.0307e-01,\n",
       "          -1.7569e-01],\n",
       "         [ 1.3994e-03,  2.6799e-01, -6.3773e-01, -4.5386e-02,  2.9614e-01,\n",
       "          -3.5988e-01,  5.7143e-01,  3.8272e-01,  2.7612e-03,  6.4258e-02,\n",
       "          -1.7470e-01,  2.9070e-01, -1.0641e-01, -4.6013e-01,  9.3013e-02,\n",
       "          -1.6641e-01],\n",
       "         [-2.2863e-02,  2.9551e-01, -6.4082e-01, -4.2893e-02,  2.9220e-01,\n",
       "          -3.5952e-01,  5.7040e-01,  3.5923e-01,  5.3209e-03,  8.7073e-02,\n",
       "          -1.6829e-01,  2.8158e-01, -1.0604e-01, -4.5295e-01,  1.0207e-01,\n",
       "          -1.4910e-01],\n",
       "         [ 3.1681e-02,  2.8571e-01, -6.3036e-01, -6.0353e-02,  2.8698e-01,\n",
       "          -3.6467e-01,  5.7695e-01,  3.8551e-01, -7.9821e-03,  7.5374e-02,\n",
       "          -1.9426e-01,  2.9762e-01, -1.1050e-01, -4.5290e-01,  8.1150e-02,\n",
       "          -1.7317e-01]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efbc4c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5569e-01,  1.0676e-01, -6.1087e-01, -7.9775e-02,  2.3798e-01,\n",
       "          -3.2078e-01,  6.9330e-01,  3.1864e-01, -2.5643e-01, -1.8558e-01,\n",
       "          -3.6022e-01,  6.1788e-02, -1.6606e-01, -6.5259e-01, -2.4641e-01,\n",
       "          -2.1432e-01],\n",
       "         [ 1.5143e-01,  9.0589e-02, -5.9957e-01, -8.0171e-02,  2.5650e-01,\n",
       "          -3.3008e-01,  6.7403e-01,  3.2564e-01, -2.6820e-01, -2.0156e-01,\n",
       "          -3.5230e-01,  7.8036e-02, -1.5919e-01, -6.6133e-01, -2.3463e-01,\n",
       "          -2.2829e-01],\n",
       "         [ 1.5039e-01,  1.1415e-01, -6.1351e-01, -9.4964e-02,  2.2960e-01,\n",
       "          -3.0630e-01,  7.0023e-01,  3.0765e-01, -2.4476e-01, -1.7589e-01,\n",
       "          -3.6727e-01,  5.1043e-02, -1.6824e-01, -6.5180e-01, -2.6089e-01,\n",
       "          -1.9861e-01],\n",
       "         [ 1.4563e-01,  8.0383e-02, -5.9320e-01, -7.2906e-02,  2.5868e-01,\n",
       "          -3.3843e-01,  6.6956e-01,  3.2497e-01, -2.6531e-01, -1.9871e-01,\n",
       "          -3.5460e-01,  7.0480e-02, -1.5454e-01, -6.6729e-01, -2.4935e-01,\n",
       "          -2.3204e-01],\n",
       "         [ 1.5815e-01,  1.0866e-01, -6.1099e-01, -8.9951e-02,  2.4548e-01,\n",
       "          -3.1543e-01,  6.8748e-01,  3.2161e-01, -2.6459e-01, -1.9648e-01,\n",
       "          -3.5435e-01,  7.7361e-02, -1.6708e-01, -6.5166e-01, -2.2686e-01,\n",
       "          -2.1630e-01]],\n",
       "\n",
       "        [[-1.0755e-01,  4.1454e-01, -6.3624e-01, -3.3375e-02,  1.8323e-01,\n",
       "          -1.5138e-01,  5.3609e-01,  2.6428e-01,  1.9110e-01,  3.0038e-02,\n",
       "          -2.3250e-01,  1.6557e-01, -1.2224e-01, -4.2554e-01,  2.9454e-03,\n",
       "           1.7571e-01],\n",
       "         [-3.0431e-02,  4.2130e-01, -6.4025e-01,  2.9888e-02,  1.7236e-01,\n",
       "          -1.5740e-01,  5.7150e-01,  3.3994e-01,  1.5041e-01,  4.5864e-03,\n",
       "          -2.4793e-01,  1.6210e-01, -1.0065e-01, -4.5894e-01,  9.6412e-03,\n",
       "           1.8278e-01],\n",
       "         [-6.7467e-02,  4.1512e-01, -6.2924e-01, -3.4687e-03,  1.9077e-01,\n",
       "          -1.5311e-01,  5.6329e-01,  2.9964e-01,  1.5962e-01,  1.3474e-02,\n",
       "          -2.6885e-01,  1.6469e-01, -1.1038e-01, -4.3612e-01, -1.3232e-02,\n",
       "           1.7507e-01],\n",
       "         [-6.5544e-02,  4.2129e-01, -6.4101e-01, -5.1028e-03,  1.7473e-01,\n",
       "          -1.5577e-01,  5.5381e-01,  3.0647e-01,  1.7412e-01,  2.1444e-02,\n",
       "          -2.3303e-01,  1.6138e-01, -1.0896e-01, -4.4761e-01,  1.1921e-02,\n",
       "           1.7973e-01],\n",
       "         [-7.9209e-02,  4.1517e-01, -6.4129e-01, -2.5239e-03,  1.7360e-01,\n",
       "          -1.5424e-01,  5.4459e-01,  2.9306e-01,  1.7739e-01,  1.6627e-02,\n",
       "          -2.2869e-01,  1.6594e-01, -1.1605e-01, -4.3872e-01,  1.2074e-02,\n",
       "           1.8005e-01]],\n",
       "\n",
       "        [[-1.7637e-01,  3.7893e-01, -6.4285e-01,  8.8977e-02,  2.1773e-01,\n",
       "          -3.7075e-01,  4.2502e-01,  4.0290e-01,  2.4373e-01,  2.2985e-01,\n",
       "          -1.1624e-01,  1.7978e-01,  8.5094e-02, -4.9852e-01, -8.5519e-03,\n",
       "           6.4910e-02],\n",
       "         [-1.7171e-01,  3.8134e-01, -6.3780e-01,  8.4797e-02,  2.1544e-01,\n",
       "          -3.6828e-01,  4.2754e-01,  4.0093e-01,  2.3952e-01,  2.2675e-01,\n",
       "          -1.2363e-01,  1.7481e-01,  8.4071e-02, -5.0000e-01, -8.1927e-03,\n",
       "           6.3953e-02],\n",
       "         [-1.5576e-01,  3.8997e-01, -6.2353e-01,  7.8024e-02,  2.1217e-01,\n",
       "          -3.5574e-01,  4.3837e-01,  4.0466e-01,  2.3126e-01,  2.2298e-01,\n",
       "          -1.4295e-01,  1.6493e-01,  8.7384e-02, -4.9639e-01, -4.8659e-04,\n",
       "           6.1785e-02],\n",
       "         [-1.9072e-01,  3.6594e-01, -6.5356e-01,  7.4995e-02,  2.2052e-01,\n",
       "          -3.7510e-01,  4.1922e-01,  3.9642e-01,  2.4510e-01,  2.3005e-01,\n",
       "          -9.4143e-02,  1.9247e-01,  7.5314e-02, -5.0906e-01, -2.9790e-02,\n",
       "           7.1791e-02],\n",
       "         [-1.8295e-01,  3.7037e-01, -6.4640e-01,  7.7618e-02,  2.1613e-01,\n",
       "          -3.7265e-01,  4.2232e-01,  3.9250e-01,  2.3898e-01,  2.2594e-01,\n",
       "          -1.0647e-01,  1.7887e-01,  7.6559e-02, -5.0460e-01, -2.3219e-02,\n",
       "           6.5768e-02]],\n",
       "\n",
       "        [[-1.5804e-02,  4.0639e-01, -6.2397e-01, -6.8415e-02,  4.3120e-01,\n",
       "          -3.1672e-01,  5.3788e-01,  4.0259e-01, -9.6298e-02, -4.4946e-02,\n",
       "          -3.4755e-01,  4.8829e-01, -4.6331e-02, -3.7721e-01, -1.5251e-02,\n",
       "          -6.9416e-03],\n",
       "         [-9.8516e-03,  4.0402e-01, -6.2506e-01, -6.2212e-02,  4.3620e-01,\n",
       "          -3.1905e-01,  5.3650e-01,  4.1492e-01, -9.5741e-02, -5.2940e-02,\n",
       "          -3.5076e-01,  4.9293e-01, -4.7339e-02, -3.7968e-01, -7.8422e-03,\n",
       "          -1.2251e-02],\n",
       "         [ 1.9694e-03,  4.1878e-01, -6.3442e-01, -5.4956e-02,  4.3427e-01,\n",
       "          -3.1843e-01,  5.5183e-01,  4.2544e-01, -1.1211e-01, -4.9770e-02,\n",
       "          -3.4628e-01,  4.9507e-01, -4.2335e-02, -3.8203e-01, -2.1615e-03,\n",
       "          -4.0791e-03],\n",
       "         [-2.6342e-02,  3.7208e-01, -6.0890e-01, -6.7363e-02,  4.4603e-01,\n",
       "          -3.2902e-01,  5.0037e-01,  4.0687e-01, -6.4345e-02, -7.2974e-02,\n",
       "          -3.6566e-01,  4.9190e-01, -6.0422e-02, -3.8094e-01, -1.1478e-02,\n",
       "          -3.7452e-02],\n",
       "         [-2.0662e-02,  4.0294e-01, -6.2575e-01, -7.0731e-02,  4.2982e-01,\n",
       "          -3.1915e-01,  5.4060e-01,  3.9478e-01, -9.5959e-02, -3.8345e-02,\n",
       "          -3.3997e-01,  4.8766e-01, -4.8864e-02, -3.7428e-01, -2.4151e-02,\n",
       "          -6.3199e-03]],\n",
       "\n",
       "        [[-9.3732e-02,  2.6940e-01, -6.1991e-01, -1.8148e-01,  1.9933e-01,\n",
       "          -2.8054e-01,  5.2727e-01,  1.6899e-01,  3.2907e-02,  1.3375e-01,\n",
       "          -8.9230e-02,  1.9845e-01, -9.9860e-02, -3.5171e-01, -2.6157e-02,\n",
       "          -6.8701e-02],\n",
       "         [-1.0542e-01,  2.7008e-01, -6.2093e-01, -1.8157e-01,  1.9980e-01,\n",
       "          -2.8427e-01,  5.2418e-01,  1.6124e-01,  3.5152e-02,  1.3872e-01,\n",
       "          -7.8307e-02,  1.9642e-01, -1.0581e-01, -3.5261e-01, -2.3102e-02,\n",
       "          -6.7594e-02],\n",
       "         [-8.5494e-02,  2.7150e-01, -6.1438e-01, -1.8060e-01,  1.8928e-01,\n",
       "          -2.8320e-01,  5.3982e-01,  1.6129e-01,  1.9864e-02,  1.4108e-01,\n",
       "          -8.4133e-02,  1.7596e-01, -1.0926e-01, -3.5951e-01, -1.7315e-02,\n",
       "          -7.8598e-02],\n",
       "         [-9.0593e-02,  2.7492e-01, -6.1214e-01, -1.6832e-01,  1.9753e-01,\n",
       "          -2.8573e-01,  5.2505e-01,  1.7568e-01,  2.6585e-02,  1.4143e-01,\n",
       "          -8.0581e-02,  2.0273e-01, -1.0491e-01, -3.4753e-01, -1.8444e-02,\n",
       "          -7.0437e-02],\n",
       "         [-1.1733e-01,  2.7472e-01, -6.1569e-01, -1.6897e-01,  2.0376e-01,\n",
       "          -2.8899e-01,  5.1086e-01,  1.6599e-01,  3.9299e-02,  1.4643e-01,\n",
       "          -6.3525e-02,  2.1023e-01, -1.0886e-01, -3.4540e-01, -1.8081e-02,\n",
       "          -6.0870e-02]],\n",
       "\n",
       "        [[-7.2085e-03,  2.0242e-01, -7.8770e-01,  1.2784e-01,  3.2346e-01,\n",
       "          -3.5007e-01,  6.9527e-01,  2.9035e-01, -8.3507e-02, -2.1216e-02,\n",
       "          -5.3083e-02,  2.4749e-01, -1.3935e-01, -3.5437e-01, -1.7906e-01,\n",
       "           8.6153e-05],\n",
       "         [ 1.0482e-03,  2.1006e-01, -7.9695e-01,  1.2910e-01,  3.2321e-01,\n",
       "          -3.6175e-01,  6.9569e-01,  2.9414e-01, -8.2257e-02, -1.7251e-02,\n",
       "          -5.3389e-02,  2.4558e-01, -1.3297e-01, -3.5106e-01, -1.8339e-01,\n",
       "          -1.6467e-03],\n",
       "         [ 7.4020e-04,  1.8282e-01, -7.8020e-01,  1.3757e-01,  3.3110e-01,\n",
       "          -3.5018e-01,  7.0214e-01,  3.0136e-01, -7.9005e-02, -1.8109e-02,\n",
       "          -4.0341e-02,  2.3935e-01, -1.3158e-01, -3.4990e-01, -1.7139e-01,\n",
       "          -1.3397e-02],\n",
       "         [-5.5519e-03,  2.0073e-01, -7.8461e-01,  1.2010e-01,  3.2423e-01,\n",
       "          -3.5215e-01,  6.9169e-01,  2.9834e-01, -6.8430e-02, -1.4718e-02,\n",
       "          -5.2356e-02,  2.5055e-01, -1.2882e-01, -3.6046e-01, -1.8014e-01,\n",
       "           3.1321e-03],\n",
       "         [ 4.3792e-03,  2.1046e-01, -7.9667e-01,  1.2964e-01,  3.2813e-01,\n",
       "          -3.5514e-01,  7.0459e-01,  3.0496e-01, -9.5018e-02, -1.6961e-02,\n",
       "          -4.9919e-02,  2.5654e-01, -1.3717e-01, -3.5144e-01, -1.7438e-01,\n",
       "          -4.6613e-03]],\n",
       "\n",
       "        [[ 6.8886e-02,  4.6638e-01, -4.4757e-01, -2.1027e-01,  2.1403e-01,\n",
       "           5.8211e-03,  6.4324e-01,  3.8431e-01,  3.5672e-04,  5.8941e-02,\n",
       "          -3.6511e-01,  2.1518e-01, -4.2056e-02, -4.8192e-01,  1.1955e-01,\n",
       "           1.0815e-01],\n",
       "         [ 7.8134e-02,  4.6253e-01, -4.2579e-01, -2.2055e-01,  2.1525e-01,\n",
       "           1.8791e-02,  6.5156e-01,  4.0071e-01, -2.3604e-02,  5.3636e-02,\n",
       "          -3.6182e-01,  2.2424e-01, -4.9091e-02, -5.0174e-01,  1.3873e-01,\n",
       "           1.0544e-01],\n",
       "         [ 7.3603e-02,  4.4893e-01, -4.2886e-01, -2.1693e-01,  2.1262e-01,\n",
       "           2.2715e-03,  6.4119e-01,  3.8208e-01, -1.4983e-02,  4.7915e-02,\n",
       "          -3.6447e-01,  2.0732e-01, -4.6507e-02, -4.9505e-01,  1.1956e-01,\n",
       "           9.4868e-02],\n",
       "         [ 7.7387e-02,  4.3828e-01, -4.0683e-01, -2.1802e-01,  2.1616e-01,\n",
       "           1.9521e-02,  6.5042e-01,  3.9005e-01, -3.5605e-02,  4.6837e-02,\n",
       "          -3.4786e-01,  2.0397e-01, -5.6017e-02, -5.0271e-01,  1.4490e-01,\n",
       "           8.8348e-02],\n",
       "         [ 8.3835e-02,  4.9262e-01, -4.6292e-01, -2.2573e-01,  2.2061e-01,\n",
       "           1.4205e-02,  6.5013e-01,  4.0415e-01, -6.7415e-03,  5.5933e-02,\n",
       "          -3.7783e-01,  2.4275e-01, -3.5167e-02, -4.8235e-01,  1.2916e-01,\n",
       "           1.2124e-01]],\n",
       "\n",
       "        [[ 7.5170e-03,  2.4746e-01, -5.8548e-01, -2.1807e-02,  2.5714e-01,\n",
       "          -3.0188e-01,  6.9924e-01,  3.2532e-01, -2.4531e-02,  2.8298e-02,\n",
       "          -1.5980e-01,  2.7199e-01, -1.6699e-01, -4.2600e-01,  9.3501e-02,\n",
       "          -1.1685e-01],\n",
       "         [ 2.2948e-02,  2.3550e-01, -5.6282e-01, -4.2856e-02,  2.4474e-01,\n",
       "          -2.9440e-01,  6.9169e-01,  3.3333e-01, -4.3801e-02,  1.1148e-02,\n",
       "          -1.6870e-01,  2.7341e-01, -1.7011e-01, -4.5179e-01,  8.0561e-02,\n",
       "          -1.1976e-01],\n",
       "         [-4.0726e-03,  2.5814e-01, -6.0310e-01, -1.7693e-02,  2.6401e-01,\n",
       "          -3.1742e-01,  7.0996e-01,  2.9851e-01, -2.3831e-02,  3.9345e-02,\n",
       "          -1.5529e-01,  2.5224e-01, -1.6444e-01, -4.0880e-01,  9.0226e-02,\n",
       "          -1.1927e-01],\n",
       "         [ 8.9744e-03,  2.5386e-01, -5.8966e-01, -2.5341e-02,  2.5622e-01,\n",
       "          -3.1179e-01,  7.0709e-01,  3.1476e-01, -3.5247e-02,  2.6307e-02,\n",
       "          -1.6246e-01,  2.5987e-01, -1.6549e-01, -4.2918e-01,  8.8560e-02,\n",
       "          -1.1839e-01],\n",
       "         [ 6.1241e-03,  2.3614e-01, -5.7934e-01, -3.3974e-02,  2.6690e-01,\n",
       "          -3.0515e-01,  6.9985e-01,  3.0969e-01, -2.9959e-02,  2.1379e-02,\n",
       "          -1.6152e-01,  2.5594e-01, -1.6852e-01, -4.1696e-01,  8.6254e-02,\n",
       "          -1.3090e-01]],\n",
       "\n",
       "        [[ 1.2848e-01,  4.2649e-01, -6.1849e-01, -6.1139e-02,  2.7327e-01,\n",
       "          -2.6265e-01,  8.7039e-01,  2.2752e-01, -1.3141e-01,  4.6065e-02,\n",
       "          -2.2038e-01,  1.5056e-01, -2.6300e-01, -4.0572e-01,  8.1474e-02,\n",
       "          -4.8831e-02],\n",
       "         [ 1.3643e-01,  4.2589e-01, -6.2183e-01, -7.0965e-02,  2.7341e-01,\n",
       "          -2.6114e-01,  8.7271e-01,  2.3256e-01, -1.3057e-01,  4.2218e-02,\n",
       "          -2.2986e-01,  1.5230e-01, -2.5850e-01, -4.0720e-01,  7.6876e-02,\n",
       "          -5.2210e-02],\n",
       "         [ 1.4243e-01,  4.3593e-01, -6.4489e-01, -4.3961e-02,  2.7963e-01,\n",
       "          -2.8666e-01,  8.7107e-01,  2.5759e-01, -1.0203e-01,  5.7728e-02,\n",
       "          -2.2847e-01,  1.6548e-01, -2.4045e-01, -4.0569e-01,  8.3537e-02,\n",
       "          -5.4524e-02],\n",
       "         [ 1.4982e-01,  4.2815e-01, -6.2880e-01, -8.3315e-02,  2.8137e-01,\n",
       "          -2.6578e-01,  8.8166e-01,  2.4246e-01, -1.2689e-01,  3.4010e-02,\n",
       "          -2.4107e-01,  1.6056e-01, -2.5542e-01, -4.1119e-01,  7.6085e-02,\n",
       "          -5.4920e-02],\n",
       "         [ 1.3140e-01,  4.0751e-01, -5.9964e-01, -7.8638e-02,  2.5299e-01,\n",
       "          -2.4258e-01,  8.6677e-01,  2.0539e-01, -1.5541e-01,  3.2079e-02,\n",
       "          -2.3078e-01,  1.2500e-01, -2.7164e-01, -4.0280e-01,  6.2119e-02,\n",
       "          -6.0626e-02]],\n",
       "\n",
       "        [[ 1.5139e-02,  2.8492e-01, -6.3906e-01, -4.9558e-02,  2.8537e-01,\n",
       "          -3.6692e-01,  5.7312e-01,  3.7895e-01, -4.2300e-03,  7.7914e-02,\n",
       "          -1.7886e-01,  2.9132e-01, -1.0923e-01, -4.5762e-01,  8.0786e-02,\n",
       "          -1.6336e-01],\n",
       "         [ 3.9781e-03,  2.6261e-01, -6.2566e-01, -5.0836e-02,  3.0184e-01,\n",
       "          -3.5376e-01,  5.7409e-01,  3.8379e-01,  6.2643e-04,  5.9987e-02,\n",
       "          -1.8324e-01,  2.9253e-01, -1.0804e-01, -4.5527e-01,  1.0307e-01,\n",
       "          -1.7569e-01],\n",
       "         [ 1.3994e-03,  2.6799e-01, -6.3773e-01, -4.5386e-02,  2.9614e-01,\n",
       "          -3.5988e-01,  5.7143e-01,  3.8272e-01,  2.7612e-03,  6.4258e-02,\n",
       "          -1.7470e-01,  2.9070e-01, -1.0641e-01, -4.6013e-01,  9.3013e-02,\n",
       "          -1.6641e-01],\n",
       "         [-2.2863e-02,  2.9551e-01, -6.4082e-01, -4.2893e-02,  2.9220e-01,\n",
       "          -3.5952e-01,  5.7040e-01,  3.5923e-01,  5.3209e-03,  8.7073e-02,\n",
       "          -1.6829e-01,  2.8158e-01, -1.0604e-01, -4.5295e-01,  1.0207e-01,\n",
       "          -1.4910e-01],\n",
       "         [ 3.1681e-02,  2.8571e-01, -6.3036e-01, -6.0353e-02,  2.8698e-01,\n",
       "          -3.6467e-01,  5.7695e-01,  3.8551e-01, -7.9821e-03,  7.5374e-02,\n",
       "          -1.9426e-01,  2.9762e-01, -1.1050e-01, -4.5290e-01,  8.1150e-02,\n",
       "          -1.7317e-01]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
