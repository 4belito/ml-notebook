{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "154968d9",
   "metadata": {},
   "source": [
    "# Multi-Head Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b5e26e",
   "metadata": {},
   "source": [
    "## Definition: Attention($\\operatorname{Attention}$)\n",
    "\n",
    "For a set of key-value pairs $\\{(k_i,v_i)\\}_{i=1}^N \\subset \\mathbb{R}^{d_k\\times d_v}$ and another set of queries $\\{q_j\\}_{j=1}^M \\subset \\mathbb{R}^{d_k}$, atention returns the \"expected\" value $o_j \\in \\mathbb{R}^{d_v}$ for each querry $q_j, \\ j=1,2,\\dots,M$.\n",
    "\n",
    "$$ \\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ee4d4",
   "metadata": {},
   "source": [
    "Inputs\n",
    "* a set of queries $Q = [q_1|q_2|\\dots|q_M]^T \\in \\mathbb{R}^{M \\times d_k}$\n",
    "* a set of keys $K = [k_1|k_2|\\dots|k_N]^T \\in \\mathbb{R}^{N \\times d_k}$\n",
    "* a set of values $V = [v_1|v_2|\\dots|v_N]^T \\in \\mathbb{R}^{N \\times d_v}$\n",
    "\n",
    "Output: $O= [o_1|o_2|\\dots|o_M]^T \\in\\mathbb{R}^{M \\times d_v}$\n",
    "$$ \\text{Attention}(Q,K,V) = O  = \\alpha V, \\quad \\text{ where } \\quad \\alpha = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) \\in \\mathbb{R}^{M\\times N}$$\n",
    "$$ o_{i}  =  \\sum_{j=1}^N\\alpha_{i,j}v_j\\, \\quad \\text{ where } \\quad  \\alpha_{i,j}=\\frac{e^{\\frac{q_i^{T}k_j}{\\sqrt{d_k}}}}{\\sum_{l=1}^Ne^{\\frac{q_i^{T}k_l}{\\sqrt{d_k}}}}$$\n",
    "\n",
    "\n",
    "where $\\text{softmax}$ is applied rowwise, so each row of $\\alpha$ sums to one. $\\sum_{j=1}^N\\alpha_{i,j}=1, \\ i =1,2,\\dots,M$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e09c25",
   "metadata": {},
   "source": [
    "**Note:** The output $o_i$ does not depends on $q_j$ for any $j\\neq i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c099fa",
   "metadata": {},
   "source": [
    "**Note:** If we denote by $v(q_i)$ the random variable \"value of the querry $q_i$\", for $i=1,2,\\dots,M$, with the induced probability\n",
    "$$p(v(q_i) = v_j)=\\alpha_{i,j}, \\quad j=1,2,\\dots,N,$$\n",
    "Then\n",
    "$$\\mathbb{E}[v(q_i)]=\\sum_{i=1}^{N}\\alpha_{i,j}v_{j}=o_i, \\quad j=1,2,\\dots,N,$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283e2ee1",
   "metadata": {},
   "source": [
    "**Note:** The value $p(v(q_i) = v_j)=\\alpha_{i,j}$ is ussually interpreted as how much attention (the value $o_i$ of) the querry $q_i$ pays to value $v_j$. So the \"attention\" of $q_i$ is partitioned along the values $v_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22207545",
   "metadata": {},
   "source": [
    "### Property: $\\text{Attention}$ Querry-Deletion Equivariance\n",
    "\n",
    "If $A^{(i)}$ denotes the matrix obtained by deleting the $i$-th row of the matrix $A$, then  \n",
    "$$\\text{Attention}(Q,K,V)^{(i)}=\\text{Attention}(Q^{(i)},K,V). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd7cde8",
   "metadata": {},
   "source": [
    "**Proof:**\n",
    "Notice that $A^{(i)}=I^{(i)}A$, where $I$ is the identity matrix. Using this, we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Attention}(Q^{(i)},K,V) & = \\text{Attention}(I^{(i)}Q,K,V),\\\\\n",
    "& = \\text{softmax}\\left(\\frac{(I^{(i)}Q)K^T}{\\sqrt{d_k}}\\right)V,\\\\\n",
    "& = \\text{softmax}\\left(I^{(i)}\\frac{QK^T}{\\sqrt{d_k}}\\right)V,\\\\\n",
    "& = I^{(i)}\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V,\\\\\n",
    "& = \\text{Attention}(Q,K,V)^{(i)} ,\n",
    "\\end{align*}\n",
    "$$\n",
    "where in the fourth equality we have used that the softmax funtion is applied rowwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297bb21b",
   "metadata": {},
   "source": [
    "### Property: $\\text{Attention}$ Key-Value Permutation Invariance\n",
    "\n",
    "If $\\pi_r(A)$ denotes an arbitrary permutation over the rows of a matrix $A$, then\n",
    "$$\\text{Attention}(Q,\\pi_r(K),\\pi_r(V))=\\text{Attention}(Q,K,V). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f4cb47",
   "metadata": {},
   "source": [
    "**Proof:**\n",
    "\n",
    "Notice that $\\pi_r(A)$ can be written as $\\pi_r(A) = R_{\\pi}A$ for some permutation matrix $R_{\\pi}$. A permutation matrix is a square binary matrix that has exactly one entry of value 1 in each row and each column with all other entries 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd7f21b",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Attention}(Q,\\pi_r(K),\\pi_r(V)) & = \\text{Attention}(Q,R_{\\pi}K,R_{\\pi}V),\\\\\n",
    "& = \\text{softmax}\\left(\\frac{Q(R_{\\pi}K)^T}{\\sqrt{d_k}}\\right)R_{\\pi}V,\\\\\n",
    "& = \\text{softmax}\\left(\\frac{QK^TR_{\\pi}^{T}}{\\sqrt{d_k}}\\right)R_{\\pi}V,\\\\\n",
    "& = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}R_{\\pi}^{T}\\right)R_{\\pi}V,\\\\\n",
    "& = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)R_{\\pi}^{T}R_{\\pi}V,\\\\\n",
    "& = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V,\\\\\n",
    "& = \\text{Attention}(Q,K,V),\n",
    "\\end{align*}\n",
    "$$\n",
    "where in the fifth equality we used that postmultiply by a permutation matrix is a column permutation and softmax is permutation equivariant. Notice that for any permutation $\\pi$ we have \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{softmax}\\left(\\pi(x)\\right)& = \\text{softmax}\\left(x_{\\pi(1)},x_{\\pi(2)},\\dots,x_{\\pi(n)}\\right),\\\\\n",
    "&=\\frac{\\left(e^{x_{\\pi(1)}},e^{x_{\\pi(2)}},\\dots,e^{x_{\\pi(n)}}\\right)}{\\sum_{i=1}^n e^{x_{\\pi(i)}}},\\\\\n",
    "&=\\frac{\\left(e^{x_{\\pi(1)}},e^{x_{\\pi(2)}},\\dots,e^{x_{\\pi(n)}}\\right)}{\\sum_{i=1}^n e^{x_{i}}},\\\\\n",
    "&=\\frac{\\pi\\left(e^{x_{1}},e^{x_{2}},\\dots,e^{x_{n}}\\right)}{\\sum_{i=1}^n e^{x_{i}}},\\\\\n",
    "&=\\pi\\left(\\frac{\\left(e^{x_{1}},e^{x_{2}},\\dots,e^{x_{n}}\\right)}{\\sum_{i=1}^n e^{x_{i}}}\\right),\\\\\n",
    "&=\\pi\\left(\\text{softmax}(x)\\right),\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "and $\\text{softmax}$ is applied to each row. So if $A = [a_1|a_2|\\dots|a_n]^T = \\frac{QK^T}{\\sqrt{d_k}}$, $f=\\text{softmax}$, and a column permutation $\\pi_c(A) = [\\pi(a_1)|\\pi(a_2)|\\dots|\\pi(a_n)]^T = AC_{\\pi}$, where  $C_{\\pi}$ is a permutation matrix, we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(AC_{\\pi}) & = f([\\pi(a_1)|\\pi(a_2)|\\dots|\\pi(a_n)]^T),\\\\\n",
    " & = ([f(\\pi(a_1))|f(\\pi(a_2))|\\dots|f(\\pi(a_n))]^T),\\\\\n",
    "& = ([\\pi(f(a_1))|\\pi(f(a_2))|\\dots|\\pi(f(a_n))]^T),\\\\\n",
    "& = \\pi_c\\left([f(a_1)|f(a_2)|\\dots|f(a_n)]^T\\right),\\\\\n",
    "& = \\pi_c\\left(f(A)\\right),\\\\\n",
    "& =f(A)C_{\\pi}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90689ed",
   "metadata": {},
   "source": [
    "### Property: $\\operatorname{Attention}$ Permutation Equivariance\n",
    "If $\\pi_r$ and $\\sigma_r$ denote arbitrary permutations over the rows of a matrix, then\n",
    "$$\\text{Attention}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))=\\pi_r\\left(\\text{Attention}(Q,K,V)\\right). $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3895a9",
   "metadata": {},
   "source": [
    "**Proof:**\n",
    "Consider  $\\pi_r(M) = R_{\\pi}M$ for some permutation matrix $R_{\\pi}$. From the previous result we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Attention}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V)) & = \\text{Attention}(\\pi_r(Q),K,V),\\\\\n",
    "&= \\text{Attention}(R_{\\pi}Q,K,V),\\\\\n",
    "& = \\text{softmax}\\left(\\frac{(R_{\\pi}Q)K^T}{\\sqrt{d_k}}\\right)V,\\\\\n",
    "& = \\text{softmax}\\left(R_{\\pi}\\frac{QK^T}{\\sqrt{d_k}}\\right)V,\\\\\n",
    "& = R_{\\pi}\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V,\\\\\n",
    "& = \\pi_r\\left(\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\\right),\\\\\n",
    "& = \\pi_r\\left(\\text{Attention}(Q,K,V) \\right),\n",
    "\\end{align*}\n",
    "$$\n",
    "where in the fifth equality we have used that the softmax funtion is applied rowwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97457b7d",
   "metadata": {},
   "source": [
    "## Definition: Masked Attention ($\\text{MAttn}_{B}$)\n",
    "\n",
    "Masked attention intentionally breaks permutation equivariance so that order matters. In many tasks, like language modeling or temporal prediction, we donâ€™t want tokens to freely attend to all others. Masking restricts attention to valid positions (e.g., past tokens), enforcing causal or directional structure instead of treating the sequence as an unordered set.\n",
    "$$ \\text{MAttn}_{B}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T+B}{\\sqrt{d_k}}\\right) V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673e2a82",
   "metadata": {},
   "source": [
    "Inputs\n",
    "* a set of queries $Q = [q_1|q_2|\\dots|q_M]^T \\in \\mathbb{R}^{M \\times d_k}$\n",
    "* a set of keys $K = [k_1|k_2|\\dots|k_N]^T \\in \\mathbb{R}^{N \\times d_k}$\n",
    "* a set of values $V = [v_1|v_2|\\dots|v_N]^T \\in \\mathbb{R}^{N \\times d_v}$\n",
    "\n",
    "Output: $O= [o_1|o_2|\\dots|o_M]^T \\in\\mathbb{R}^{M \\times d_v}$\n",
    "$$ \\text{MAttn}_{B}(Q,K,V) = O  = \\alpha V, \\quad \\text{ where } \\quad \\alpha = \\text{softmax}\\left(\\frac{QK^T+B}{\\sqrt{d_k}}\\right) \\in \\mathbb{R}^{M\\times N}$$\n",
    "where $B\\in\\mathbb{R}^{M\\times N}$, and $\\text{softmax}$ is applied to each row, so each row of $\\alpha$ sums to one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f548a734",
   "metadata": {},
   "source": [
    "**Note:** In general, the mask breaks the permutation equivariance properties of attention. Notice also that the $\\operatorname{Attention}$ function definition now depends on the sequence lengths $M$ and $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34c8f18",
   "metadata": {},
   "source": [
    "### Property: $\\text{MAtten}_B$ Querry-Deletion\n",
    "If $A^{(i)}$ denotes the matrix obtained by deleting the $i$-th row of the matrix $A$, then \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{MAttn}_{B}(Q,K,V)^{(i)}=\\text{MAttn}_{B^{(i)}}(Q^{(i)},K,V)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9687c3b0",
   "metadata": {},
   "source": [
    "**Proof:**\n",
    "Notice that $A^{(i)}=I^{(i)}A$, where $I$ is the identity matrix. Using this, we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{MAttn}_{B^{(i)}}(Q^{(i)},K,V) & = \\text{MAttn}_{I^{(i)}B}(I^{(i)}Q,K,V),\\\\\n",
    "& = \\text{softmax}\\left(\\frac{(I^{(i)}Q)K^T+I^{(i)}B}{\\sqrt{d_k}}\\right)V,\\\\\n",
    "& = \\text{softmax}\\left(I^{(i)}\\frac{QK^T+B}{\\sqrt{d_k}}\\right)V,\\\\\n",
    "& = I^{(i)}\\text{softmax}\\left(\\frac{QK^T+B}{\\sqrt{d_k}}\\right)V,\\\\\n",
    "& = \\text{MAttn}_{B}(Q,K,V)^{(i)} ,\n",
    "\\end{align*}\n",
    "$$\n",
    "where in the fourth equality we have used that the softmax funtion is applied rowwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabd690b",
   "metadata": {},
   "source": [
    "### Property: The entries of $B$ in $\\text{MAttn}_{B}$ can selectively suppress attention.\n",
    "\n",
    "If $b_{i,s} \\to -\\infty$, then the $i$-th output $o_i$ does not depend on $k_s$ or $v_s$. In particular, we have the formula:\n",
    "$$ \n",
    "\\begin{align*}\n",
    "o_i \\to \\frac{1}{\\sum_{l=1,l\\neq s}^Ne^{\\frac{q_i^{T}k_l+b_{i,l}}{\\sqrt{d_k}}}}\\sum_{j=1,j\\neq s}^Ne^{\\frac{q_i^{T}k_j+b_{i,j}}{\\sqrt{d_k}}} v_j.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Note:** Remeber that $o_i$ does not depend on any $q_j$ with $j \\neq i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da83900",
   "metadata": {},
   "source": [
    "**Proof:** If $B_{i,s}\\to -\\infty$ for some $(i,s)\\in M\\times N$ then\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\alpha_{i,j} &= \\text{softmax}\\left(\\frac{QK^T+B}{\\sqrt{d_k}}\\right)_{i,j} =\\frac{e^{\\frac{q_i^{T}k_j+b_{i,j}}{\\sqrt{d_k}}}}{\\sum_{l=1}^Ne^{\\frac{q_i^{T}k_l+b_{i,l}}{\\sqrt{d_k}}}},\\\\\n",
    "&\\to\\begin{cases}\n",
    "\\frac{e^{\\frac{q_i^{T}k_j+b_{i,j}}{\\sqrt{d_k}}}}{\\sum_{l=1,l\\neq s}^Ne^{\\frac{q_i^{T}k_l+b_{i,l}}{\\sqrt{d_k}}}},&\\quad j\\neq s\\\\\n",
    "0 ,&\\quad j = s.\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "Then, we get\n",
    "$$ \n",
    "\\begin{align*}\n",
    "o_i &= \\sum_{j=1}^N\\alpha_{i,j} v_j\\to\\sum_{j=1,j\\neq s}^N\\alpha_{i,j} v_j,\\\\\n",
    "&=\\sum_{j=1,j\\neq s}^N\\frac{e^{\\frac{q_i^{T}k_j+b_{i,j}}{\\sqrt{d_k}}}}{\\sum_{l=1,l\\neq s}^Ne^{\\frac{q_i^{T}k_l+b_{i,l}}{\\sqrt{d_k}}}} v_j.\n",
    "\\end{align*}\n",
    "$$\n",
    "Therefore, we conclude that $o_i$ does not depend on $k_s$ or $v_s$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04fa60a",
   "metadata": {},
   "source": [
    "### Property: $\\text{MAtten}_B$ Key-Value deletion\n",
    "\n",
    "If $A^{(i)}$ and $A^{[i]}$ denote the matrix obtained by deleting the $i$-th row and column resectively, and $B_{i,s}\\to-\\infty$ for $i=1,2,\\dots,M$ then \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{MAttn}_{B}(Q,K,V) \\to \\text{MAttn}_{B^{[s]}}(Q,K^{(s)},V^{(s)}), \\quad Q \\in \\mathbb{R}^{M\\times d_k}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78846c3",
   "metadata": {},
   "source": [
    "**Proof:** From the previous property, for $i=1,2,\\dots,M$ we have\n",
    "$$ \n",
    "\\begin{align*}\n",
    "o_i & \\to\\sum_{j=1,j\\neq s}^N \\frac{e^{\\frac{q_i^{T}k_j+b_{i,j}}{\\sqrt{d_k}}}}{\\sum_{l=1,l\\neq s}^Ne^{\\frac{q_i^{T}k_l+b_{i,l}}{\\sqrt{d_k}}}} v_j,\\\\\n",
    "&=\\sum_{j=1,j\\neq s}^N \\text{Softmax}\\left(\\frac{\\left(QK^T+B\\right)^{[s]}}{\\sqrt{d_k}}\\right)_{i,j}v_j,\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "Appliying this result to every row, we have\n",
    "$$ \n",
    "\\begin{align*}\n",
    "O & \\to \\text{Softmax}\\left(\\frac{\\left(QK^T+B\\right)^{[s]}}{\\sqrt{d_k}}\\right)V^{(s)},\\\\\n",
    "&=\\text{Softmax}\\left(\\frac{Q(K^T)^{[s]}+B^{[s]}}{\\sqrt{d_k}}\\right)V^{(s)},\\\\\n",
    "&=\\text{Softmax}\\left(\\frac{Q(K^{(s)})^T+B^{[s]}}{\\sqrt{d_k}}\\right)V^{(s)},\\\\\n",
    "&=\\text{MAttn}_{B^{[s]}}(Q,K^{(s)},V^{(s)}).\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08462cec",
   "metadata": {},
   "source": [
    "## Definition: Attention with weights($\\text{Attn}_{\\mathcal{W}}$)\n",
    "\n",
    "$$\\text{Attn}_{\\mathcal{W}}(Q,K,V)=\\text{Attention}(QW_q+1_Mb^T_q,KW_k+1_Mb^T_k,VW_v+1_Mb^T_v)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92708176",
   "metadata": {},
   "source": [
    "\n",
    "Inputs\n",
    "* a set of queries $Q = [q_1|q_2|\\dots|q_M]^T \\in \\mathbb{R}^{M \\times c_q}, \\quad M\\in\\mathbb{N}$\n",
    "* a set of keys $K = [k_1|k_2|\\dots|k_N]^T \\in \\mathbb{R}^{N \\times c_k}, \\quad N\\in\\mathbb{N}$\n",
    "* a set of values $V = [v_1|v_2|\\dots|v_N]^T \\in \\mathbb{R}^{N \\times c_v}, \\quad N\\in\\mathbb{N}$.\n",
    "\n",
    "Weights\n",
    "* a set of querry weights $W_q \\in \\mathbb{R}^{c_q\\times d_k}$ and bias $b_q\\in \\mathbb{R}^{d_k}$\n",
    "* a set of key weights $W_k \\in \\mathbb{R}^{c_k \\times d_k}$ and bias $b_k\\in \\mathbb{R}^{d_k}$\n",
    "* a set of value weights $W_v \\in \\mathbb{R}^{c_v \\times d_v}$ and bias $b_q\\in \\mathbb{R}^{d_v}$\n",
    "\n",
    "Output: \n",
    "* Output $O= [o_1|o_2|\\dots|o_N]^T \\in\\mathbb{R}^{M\\times d_v}$\n",
    "\\begin{align*}\n",
    "O =\\text{Attn}_{\\mathcal{W}}(Q,K,V)=\\text{Attention}(QW_q+1_Mb^T_q,KW_k+1_Mb^T_k,VW_v+1_Mb^T_v),\n",
    "\\end{align*}\n",
    "where\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{W} &= \\left(W_q,b_q,W_k,b_k,W_v,b_v\\right),\\\\\n",
    "1_M &= (1,1,\\dots,1)\\in \\mathbb{R}^M.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dbe492",
   "metadata": {},
   "source": [
    "**Note:** The same $\\operatorname{Attention}$ function can be applied to inputs of different sequence lengths. The model parameters are not tied to specific positions. In this sense, $\\operatorname{Attention}$ is **position-agnostic**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ce8c8d",
   "metadata": {},
   "source": [
    "**Note:** From the $\\text{Attention}$ definition we have \n",
    "\n",
    "\\begin{align*}\n",
    "o_i = \\frac{1}{\\sum_{l=1}^Ne^{\\frac{\\hat{q}_i^{T}\\hat{k}_l+b_{i,l}}{\\sqrt{d_k}}}}\\sum_{j=1}^Ne^{\\frac{\\hat{q}_i^{T}\\hat{k}_j}{\\sqrt{d_k}}} \\hat{v}_j.\n",
    "\\end{align*}\n",
    "\n",
    "where\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\hat{q}_i&=W_q^Tq_i+b_q,\\\\\n",
    "\\hat{k}_i&=W_k^Tk_i+b_k,\\\\\n",
    "\\hat{v}_i&=W_v^Tv_i+b_v,\\\\\n",
    "\\end{align*}\n",
    "$$ \n",
    "So, the output $o_i$ does not depends on $q_j$ for any $j\\neq i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e48764",
   "metadata": {},
   "source": [
    "### Property: $\\text{Attn}_{\\mathcal{W}}$ Querry-Deletion Equivariance\n",
    "\n",
    "If $A^{(i)}$ denotes the matrix obtained by deleting the $i$-th row of the matrix $A$, then  \n",
    "$$\\text{Attn}_{\\mathcal{W}}(Q,K,V)^{(i)}=\\text{Attn}_{\\mathcal{W}}(Q^{(i)},K,V). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc4e0b",
   "metadata": {},
   "source": [
    "**Proof:**\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\operatorname{Attn}_{\\mathcal{W}}(Q^{(i)},K,V)\n",
    "&=\\text{Attention}(Q^{(i)}W_q+1_{M-1}b_q^T,KW_k+1_Mb_k^T,VW_v+1_Mb_v^T),\\\\\n",
    "&=\\text{Attention}((QW_q)^{(i)}+(1_{M}b_q^T)^{(i)},KW_k+1_Mb_k^T,VW_v+1_Mb_v^T),\\\\\n",
    "&=\\text{Attention}((QW_q+1_{M}b_q^T)^{(i)},KW_k+1_Mb_k^T,VW_v+1_Mb_v^T),\\\\\n",
    "&=\\text{Attention}(QW_q+1_{M}b_q^T,KW_k+1_Mb_k^T,VW_v+1_Mb_v^T)^{(i)},\\\\\n",
    "&=\\operatorname{Attn}_{\\mathcal{W}}(Q,K,V)^{(i)}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6f0ad6",
   "metadata": {},
   "source": [
    "### Property: $\\text{Attn}_{\\mathcal{W}}$ Permutation Equivariance\n",
    "\n",
    "If Attention with weights has no bias (projections are linear instead of affine), the Attention is permutation equivariant. This means, if $\\pi_r$ and $\\sigma_r$ denote arbitrary permutations over the rows of a matrix, then\n",
    "$$\\operatorname{Attn}_{\\mathcal{W}}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))=\\pi_r\\left(\\operatorname{Attn}_{\\mathcal{W}}(Q,K,V)\\right) $$\n",
    "where \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{W} &= \\left(W_q,0_{\\mathbb{R}^{d_k}},W_k,0_{\\mathbb{R}^{d_k}},W_v,0_{\\mathbb{R}^{d_v}}\\right)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28a8da0",
   "metadata": {},
   "source": [
    "Proof: Consider  $\\pi_r(M) = R_{\\pi}M$ and  $\\sigma_r(M) = R_{\\sigma}M$ for some permutation matrix $R_{\\pi}$ and $R_{\\sigma}$ respectively. From the previous result we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\operatorname{Attn}_{\\mathcal{W}}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))\n",
    "&=\\text{Attention}(\\pi_r(Q)W_q,\\sigma_r(K)W_k,\\sigma_r(V)W_v),\\\\\n",
    "&=\\text{Attention}((R_{\\pi}Q)W_q,(R_{\\sigma}K)W_k,(R_{\\sigma}V)W_v),\\\\\n",
    "&=\\text{Attention}(R_{\\pi}(QW_q),R_{\\sigma}(KW_k),R_{\\sigma}(VW_v)),\\\\\n",
    "&=\\text{Attention}(\\pi_r(QW_q),\\sigma_r(KW_k),\\sigma_r(VW_v)),\\\\\n",
    "&=\\pi_r\\left(\\text{Attention}(QW_q,KW_k,VW_v)\\right),\\\\\n",
    "&=\\pi_r\\left(\\operatorname{Attn}_{\\mathcal{W}}(Q,K,V)\\right).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8991a9bb",
   "metadata": {},
   "source": [
    "## Definition: Masked Attention with weights ($\\text{MAttn}_{\\mathcal{W}}$)\n",
    "$$\\text{MAttn}_{\\mathcal{W}}(Q,K,V)=\\text{MAttn}_B(QW_q+1_Mb_k^T,KW_k+1_Mb_k^T,VW_v+1_Mb_v^T)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72b3d53",
   "metadata": {},
   "source": [
    "Inputs\n",
    "* a set of queries $Q = [q_1|q_2|\\dots|q_M]^T \\in \\mathbb{R}^{M \\times c_q}$\n",
    "* a set of keys $K = [k_1|k_2|\\dots|k_N]^T \\in \\mathbb{R}^{N \\times c_k}$\n",
    "* a set of values $V = [v_1|v_2|\\dots|v_N]^T \\in \\mathbb{R}^{N \\times c_v}$.\n",
    "\n",
    "Weights\n",
    "* a set of querry weights $W_q \\in \\mathbb{R}^{c_q\\times d_k}$ and bias $b_q\\in \\mathbb{R}^{d_k}$\n",
    "* a set of key weights $W_k \\in \\mathbb{R}^{c_k \\times d_k}$ and bias $b_k\\in \\mathbb{R}^{d_k}$\n",
    "* a set of value weights $W_v \\in \\mathbb{R}^{c_v \\times d_v}$ and bias $b_q\\in \\mathbb{R}^{d_v}$\n",
    "\n",
    "* a mask matrix $B\\in \\mathbb{R}^{M\\times N}$.\n",
    "\n",
    "Output: \n",
    "* Output $O= [o_1|o_2|\\dots|o_N]^T \\in\\mathbb{R}^{M\\times d_v}$\n",
    "\\begin{align*}\n",
    "O =\\text{MAttn}_{\\mathcal{W}}(Q,K,V)=\\text{MAttn}_B(QW_q+1_Mb_k^T,KW_k+1_Mb_k^T,VW_v+1_Mb_v^T)\n",
    "\\end{align*}\n",
    "where\n",
    "\\begin{align*}\n",
    "1_M &= (1,1,\\dots,1) \\in \\mathbb{R}^{M},\\\\ \n",
    "\\mathcal{W} &= \\left(W_q,b_q,W_k,b_k,W_v,b_v,B\\right).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d53e15",
   "metadata": {},
   "source": [
    "**Note:** The mask breaks the permutation-equivariance property of attention. In general, attention is not even position-agnostic, since the entries of $B$ depend on the input sequence lengths $M$ and $N$. Although this limitation can be mitigated by imposing specific structures on $B$, the masked version remains inherently order-dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eabe0ea",
   "metadata": {},
   "source": [
    "### Property: $\\text{MAttn}_{\\mathcal{W}}$ Querry-Deletion\n",
    "If $A^{(i)}$ denotes the matrix obtained by deleting the $i$-th row of the matrix $A$, then \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{MAttn}_{\\mathcal{W}}(Q,K,V)^{(i)}=\\text{MAttn}_{{\\mathcal{W}}^{(i)}}(Q^{(i)},K,V)\n",
    "\\end{align*}\n",
    "$$\n",
    "where\n",
    "\\begin{align*}\n",
    "\\mathcal{W}^{(i)} &= \\left(W_q,b_q,W_k,b_k,W_v,b_v,B^{(i)}\\right).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6e2f54",
   "metadata": {},
   "source": [
    "**Proof:**\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\operatorname{MAttn}_{\\mathcal{W}}(Q^{(i)},K,V)\n",
    "&=\\text{MAttn}_{B}(Q^{(i)}W_q+1_{M-1}b_q^T,KW_k+1_Mb_k^T,VW_v+1_Mb_v^T),\\\\\n",
    "&=\\text{MAttn}_{B}((QW_q)^{(i)}+(1_{M}b_q^T)^{(i)},KW_k+1_Mb_k^T,VW_v+1_Mb_v^T),\\\\\n",
    "&=\\text{MAttn}_{B}((QW_q+1_{M}b_q^T)^{(i)},KW_k+1_Mb_k^T,VW_v+1_Mb_v^T),\\\\\n",
    "&=\\text{MAttn}_{B^{(i)}}(QW_q+1_{M}b_q^T,KW_k+1_Mb_k^T,VW_v+1_Mb_v^T)^{(i)},\\\\\n",
    "&=\\operatorname{MAttn}_{\\mathcal{W}^{(i)}}(Q,K,V)^{(i)}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f80602",
   "metadata": {},
   "source": [
    "### Property: The entries of $B$ in $\\text{MAttn}_{\\mathcal{W}}$ can selectively suppress attention in the Attention with weights.\n",
    "\n",
    "If $b_{i,s} \\to -\\infty$, then the $i$-th output $o_i$ does not depend on $k_s$ or $v_s$. In particular, we have\n",
    "\\begin{align*}\n",
    "o_i \\to \\frac{1}{\\sum_{l=1,l\\neq s}^Ne^{\\frac{\\hat{q}_i^{T}\\hat{k}_l+b_{i,l}}{\\sqrt{d_k}}}}\\sum_{j=1,j\\neq s}^Ne^{\\frac{\\hat{q}_i^{T}\\hat{k}_j+b_{i,j}}{\\sqrt{d_k}}} \\hat{v}_j.\n",
    "\\end{align*}\n",
    "where\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\hat{q}_i&=W_q^Tq_i+b_q,\\\\\n",
    "\\hat{k}_i&=W_k^Tk_i+b_k,\\\\\n",
    "\\hat{v}_i&=W_v^Tv_i+b_v,\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c97d494",
   "metadata": {},
   "source": [
    "**Proof:** If $b_{i,s}\\to -\\infty$ for some $(i,s)\\in M\\times N$, we can apply the result for the Attention function to obtain \n",
    "$$ \n",
    "\\begin{align*}\n",
    "o_i \\to \\frac{1}{\\sum_{l=1,l\\neq s}^Ne^{\\frac{\\hat{q}_i^{T}\\hat{k}_l+b_{i,l}}{\\sqrt{d_k}}}}\\sum_{j=1,j\\neq s}^Ne^{\\frac{\\hat{q}_i^{T}\\hat{k}_j+b_{i,j}}{\\sqrt{d_k}}} \\hat{v}_j.\n",
    "\\end{align*}\n",
    "$$\n",
    "where the vectors $\\hat{q}_i$, $\\hat{k}_i$ and $\\hat{v}_i$ are the $i$-th rows of $QW_q+1_Mb^T_q$, $KW_k+1_Mb^T_k$ and $VW_v+1_Mb^T_v$ repectively. So we have\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\hat{q}^T_i&=q_i^TW_q+b_q^T,\\\\\n",
    "\\hat{k}^T_i&=k_i^TW_k+b_k^T,\\\\\n",
    "\\hat{v}^T_i&=v_i^TW_v+b_v^T.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Taking matrix tranpose in the previous equations we get the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaae6ca",
   "metadata": {},
   "source": [
    "### Property: $\\text{MAtten}_{\\mathcal{W}}$ Key-Value deletion\n",
    "\n",
    "If $A^{(i)}$ and $A^{[i]}$ denote the matrix obtained by deleting the $i$-th row and column resectively, and $B_{i,s}\\to-\\infty$ for $i=1,2,\\dots,M$ then \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{MAtten}_{\\mathcal{W}}(Q,K,V)\\to \\text{MAtten}_{\\mathcal{W}^{[s]}}(Q,K^{(s)},V^{(s)}), \\quad Q \\in \\mathbb{R}^{M\\times d_k}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190059fc",
   "metadata": {},
   "source": [
    "**Proof:** From the $\\text{MAtten}_{B}$ Key-Value deletion property we get\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\operatorname{MAttn}_{\\mathcal{W}}(Q,K,V)\n",
    "&=\\text{MAttn}_{B}(QW_q+1_{M}b_q^T,KW_k+1_Mb_k^T,VW_v+1_Mb_v^T),\\\\\n",
    "&\\to\\text{MAttn}_{B^{[s]}}(QW_q+1_{M}b_q^T,(KW_k+1_Mb_k^T)^{(s)},(VW_v+1_Mb_v^T)^{(s)}),\\\\\n",
    "&=\\text{MAttn}_{B^{[s]}}(QW_q+1_{M}b_q^T,(KW_k)^{(s)}+(1_Mb_k^T)^{(s)},(VW_v)^{(s)}+(1_Mb_v^T)^{(s)}),\\\\\n",
    "&=\\text{MAttn}_{B^{[s]}}(QW_q+1_{M}b_q^T,K^{(s)}W_k+1_{M-1}b_k^T,V^{(s)}W_v+1_{M-1}b_v^T),\\\\\n",
    "&=\\operatorname{MAttn}_{\\mathcal{W}^{[s]}}(Q,K^{(s)},V^{(s)}).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72cad5b",
   "metadata": {},
   "source": [
    "## Definition: Self-Attention ( $\\text{SAttn}_{\\mathcal{W}}$)\n",
    "$$\\text{SAttn}_{\\mathcal{W}}(X)=\\operatorname{Attn}_{\\mathcal{W}}(X,X,X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d632046",
   "metadata": {},
   "source": [
    "Inputs\n",
    "* $X \\in \\mathbb{R}^{N \\times c}, \\quad N\\in \\mathbb{N}$ \n",
    "\n",
    "Weights\n",
    "* a set of querry weights $W_q \\in \\mathbb{R}^{c\\times d_k}$ and bias $b_q\\in \\mathbb{R}^{d_k}$\n",
    "* a set of key weights $W_k \\in \\mathbb{R}^{c \\times d_k}$ and bias $b_k\\in \\mathbb{R}^{d_k}$\n",
    "* a set of value weights $W_v \\in \\mathbb{R}^{c \\times d_v}$ and bias $b_q\\in \\mathbb{R}^{d_v}$\n",
    "\n",
    "Output: \n",
    "* Output $O= [o_1|o_2|\\dots|o_N]^T \\in\\mathbb{R}^{N\\times d_v}$\n",
    "\\begin{align*}\n",
    "O =\\text{SAttn}_{\\mathcal{W}}(X)=\\operatorname{Attn}_{\\mathcal{W}}(X,X,X),\n",
    "\\end{align*}\n",
    "where\n",
    "\\begin{align*}\n",
    "\\mathcal{W} &= \\left(W_q,b_q,W_k,b_k,W_v,b_v\\right).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d49465",
   "metadata": {},
   "source": [
    "### Property: $\\text{SAttn}_{\\mathcal{W}}$ Permutation Equivariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741cee4b",
   "metadata": {},
   "source": [
    "If Self-Attention has no bias (projections are linear instead of affine), the Self-Attention is permutation equivariant. If $\\pi_r$ denotes an arbitrary permutation over the rows of a matrix, then\n",
    "$$\\text{SAttn}_{\\mathcal{W}}(\\pi_r(X))=\\pi_r\\left(\\text{SAttn}_{\\mathcal{W}}(X)\\right), $$\n",
    "where \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{W} &= \\left(W_q,0_{\\mathbb{R}^{d_k}},W_k,0_{\\mathbb{R}^{d_k}},W_v,0_{\\mathbb{R}^{d_v}}\\right).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f719472",
   "metadata": {},
   "source": [
    "Proof: From the previous result we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{SAttn}_{\\mathcal{W}}(\\pi_r(X))\n",
    "&=\\operatorname{Attn}_{\\mathcal{W}}(\\pi_r(X),\\pi_r(X),\\pi_r(X)),\\\\\n",
    "&=\\pi_r\\left(\\operatorname{Attn}_{\\mathcal{W}}(X,X,X)\\right),\\\\\n",
    "&=\\pi_r\\left(\\text{SAttn}_{\\mathcal{W}}(X)\\right).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8385f7b",
   "metadata": {},
   "source": [
    "## Definition: Masked Self-Attention ($\\text{MSAttn}_{\\mathcal{W}}$)\n",
    "$$\\text{MSAttn}_{\\mathcal{W}}(X)=\\text{MAttn}_{\\mathcal{W}}(X,X,X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f3ef8d",
   "metadata": {},
   "source": [
    "Inputs\n",
    "* $X = [x_1|x_2|\\dots|x_N]^T \\in \\mathbb{R}^{N \\times c}$ \n",
    "\n",
    "Weights\n",
    "* a set of querry weights $W_q \\in \\mathbb{R}^{c\\times d_k}$ and bias $b_q\\in \\mathbb{R}^{d_k}$\n",
    "* a set of key weights $W_k \\in \\mathbb{R}^{c \\times d_k}$ and bias $b_k\\in \\mathbb{R}^{d_k}$\n",
    "* a set of value weights $W_v \\in \\mathbb{R}^{c \\times d_v}$ and bias $b_q\\in \\mathbb{R}^{d_v}$\n",
    "* a mask matrix $B\\in \\mathbb{R}^{N\\times N}.$ \n",
    "\n",
    "Output: \n",
    "* Output $O= [o_1|o_2|\\dots|o_N]^T \\in\\mathbb{R}^{N\\times d_v}$\n",
    "\\begin{align*}\n",
    "O =\\text{MSAttn}_{\\mathcal{W}}(X)=\\text{MAttn}_{\\mathcal{W}}(X,X,X),\n",
    "\\end{align*}\n",
    "where\n",
    "\\begin{align*}\n",
    "\\mathcal{W} &= \\left(W_q,b_q,W_k,b_k,W_v,b_v,B\\right).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5aed18",
   "metadata": {},
   "source": [
    "**Note:** In general, the mask breaks the permutation equivariance properties of Self-Attention and the Self-Attention is not even position agnostic anymore, notice that the weights in $B$ depend on input sequence legth $N$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec2c217",
   "metadata": {},
   "source": [
    "### Property: The entries of $B$ in $\\text{MSAttn}_{\\mathcal{W}}$ can selectively suppress attention.\n",
    "\n",
    "If $b_{i,s} \\to -\\infty$ for $s \\neq i$, then the $i$-th output $o_i$ does not depend on $x_s$. In particular, we have\n",
    "\\begin{align*}\n",
    "o_i =\\frac{1}{\\sum_{l=1,l\\neq s}^Ne^{\\frac{\\hat{q}_i^{T}\\hat{k}_l+b_{i,l}}{\\sqrt{d_k}}}}\\sum_{j=1,j\\neq s}^Ne^{\\frac{\\hat{q}_i^{T}\\hat{k}_j+b_{i,j}}{\\sqrt{d_k}}} \\hat{v}_j.\n",
    "\\end{align*}\n",
    "where\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\hat{q}_l&=W_q^Tx_l+b_q,\\\\\n",
    "\\hat{k}_l&=W_k^Tx_l+b_k,\\\\\n",
    "\\hat{v}_l&=W_v^Tx_l+b_v,\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d74a40",
   "metadata": {},
   "source": [
    "**Proof:** It is straigthforwrd from the same result for Attention with weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c724caa8",
   "metadata": {},
   "source": [
    "### Property: $\\text{MSAttn}_{\\mathcal{W}}$ Input-deletion\n",
    "If $A^{<i>}$ denote the matrix obtained by deleting the $i$-th row and column of the $A$, and $B_{i,s}\\to-\\infty$ for $i=1,2,\\dots,s-1,s+1,\\dots,N$ then \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{MSAttn}_{\\mathcal{W}}(X)^{(s)} \\to \\text{MSAttn}_{\\mathcal{W}^{<s>}}(X^{(s)}).\n",
    "\\end{align*}\n",
    "$$\n",
    "where\n",
    "\\begin{align*}\n",
    "\\mathcal{W^{<s>}}_0 &=\\mathcal{W}_0 = \\left(W_{o},b_{o}\\right),\\\\\n",
    "\\mathcal{W}^{<s>}_i &= \\left(W_{q,i},b_{q,i},W_{k,i},b_{k,i},W_{v,i},b_{v,i},B^{<s>}_i\\right), \\quad i = 1,2,\\dots,h.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c910172a",
   "metadata": {},
   "source": [
    "**Proof:** Combining $\\text{MAttn}_{\\mathcal{W}}$ Querry and Key-Value deletion properties we obtain\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{MSAttn}_{\\mathcal{W}}(X)^{(s)}&=\\text{MAttn}_{\\mathcal{W}}(X,X,X)^{(s)},\\\\\n",
    "&=\\text{MAttn}_{\\mathcal{W}^{(s)}}(X^{(s)},X,X),\\\\\n",
    "&\\to\\text{MAttn}_{\\mathcal{W}^{<s>}}(X^{(s)},X^{(s)},X^{(s)}),\\\\\n",
    "&=\\text{MSAttn}_{\\mathcal{W}^{<s>}}(X^{(s)})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eb8e04",
   "metadata": {},
   "source": [
    "## Definition: Multi-Head Attention ($\\text{MHAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}$)\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{MHAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}&(Q,K,V)=\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\operatorname{Attn}_{\\mathcal{W}_{1}}(Q,K,V)|\n",
    "\\operatorname{Attn}_{\\mathcal{W}_{2}}(Q,K,V)|\n",
    "\\dots|\n",
    "\\operatorname{Attn}_{\\mathcal{W}_{h}}(Q,K,V)\n",
    "\\end{pmatrix}W_o + B_o\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdcbb8f",
   "metadata": {},
   "source": [
    "Inputs\n",
    "* a set of queries $Q = [q_1|q_2|\\dots|q_M]^T \\in \\mathbb{R}^{M \\times c_q}, \\quad M\\in\\mathbb{N}$\n",
    "* a set of keys $K = [k_1|k_2|\\dots|k_N]^T \\in \\mathbb{R}^{N \\times c_k}, \\quad N\\in\\mathbb{N}$\n",
    "* a set of values $V = [v_1|v_2|\\dots|v_N]^T \\in \\mathbb{R}^{N \\times c_v}, \\quad N\\in\\mathbb{N}$\n",
    "\n",
    "Weights\n",
    "* $W_{q,i} \\in \\mathbb{R}^{c_q \\times d_k}$ and $b_{q,i}\\in \\mathbb{R}^{d_k}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{k,i} \\in \\mathbb{R}^{c_k \\times d_k}$ and $b_{k,i}\\in \\mathbb{R}^{d_k}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{v,i} \\in \\mathbb{R}^{c_v \\times d_v}$ and $b_{v,i}\\in \\mathbb{R}^{d_v}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{o} \\in \\mathbb{R}^{hd_v \\times d_o }$ and $b_{o}\\in \\mathbb{R}^{d_0}$.\n",
    "\n",
    "Output: \n",
    "* Output $O= [o_1|o_2|\\dots|o_M]^T \\in\\mathbb{R}^{M\\times d_o}$\n",
    "\\begin{align*}\n",
    "O  &= \\text{MHAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(Q,K,V),\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\operatorname{Attn}_{\\mathcal{W}_{1}}(Q,K,V)|\n",
    "\\operatorname{Attn}_{\\mathcal{W}_{2}}(Q,K,V)|\n",
    "\\dots|\n",
    "\\operatorname{Attn}_{\\mathcal{W}_{h}}(Q,K,V)\n",
    "\\end{pmatrix}W_o + 1_{M}b_o\n",
    "\\end{align*}\n",
    "where\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{W}_0 &= \\left(W_{o},b_{o}\\right),\\\\\n",
    "\\mathcal{W}_i &= \\left(W_{q,i},b_{q,i},W_{k,i},b_{k,i},W_{v,i},b_{v,i}\\right), \\quad i = 1,2,\\dots,h.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a11aa8",
   "metadata": {},
   "source": [
    "Usually we have:\n",
    " * $d_h:=d_k=d_v$ (head dimensions)\n",
    " * $d := c_q = c_k = c_v = d_h\\cdot h = d_o$ (model dimension)  \n",
    "**Note:** In this case, internally we have:\n",
    "    * $W_q=[W_{q,1}|W_{q,2}|\\dots|W_{q,h}]\\in\\mathbb{R}^{c_q\\times hd_k}=\\mathbb{R}^{d\\times d}$  \n",
    "    * $W_k=[W_{k,1}|W_{k,2}|\\dots|W_{k,h}]\\in\\mathbb{R}^{c_k\\times hd_k}=\\mathbb{R}^{d\\times d}$  \n",
    "    * $W_v=[W_{v,1}|W_{v,2}|\\dots|W_{v,h}]\\in\\mathbb{R}^{c_v\\times hd_v}=\\mathbb{R}^{d\\times d}$  \n",
    "\n",
    "**Note:** The same $\\operatorname{MHAttn}$ function can be applied to inputs of different sequence lengths. The model parameters are not tied to specific positions. In this sense, $\\operatorname{MHAttn}$ is **position-agnostic**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd54247",
   "metadata": {},
   "source": [
    "### Property: $\\text{MHAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}$ Querry-Deletion\n",
    "If $A^{(i)}$ denotes the matrix obtained by deleting the $i$-th row of the matrix $A$, then \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{MHAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(Q,K,V)^{(s)}=\\text{MHAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(Q^{(s)},K,V)\n",
    "\\end{align*}\n",
    "$$\n",
    "where $s=1,2,\\dots,M$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecaec75",
   "metadata": {},
   "source": [
    "**Proof:**\n",
    "From $\\text{Attn}$ Querry-Deletion we get\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{MHAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}&(Q,K,V)^{(i)}\\\\\n",
    "&=\\left(\\begin{pmatrix}\n",
    "\\text{Attn}_{\\mathcal{W}_{1}}(Q,K,V)|\n",
    "\\text{Attn}_{\\mathcal{W}_{2}}(Q,K,V)|\n",
    "\\dots|\n",
    "\\text{Attn}_{\\mathcal{W}_{h}}(Q,K,V)\n",
    "\\end{pmatrix}W_o+1_Mb_o\\right)^{(i)},\\\\\n",
    "&=\\left(\\begin{pmatrix}\n",
    "\\text{Attn}_{\\mathcal{W}_{1}}(Q,K,V)|\n",
    "\\text{Attn}_{\\mathcal{W}_{2}}(Q,K,V)|\n",
    "\\dots|\n",
    "\\text{Attn}_{\\mathcal{W}_{h}}(Q,K,V)\n",
    "\\end{pmatrix}W_o\\right)^{(i)}+\\left(1_Mb_o\\right)^{(i)},\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\text{Attn}_{\\mathcal{W}_{1}}(Q,K,V)|\n",
    "\\text{Attn}_{\\mathcal{W}_{2}}(Q,K,V)|\n",
    "\\dots|\n",
    "\\text{Attn}_{\\mathcal{W}_{h}}(Q,K,V)\n",
    "\\end{pmatrix}^{(i)}W_o+1_{M-1}b_o,\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\text{Attn}_{\\mathcal{W}_{1}}(Q,K,V)^{(i)}|\n",
    "\\text{Attn}_{\\mathcal{W}_{2}}(Q,K,V)^{(i)}|\n",
    "\\dots|\n",
    "\\text{Attn}_{\\mathcal{W}_{h}}(Q,K,V)^{(i)}\n",
    "\\end{pmatrix}W_o+1_{M-1}b_o,\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\text{Attn}_{\\mathcal{W}_{1}}(Q^{(i)},K,V)|\n",
    "\\text{Attn}_{\\mathcal{W}_{2}}(Q^{(i)},K,V)|\n",
    "\\dots|\n",
    "\\text{Attn}_{\\mathcal{W}_{h}}(Q^{(i)},K,V)\n",
    "\\end{pmatrix}W_o+1_{M-1}b_o,\\\\\n",
    "&=\\text{MHAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(Q^{(i)},K,V)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6759e72",
   "metadata": {},
   "source": [
    "### Property: Multi-Head Attention Permutation Equivariance\n",
    "\n",
    "If Multi-Head Attention has no bias (projections are linear  instead of affine), the Multi-Head Attention is permutation equivariant. If $\\pi_r$ and $\\sigma_r$ denote arbitrary permutations over the rows of a matrix, then\n",
    "$$\\text{MHAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))=\\pi_r\\left(\\text{MHAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(Q,K,V)\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e562f03e",
   "metadata": {},
   "source": [
    "**Proof:**  Consider  $\\pi_r(M) = R_{\\pi}M$ and $\\sigma_r(M) = R_{\\sigma}M$ for permutation matrices $R_{\\pi}$ and $R_{\\sigma}$ respectively. From the Attention equivariance we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{MHAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\text{Attn}_{\\mathcal{W}_{1}}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))|\n",
    "\\text{Attn}_{\\mathcal{W}_{2}}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))|\n",
    "\\dots|\n",
    "\\text{Attn}_{\\mathcal{W}_{h}}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))\n",
    "\\end{pmatrix}W_o,\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\pi_r\\left(\\text{Attn}_{\\mathcal{W}_{1}}(Q,K,V)\\right)|\n",
    "\\pi_r\\left(\\text{Attn}_{\\mathcal{W}_{2}}(Q,K,V)\\right)|\n",
    "\\dots|\n",
    "\\pi_r\\left(\\text{Attn}_{\\mathcal{W}_{h}}(Q,K,V)\\right)\n",
    "\\end{pmatrix}W_o,\\\\\n",
    "&=\\begin{pmatrix}\n",
    "R_{\\pi}\\text{Attn}_{\\mathcal{W}_{1}}(Q,K,V)|\n",
    "R_{\\pi}\\text{Attn}_{\\mathcal{W}_{2}}(Q,K,V)|\n",
    "\\dots|\n",
    "R_{\\pi}\\text{Attn}_{\\mathcal{W}_{h}}(Q,K,V)\n",
    "\\end{pmatrix}W_o,\\\\\n",
    "&=R_{\\pi}\\begin{pmatrix}\n",
    "\\text{Attn}_{\\mathcal{W}_{1}}(Q,K,V)|\n",
    "\\text{Attn}_{\\mathcal{W}_{2}}(Q,K,V)|\n",
    "\\dots|\n",
    "\\text{Attn}_{\\mathcal{W}_{h}}(Q,K,V)\n",
    "\\end{pmatrix}W_o,\\\\\n",
    "&=R_{\\pi}\\text{MHAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(Q,K,V),\\\\\n",
    "&=\\pi_r\\left(\\text{MHAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(Q,K,V)\\right).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9047d1",
   "metadata": {},
   "source": [
    "## Definition: Multi-Head Attention (torch implementation)\n",
    "\n",
    "Torch implementation corresponds to the particular (usual) case\n",
    " * $d_h:=d_k=d_v$ (head dimensions)\n",
    " * $d := c_q = d_h\\cdot h = d_o$ (model dimension)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021f71a",
   "metadata": {},
   "source": [
    "Denote $d_h = d|h$ ($h$ must divide $d$). Then we have the simplified defintion\n",
    "\n",
    "Inputs\n",
    "* a set of queries $Q = [q_1|q_2|\\dots|q_M]^T \\in \\mathbb{R}^{M \\times d}$\n",
    "* a set of keys $K = [k_1|k_2|\\dots|k_N]^T \\in \\mathbb{R}^{N \\times c_k}$\n",
    "* a set of values $V = [v_1|v_2|\\dots|v_N]^T \\in \\mathbb{R}^{N \\times c_v}$\n",
    "\n",
    "Weights\n",
    "* $W_{q,i} \\in \\mathbb{R}^{d \\times d_h}$ and $b_{q,i}\\in \\mathbb{R}^{d_h}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{k,i} \\in \\mathbb{R}^{c_k \\times d_h}$ and $b_{k,i}\\in \\mathbb{R}^{d_h}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{v,i} \\in \\mathbb{R}^{c_v \\times d_h}$ and $b_{v,i}\\in \\mathbb{R}^{d_h}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{o} \\in \\mathbb{R}^{d \\times d }$ and $b_{o}\\in \\mathbb{R}^{d}$.\n",
    "\n",
    "\n",
    "Output: \n",
    "* Output $O= [o_1|o_2|\\dots|o_M]^T \\in\\mathbb{R}^{M\\times d}$\n",
    "\\begin{align*}\n",
    "O  &= \\text{MHAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(Q,K,V),\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\operatorname{Attn}_{\\mathcal{W}_1}(Q,K,V)|\n",
    "\\operatorname{Attn}_{\\mathcal{W}_2}(Q,K,V)|\n",
    "\\dots|\n",
    "\\operatorname{Attn}_{\\mathcal{W}_h}(Q,K,V)\n",
    "\\end{pmatrix}W_o + B_o\n",
    "\\end{align*}\n",
    "where:\n",
    "    * $W_q=[W_{q,1}|W_{q,2}|\\dots|W_{q,h}]\\in\\mathbb{R}^{d\\times d}$  \n",
    "    * $W_k=[W_{k,1}|W_{k,2}|\\dots|W_{k,h}]\\in\\mathbb{R}^{d\\times d}$  \n",
    "    * $W_v=[W_{v,1}|W_{v,2}|\\dots|W_{v,h}]\\in\\mathbb{R}^{d\\times d}$  \n",
    "    * $b_q=[b^T_{q,1}|b^T_{q,2}|\\dots|b^T_{q,h}]^T\\in\\mathbb{R}^{d}$  \n",
    "    * $b_k=[b^T_{k,1}|b^T_{k,2}|\\dots|b^T_{k,h}]^T\\in\\mathbb{R}^{d}$ \n",
    "    * $b_v=[b^T_{v,1}|b^T_{v,2}|\\dots|b^T_{v,h}]^T\\in\\mathbb{R}^{d}$  \n",
    "    * $B_o= [b_o|b_o|\\dots|b_o]^T \\in \\mathbb{R}^{M \\times d}$\n",
    "    * $\\mathcal{W}_0 = \\left(W_{o},b_{o}\\right)$\n",
    "    * $\\mathcal{W}_i =\\left(W_{q,i},b_{q,i},W_{k,i},b_{k,i},W_{v,i},b_{v,i}\\right), \\quad i = 1,2,\\dots,h$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a85e0a",
   "metadata": {},
   "source": [
    "## Definition: Masked Multi-Head Attention ($ \\text{MMHAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}$)\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{MMHAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}&(Q,K,V),\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\text{MAttn}_{\\mathcal{W}_{1}}(Q,K,V)|\n",
    "\\text{MAttn}_{\\mathcal{W}_{2}}(Q,K,V)|\n",
    "\\dots|\n",
    "\\text{MAttn}_{\\mathcal{W}_{h}}(Q,K,V)\n",
    "\\end{pmatrix}W_o + 1_Mb_o\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf061ec4",
   "metadata": {},
   "source": [
    "Inputs\n",
    "* a set of queries $Q = [q_1|q_2|\\dots|q_M]^T \\in \\mathbb{R}^{M \\times c_q}$\n",
    "* a set of keys $K = [k_1|k_2|\\dots|k_N]^T \\in \\mathbb{R}^{N \\times c_k}$\n",
    "* a set of values $V = [v_1|v_2|\\dots|v_N]^T \\in \\mathbb{R}^{N \\times c_v}$\n",
    "\n",
    "Weights\n",
    "* $W_{q,i} \\in \\mathbb{R}^{c_q \\times d_k}$ and $b_{q,i}\\in \\mathbb{R}^{d_k}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{k,i} \\in \\mathbb{R}^{c_k \\times d_k}$ and $b_{k,i}\\in \\mathbb{R}^{d_k}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{v,i} \\in \\mathbb{R}^{c_v \\times d_v}$ and $b_{v,i}\\in \\mathbb{R}^{d_v}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $B_{i} \\in \\mathbb{R}^{M \\times N}, \\quad i=1,2,\\dots,h.$\n",
    "* $W_{o} \\in \\mathbb{R}^{d_vh \\times d_o }$ and $b_{o}\\in \\mathbb{R}^{d_0}$.\n",
    "\n",
    "Output: \n",
    "* Output $O= [o_1|o_2|\\dots|o_M]^T \\in\\mathbb{R}^{M\\times d_o}$\n",
    "$$\n",
    "\\begin{align*}\n",
    "O  &= \\text{MMHAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(Q,K,V),\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\text{MAttn}_{\\mathcal{W}_{1}}(Q,K,V)|\n",
    "\\text{MAttn}_{\\mathcal{W}_{2}}(Q,K,V)|\n",
    "\\dots|\n",
    "\\text{MAttn}_{\\mathcal{W}_{h}}(Q,K,V)\n",
    "\\end{pmatrix}W_o + 1_Mb_o\n",
    "\\end{align*}\n",
    "$$\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{W}_0 &= \\left(W_{o},b_{o}\\right),\\\\\n",
    "\\mathcal{W}_i &= \\left(W_{q,i},b_{q,i},W_{k,i},b_{k,i},W_{v,i},b_{v,i},B_i\\right), \\quad i = 1,2,\\dots,h.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c69d588",
   "metadata": {},
   "source": [
    "### Property: $\\text{MMHAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}$ Querry-Deletion\n",
    "If $A^{(i)}$ denotes the matrix obtained by deleting the $i$-th row of the matrix $A$, then \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{MMHAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(Q,K,V)^{(s)}=\\text{MMHAttn}_{\\{\\mathcal{W}^{(s)}_{i}\\}_{i=0}^h}(Q^{(s)},K,V)\n",
    "\\end{align*}\n",
    "$$\n",
    "where $s=1,2,\\dots,M$ and \n",
    "\\begin{align*}\n",
    "\\mathcal{W}^{(s)}_0 &=\\mathcal{W}_0= \\left(W_{o},b_{o}\\right),\\\\\n",
    "\\mathcal{W}^{(s)}_i &= \\left(W_{q,i},b_{q,i},W_{k,i},b_{k,i},W_{v,i},b_{v,i},B^{(s)}_i\\right), \\quad i = 1,2,\\dots,h.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f868a4",
   "metadata": {},
   "source": [
    "**Proof:**\n",
    "From $\\text{MAttn}$ Querry-Deletion we get\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{MMHAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}&(Q,K,V)^{(s)}\\\\\n",
    "&=\\left(\\begin{pmatrix}\n",
    "\\text{MAttn}_{\\mathcal{W}_{1}}(Q,K,V)|\n",
    "\\text{MAttn}_{\\mathcal{W}_{2}}(Q,K,V)|\n",
    "\\dots|\n",
    "\\text{MAttn}_{\\mathcal{W}_{h}}(Q,K,V)\n",
    "\\end{pmatrix}W_o+1_Mb_o\\right)^{(s)},\\\\\n",
    "&=\\left(\\begin{pmatrix}\n",
    "\\text{MAttn}_{\\mathcal{W}_{1}}(Q,K,V)|\n",
    "\\text{MAttn}_{\\mathcal{W}_{2}}(Q,K,V)|\n",
    "\\dots|\n",
    "\\text{MAttn}_{\\mathcal{W}_{h}}(Q,K,V)\n",
    "\\end{pmatrix}W_o\\right)^{(s)}+\\left(1_Mb_o\\right)^{(s)},\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\text{MAttn}_{\\mathcal{W}_{1}}(Q,K,V)|\n",
    "\\text{MAttn}_{\\mathcal{W}_{2}}(Q,K,V)|\n",
    "\\dots|\n",
    "\\text{MAttn}_{\\mathcal{W}_{h}}(Q,K,V)\n",
    "\\end{pmatrix}^{(s)}W_o+1_{M-1}b_o,\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\text{MAttn}_{\\mathcal{W}_{1}}(Q,K,V)^{(s)}|\n",
    "\\text{MAttn}_{\\mathcal{W}_{2}}(Q,K,V)^{(s)}|\n",
    "\\dots|\n",
    "\\text{MAttn}_{\\mathcal{W}_{h}}(Q,K,V)^{(i)}\n",
    "\\end{pmatrix}W_o+1_{M-1}b_o,\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\text{MAttn}_{\\mathcal{W}^{(s)}_{1}}(Q^{(s)},K,V)|\n",
    "\\text{MAttn}_{\\mathcal{W}^{(s)}_{2}}(Q^{(s)},K,V)|\n",
    "\\dots|\n",
    "\\text{MAttn}_{\\mathcal{W}^{(s)}_{h}}(Q^{(s)},K,V)\n",
    "\\end{pmatrix}W_o+1_{M-1}b_o,\\\\\n",
    "&=\\text{MHAttn}_{\\{\\mathcal{W}^{(s)}_{i}\\}_{i=0}^h}(Q^{(s)},K,V)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77786d61",
   "metadata": {},
   "source": [
    "### Property: The entries of the masks $B_i$ (when they agree) can selectively suppress attention in $\\text{MMHAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}$.\n",
    "\n",
    "Denote the $(i,s) \\in M\\times N$ entry of the $l$-th mask matrix by $b^{(l)}_{i,s}$. If $b_{l,i,s} \\to -\\infty$ for all values $l=1,2,\\dots,h$, then the $i$-th output $o_i$ of the masked Multi-Head Attention does not depend on $k_s$ or $v_s$. Particulary, we have:\n",
    "$$\n",
    "\\begin{align*}\n",
    "o_i &=W_o^T\\begin{pmatrix}\n",
    "o_{i}^{(1)}\\\\\n",
    "o_{i}^{(2)}\\\\\n",
    "\\vdots\\\\\n",
    "o_{i}^{(h)}\n",
    "\\end{pmatrix}\n",
    "+ B^T_o\\\\\n",
    "o_i^{(\\kappa)} &=\\frac{1}{\\sum_{l=1,l\\neq s}^Ne^{\\frac{(\\hat{q}_i^{(\\kappa)})^{T}\\hat{k}_l^{(\\kappa)}+b^{(\\kappa)}_{i,l}}{\\sqrt{d_k}}}}\\sum_{j=1,j\\neq s}^Ne^{\\frac{(\\hat{q}_i^{(\\kappa)})^{T}\\hat{k}_j^{(\\kappa)}+b^{(\\kappa)}_{i,j}}{\\sqrt{d_k}}} \\hat{v}_j^{(\\kappa)}, \\quad \\kappa = 1,2,\\dots,h.\n",
    "\\end{align*}\n",
    "$$\n",
    "where\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\hat{q}^{(\\kappa)}_l&=W_{q,\\kappa}^Tq_l+b_q, \\quad \\kappa = 1,2,\\dots,h, \\ l = 1,2,\\dots, N\\\\\n",
    "\\hat{k}^{(\\kappa)}_l&=W_{k,\\kappa}^Tk_l+b_k, \\quad \\kappa = 1,2,\\dots,h, \\ l = 1,2,\\dots, N\\\\\n",
    "\\hat{v}^{(\\kappa)}_l&=W_{v,\\kappa}^Tv_l+b_v, \\quad \\kappa = 1,2,\\dots,h, \\ l = 1,2,\\dots, N\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f56951",
   "metadata": {},
   "source": [
    "**Proof:** It is strightforward from hte same property for $\\text{MAttn}_{\\mathcal{W}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a506cc",
   "metadata": {},
   "source": [
    "### Property: $\\text{MMHAtten}_{\\{\\mathcal{W}\\}_{i=0}^{h}}$ Key-Value deletion\n",
    "\n",
    "If $A^{(i)}$ and $A^{[i]}$ denote the matrix obtained by deleting the $i$-th row and column resectively, and Denote the $(i,s) \\in M\\times N$ entry of the $l$-th mask matrix by $B^{(l)}_{i,s}$.\n",
    "If $B^{(l)}_{i,s}\\to-\\infty$ for $i=1,2,\\dots,M$ and $l=1,2,\\dots,h$ then \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{MMHAtten}_{{\\{\\mathcal{W}\\}_{i=0}^{h}}}(Q,K,V)\\to \\text{MMHAtten}_{\\{\\mathcal{W}^{[s]}\\}_{i=0}^{h}}(Q,K^{(s)},V^{(s)}), \\quad Q \\in \\mathbb{R}^{M\\times d_k}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $s=1,2,\\dots,M$ and \n",
    "\\begin{align*}\n",
    "\\mathcal{W}^{[s]}_0 &=\\mathcal{W}_0= \\left(W_{o},b_{o}\\right),\\\\\n",
    "\\mathcal{W}^{[s]}_i &= \\left(W_{q,i},b_{q,i},W_{k,i},b_{k,i},W_{v,i},b_{v,i},B^{[s]}_i\\right), \\quad i = 1,2,\\dots,h.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3654b696",
   "metadata": {},
   "source": [
    "**Proof:** From the $\\text{MAtten}_{B}$ Key-Value deletion property we get\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\operatorname{MMHAttn}_{{{\\{\\mathcal{W}\\}_{i=0}^{h}}}}(Q,K,V)\n",
    "&=\\begin{pmatrix}\n",
    "\\text{MAttn}_{\\mathcal{W}_{1}}(Q,K,V)|\n",
    "\\text{MAttn}_{\\mathcal{W}_{2}}(Q,K,V)|\n",
    "\\dots|\n",
    "\\text{MAttn}_{\\mathcal{W}_{h}}(Q,K,V)\n",
    "\\end{pmatrix}W_o + 1_Mb_o,\\\\\n",
    "&\\to \\begin{pmatrix}\n",
    "\\text{MAttn}_{\\mathcal{W}^{[s]}\n",
    "_{1}}(Q,K^{(s)},V^{(s)})|\n",
    "\\dots|\n",
    "\\text{MAttn}_{\\mathcal{W}_{h}^{[s]}}(Q,K^{(s)},V^{(s)})\n",
    "\\end{pmatrix}W_o + 1_Mb_o,\\\\\n",
    "&=\\text{MMHAtten}_{\\{\\mathcal{W}^{[i]}\\}_{i=0}^{h}}(Q,K^{(s)},V^{(s)}).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d109461",
   "metadata": {},
   "source": [
    "## Definition: Multi-Head Self-Attention ($\\text{MHSAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}$)\n",
    "$$\\text{MHSAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(X)= \\text{MHAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(X,X,X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc23134",
   "metadata": {},
   "source": [
    "This is usually denoted as $\\text{MHAttn}(X)$ with only one argument.\n",
    "\n",
    "Inputs\n",
    "* $X \\in \\mathbb{R}^{N \\times c}, \\quad N \\in \\mathbb{N}.$ \n",
    "\n",
    "Weights\n",
    "* $W_{q,i} \\in \\mathbb{R}^{c \\times d_k}$ and $b_{q,i}\\in \\mathbb{R}^{d_k}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{k,i} \\in \\mathbb{R}^{c \\times d_k}$ and $b_{k,i}\\in \\mathbb{R}^{d_k}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{v,i} \\in \\mathbb{R}^{c \\times d_v}$ and $b_{v,i}\\in \\mathbb{R}^{d_v}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{o} \\in \\mathbb{R}^{d_vh \\times d_o }$ and $b_{o}\\in \\mathbb{R}^{d_0}$.\n",
    "\n",
    "Output: \n",
    "* Output $O= [o_1|o_2|\\dots|o_M]^T \\in\\mathbb{R}^{M\\times d_o}$\n",
    "\\begin{align*}\n",
    "O = \\text{MHSAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(X)= \\text{MHAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(X,X,X).\n",
    "\\end{align*}\n",
    "where\n",
    "\\begin{align*}\n",
    "\\mathcal{W}_0 &= \\left(W_{o},b_{o}\\right),\\\\\n",
    "\\mathcal{W}_i &= \\left(W_{q,i},b_{q,i},W_{k,i},b_{k,i},W_{v,i},b_{v,i}\\right), \\quad i = 1,2,\\dots,h.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369a7e28",
   "metadata": {},
   "source": [
    "**Note:** The same $\\operatorname{MHSAttn}$ function can be applied to inputs of different sequence lengths. The model parameters are not tied to specific positions. In this sense, $\\operatorname{MultiHeadHeadAttention}$ is **position-agnostic**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b398a204",
   "metadata": {},
   "source": [
    "### Property: $\\text{MHSAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}$ Permutation Equivariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0764344d",
   "metadata": {},
   "source": [
    "If Multi-Head-Self-Attention has no bias (linear projections instead of affine projections), the Multi-Head-Self-Attention is permutation equivariant. If $\\pi_r(M)$ denotes an arbitrary permutation over the rows of a matrix $M$, then\n",
    "$$\\text{MHSAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(\\pi_r(X))=\\pi_r\\left(\\text{MHSAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(X)\\right),$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce029db0",
   "metadata": {},
   "source": [
    "Proof: From the previous result we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{MHSAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(\\pi_r(X))\n",
    "&=\\text{MHAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(\\pi_r(X),\\pi_r(X),\\pi_r(X)),\\\\\n",
    "&=\\pi_r\\left(\\text{MHAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(X,X,X)\\right),\\\\\n",
    "&=\\pi_r\\left(\\text{MHSAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(X)\\right).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc033ed",
   "metadata": {},
   "source": [
    " ## Definition: Masked Multi-Head Self-Attention ($\\text{MMHSAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}$)\n",
    " $$\\text{MMHSAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(X)= \\text{MMHA}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(X,X,X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac4e810",
   "metadata": {},
   "source": [
    "This is usually denoted as $\\text{MHAttn}(X)$ with only one argument.\n",
    "\n",
    "Inputs\n",
    "* $X = [x_1|x_2|\\dots|x_N]^T\\in  \\mathbb{R}^{N \\times c}.$ \n",
    "\n",
    "Weights\n",
    "* $W_{q,i} \\in \\mathbb{R}^{c \\times d_k}$ and $b_{q,i}\\in \\mathbb{R}^{d_k}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{k,i} \\in \\mathbb{R}^{c \\times d_k}$ and $b_{k,i}\\in \\mathbb{R}^{d_k}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{v,i} \\in \\mathbb{R}^{c \\times d_v}$ and $b_{v,i}\\in \\mathbb{R}^{d_v}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $B_{i} \\in \\mathbb{R}^{N \\times N}, \\quad i=1,2,\\dots,h.$\n",
    "* $W_{o} \\inÃ§ and $b_{o}\\in \\mathbb{R}^{d_0}$.\n",
    "\n",
    "Output: \n",
    "* Output $O= [o_1|o_2|\\dots|o_M]^T \\in\\mathbb{R}^{M\\times d_o}$\n",
    "\\begin{align*}\n",
    "O = \\text{MMHSAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(X)= \\text{MMHA}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(X,X,X).\n",
    "\\end{align*}\n",
    "where\n",
    "\\begin{align*}\n",
    "\\mathcal{W}_0 &= \\left(W_{o},b_{o}\\right),\\\\\n",
    "\\mathcal{W}_i &= \\left(W_{q,i},b_{q,i},W_{k,i},b_{k,i},W_{v,i},b_{v,i},B_i\\right), \\quad i = 1,2,\\dots,h.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64123834",
   "metadata": {},
   "source": [
    "### Property: The entries of the masks $B_i$ (when they agree) can selectively suppress attention in the Multi-Head Self-Attention.\n",
    "\n",
    "Denote the $(i,s) \\in M\\times N$ entry of the $l$-th mask matrix by $b^{(l)}_{i,s}$. If $b_{l,i,s} \\to -\\infty$ for fixed values $s \\neq i$ and for all values $l=1,2,\\dots,h$, then the $i$-th output $o_i$ of the masked Multi-Head Self-Attention does not depend on $x_s$. Particulary, we have:\n",
    "$$\n",
    "\\begin{align*}\n",
    "o_i &=W_o^T\\begin{pmatrix}\n",
    "o_{i}^{(1)}\\\\\n",
    "o_{i}^{(2)}\\\\\n",
    "\\vdots\\\\\n",
    "o_{i}^{(h)}\n",
    "\\end{pmatrix}\n",
    "+ B^T_o\\\\\n",
    "o_i^{(\\kappa)} &=\\frac{1}{\\sum_{l=1,l\\neq s}^Ne^{\\frac{(\\hat{q}_i^{(\\kappa)})^{T}\\hat{k}_l^{(\\kappa)}+b^{(\\kappa)}_{i,l}}{\\sqrt{d_k}}}}\\sum_{j=1,j\\neq s}^Ne^{\\frac{(\\hat{q}_i^{(\\kappa)})^{T}\\hat{k}_j^{(\\kappa)}+b^{(\\kappa)}_{i,j}}{\\sqrt{d_k}}} \\hat{v}_j^{(\\kappa)}, \\quad \\kappa = 1,2,\\dots,h.\n",
    "\\end{align*}\n",
    "$$\n",
    "where\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\hat{q}^{(\\kappa)}_l&=W_{q,\\kappa}^Tx_l+b_q, \\quad \\kappa = 1,2,\\dots,h, \\ l = 1,2,\\dots, N\\\\\n",
    "\\hat{k}^{(\\kappa)}_l&=W_{k,\\kappa}^Tx_l+b_k, \\quad \\kappa = 1,2,\\dots,h, \\ l = 1,2,\\dots, N\\\\\n",
    "\\hat{v}^{(\\kappa)}_l&=W_{v,\\kappa}^Tx_l+b_v, \\quad \\kappa = 1,2,\\dots,h, \\ l = 1,2,\\dots, N\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d32693",
   "metadata": {},
   "source": [
    "### Property: $\\text{MMHSAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}$ Input-deletion\n",
    "If $A^{<i>}$ denote the matrix obtained by deleting the $i$-th row and column of the $A$, and $B_{i,s}\\to-\\infty$ for $i=1,2,\\dots,s-1,s+1,\\dots,N$ then \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{MMHSAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(X)^{(s)} \\to \\text{MMHSAttn}_{\\{\\mathcal{W}_i^{<s>}\\}_{i=0}^h}(X^{(s)}).\n",
    "\\end{align*}\n",
    "$$\n",
    "where\n",
    "\\begin{align*}\n",
    "\\mathcal{W^{<s>}}_0 &=\\mathcal{W}_0 = \\left(W_{o},b_{o}\\right),\\\\\n",
    "\\mathcal{W}^{<s>}_i &= \\left(W_{q,i},b_{q,i},W_{k,i},b_{k,i},W_{v,i},b_{v,i},B^{<s>}_i\\right), \\quad i = 1,2,\\dots,h.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f1fb97",
   "metadata": {},
   "source": [
    "**Proof:** Combining $\\text{MMHAtten}_{\\{\\mathcal{W}_i\\}_{i=0}^{h}}$ Querry and Key-Value deletion properties we obtain\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{MMHSAttn}_{\\{\\mathcal{W}\\}_{i=0}^{h}}(X)^{(s)}&=\\text{MMHAttn}_{\\{\\mathcal{W}_i\\}_{i=0}^{h}}(X,X,X)^{(s)},\\\\\n",
    "&=\\text{MMHAttn}_{\\{\\mathcal{W}^{(s)}_{i}\\}_{i=0}^{h}}(X^{(s)},X,X),\\\\\n",
    "&\\to\\text{MMHAttn}_{\\{\\mathcal{W}^{<s>}_{i}\\}_{i=0}^{h}}(X^{(s)},X^{(s)},X^{(s)}),\\\\\n",
    "&=\\text{MMHSAttn}_{\\{\\mathcal{W}_i^{<s>}\\}_{i=0}^h}(X^{(s)}).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a6d386",
   "metadata": {},
   "source": [
    "## Code: Masked Multi-Head Attention\n",
    "\n",
    "The code for the remaining models are easily deduced from $\\text{MMHAttn}$:\n",
    "* $\\text{MHAttn}_{\\{\\mathcal{W}_i\\}_{i=0}^h}=\\text{MMHAttn}_{\\{\\hat{\\mathcal{W}}_{i}\\}_{i=0}^h}(Q,K,V)$\n",
    "where\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{W}_0 &= \\hat{\\mathcal{W}}_0 =\\left(W_{o},b_{o}\\right),\\\\\n",
    "\\hat{\\mathcal{W}}_i &= \\left(W_{q,i},b_{q,i},W_{k,i},b_{k,i},W_{v,i},b_{v,i},0_{M\\times N}\\right), \\quad i = 1,2,\\dots,h,\\\\\n",
    "\\mathcal{W}_i &= \\left(W_{q,i},b_{q,i},W_{k,i},b_{k,i},W_{v,i},b_{v,i}\\right), \\quad i = 1,2,\\dots,h,\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "* $\\text{MAttn}_{\\mathcal{W}}=  \\text{MMHAttn}_{\\{\\mathcal{W}_i\\}_{i=0}^{1}}(Q,K,V)$\n",
    "where $h=1$ , $d_o=d_v$\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{W}_0 &= \\left(I_{d_v\\times d_v},0_{\\mathbb{R}^{d_v}}\\right),\\\\\n",
    "\\mathcal{W} &= \\mathcal{W}_1 = \\left(W_{q,1},b_{q,1},W_{k,1},b_{k,1},W_{v,1},b_{v,1},B_1\\right).\n",
    "\\end{align*}\n",
    "$$\n",
    "* $\\operatorname{Attn}_{\\mathcal{W}}(Q,K,V) = \\text{MAttn}_{\\hat{\\mathcal{W}}}(Q,K,V)$ \n",
    "where\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\mathcal{W}} &= \\left(W_{q},b_{q},W_{k},b_{k},W_{v},b_{v},0_{M\\times N}\\right),\\\\\n",
    "\\mathcal{W} &= \\left(W_{q},b_{q},W_{k},b_{k},W_{v},b_{v}\\right).\n",
    "\\end{align*}\n",
    "$$\n",
    "* $\\text{SAttn}_{\\mathcal{W}}(X) = \\operatorname{Attn}_{\\mathcal{W}}(X,X,X)$\n",
    "* $\\text{MSAttn}_{\\mathcal{W}}(X) = \\operatorname{MAttn}_{\\mathcal{W}}(X,X,X)$\n",
    "* $\\text{MHSAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(X) = \\text{MHAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(X,X,X)$\n",
    "* $\\text{MMHSAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(X) = \\text{MMHAttn}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(X,X,X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab3980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e46913",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHAttn(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention module with (general) parameters\n",
    "    cq, ck, cv: input dimensions for Q, K, V\n",
    "    dk, dv: dimensions for each head's Q, K and V\n",
    "    do: output dimension\n",
    "    h: number of heads.\n",
    "    \n",
    "    The initialization of the weights differs from PyTorchâ€™s `nn.MHAttn`.\n",
    "    Here we use standard `nn.Linear` initialization (Xavier uniform for weights and\n",
    "    zeros for biases) for clarity and simplicity. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, cq, ck, cv, dk, dv, do, h, bias=True, add_bias_kv=False, device=None, dtype=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert dk % h == 0, \"dk must be divisible by h\"\n",
    "        self.cq = cq\n",
    "        self.ck = ck\n",
    "        self.cv = cv\n",
    "        self.dk = dk\n",
    "        self.dv = dv\n",
    "        self.do = do\n",
    "        self.h = h\n",
    "        self.add_bias_kv = add_bias_kv\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        # Q -> QW_q+1_Mb^T_q\n",
    "        self.q_proj = torch.nn.Linear(cq, dk * h, bias, self.device, self.dtype)\n",
    "        # K -> KW_k+1_Mb^T_k\n",
    "        self.k_proj = torch.nn.Linear(ck, dk * h, bias, self.device, self.dtype)\n",
    "        # V -> VW_v+1_Mb^T_v\n",
    "        self.v_proj = torch.nn.Linear(cv, dv * h, bias, self.device, self.dtype)\n",
    "\n",
    "        self.out_proj = torch.nn.Linear(dv * h, do, bias, self.device, self.dtype)\n",
    "        if self.add_bias_kv:\n",
    "            self.bias_k = torch.nn.Parameter(\n",
    "                torch.zeros(1, 1, dk * h, device=self.device, dtype=self.dtype)\n",
    "            )\n",
    "            self.bias_v = torch.nn.Parameter(\n",
    "                torch.zeros(1, 1, dv * h, device=self.device, dtype=self.dtype)\n",
    "            )\n",
    "\n",
    "    def forward(self, Q, K, V,attn_mask=None):\n",
    "        \"\"\"Forward pass of the MHA module.\"\"\"\n",
    "        # Linear projections\n",
    "        proj_q = self.q_proj(Q)  # Q=QW_q+1_Mb^T_q\n",
    "        proj_k = self.k_proj(K)  # K=KW_k+1_Mb^T_k\n",
    "        proj_v = self.v_proj(V)  # V=VW_v+1_Mb^T_v\n",
    "        if self.add_bias_kv:\n",
    "            # append bias to the key and value sequences\n",
    "            batch_size = proj_k.shape[0]\n",
    "            proj_k = torch.cat([proj_k, self.bias_k.repeat(batch_size, 1, 1)], dim=1)\n",
    "            proj_v = torch.cat([proj_v, self.bias_v.repeat(batch_size, 1, 1)], dim=1)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        r_q = einops.rearrange(proj_q, \"b m (h dk) -> b h m dk\", h=self.h)\n",
    "        r_k = einops.rearrange(proj_k, \"b n (h dk) -> b h n dk\", h=self.h)\n",
    "        r_v = einops.rearrange(proj_v, \"b n (h dv) -> b h n dv\", h=self.h)\n",
    "\n",
    "        # QK^T\n",
    "        scores = torch.einsum(\"bhmd, bhnd -> bhmn\", r_q, r_k)\n",
    "        if attn_mask is not None:\n",
    "            match attn_mask.dim():\n",
    "                case 2:\n",
    "                    attn_mask = attn_mask.unsqueeze(0).unsqueeze(0) \n",
    "                case 3:\n",
    "                    attn_mask = einops.rearrange(attn_mask, \"(b h) m n -> b h m n\", h = self.h)\n",
    "                case 4:\n",
    "                    pass \n",
    "                case _:\n",
    "                    raise ValueError(\"attn_mask has incorrect dimensions\")\n",
    "            scores += attn_mask\n",
    "        \n",
    "        # softmax(QK^T/sqrt(dk))\n",
    "        attn = torch.nn.functional.softmax(scores / (self.dk**0.5), dim=-1)\n",
    "\n",
    "        # softmax(QK^T/sqrt(dk))V\n",
    "        o = torch.einsum(\"bhmn, bhnv -> bhmv\", attn, r_v)\n",
    "\n",
    "        # Reshape back\n",
    "        r_o = einops.rearrange(o, \"b h m dv -> b m (h dv)\")\n",
    "\n",
    "        # Final linear projection\n",
    "        proj_o = self.out_proj(r_o)\n",
    "        return proj_o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac18d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dim = 3\n",
    "M = 5  # sequence length of q\n",
    "N = 3  # sequence length of k,v\n",
    "d = 4 # embedding/model dimension\n",
    "ck = 8  # key dimension \n",
    "cv = 16  # value dimension\n",
    "h = 2  # number of heads\n",
    "\n",
    "mask = torch.full((M, N), float('-inf'),device=device)\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "#mask = mask.repeat(batch_dim * h, 1, 1)\n",
    "#mask[0]= torch.full((M, N), 0,device=device)\n",
    "\n",
    "  # Lower triangular mask\n",
    "\n",
    "bias = True\n",
    "add_bias_kv = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59ebb393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e18b6f",
   "metadata": {},
   "source": [
    "#### Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fed376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_attn = torch.nn.MHAttn(embed_dim=d,kdim=ck,vdim=cv,\n",
    "                                        num_heads=h,batch_first=True,\n",
    "                                        bias=bias,add_bias_kv=add_bias_kv).to(device)\n",
    "\n",
    "assert d % h == 0, \"d must be divisible by h\"\n",
    "dh = d // h\n",
    "attn = MHAttn(cq = d, ck = ck, cv=cv, dk = dh ,dv=dh,do=d,h=h,bias=bias,add_bias_kv=add_bias_kv).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c3405b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch MHA Weights:\n",
      "q_proj_weight - torch.Size([4, 4])\n",
      "k_proj_weight - torch.Size([4, 8])\n",
      "v_proj_weight - torch.Size([4, 16])\n",
      "in_proj_bias - torch.Size([12])\n",
      "out_proj.weight - torch.Size([4, 4])\n",
      "out_proj.bias - torch.Size([4])\n",
      "\n",
      "Our MHA Weights:\n",
      "q_proj.weight - torch.Size([4, 4])\n",
      "q_proj.bias - torch.Size([4])\n",
      "k_proj.weight - torch.Size([4, 8])\n",
      "k_proj.bias - torch.Size([4])\n",
      "v_proj.weight - torch.Size([4, 16])\n",
      "v_proj.bias - torch.Size([4])\n",
      "out_proj.weight - torch.Size([4, 4])\n",
      "out_proj.bias - torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch MHA Weights:\")\n",
    "for name, w in nn_attn.named_parameters():\n",
    "    print(f\"{name} - {w.shape}\")\n",
    "\n",
    "print(\"\\nOur MHA Weights:\")\n",
    "for name, w in attn.named_parameters():\n",
    "    print(f\"{name} - {w.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cee4a8",
   "metadata": {},
   "source": [
    "#### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "72f74493",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand(batch_dim,M,d,device=device)\n",
    "k = torch.rand(batch_dim,N,ck,device=device) \n",
    "v = torch.rand(batch_dim,N,cv,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c520f7",
   "metadata": {},
   "source": [
    "#### Load weights\n",
    "\n",
    "Weights are created differently, so lets load the nn wieghts on our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53d3e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # 1) copy weights (shapes already match)\n",
    "    attn.q_proj.weight.copy_(nn_attn.q_proj_weight)   # (d, d)\n",
    "    attn.k_proj.weight.copy_(nn_attn.k_proj_weight)   # (d, ck)\n",
    "    attn.v_proj.weight.copy_(nn_attn.v_proj_weight)   # (d, cv)\n",
    "\n",
    "    # 2) split the packed bias: (3d,) -> (d,) + (d,) + (d,)\n",
    "    b = nn_attn.in_proj_bias      # shape (48,)\n",
    "    if bias:\n",
    "        attn.q_proj.bias.copy_(b[0:d])        # 0:d\n",
    "        attn.k_proj.bias.copy_(b[d:2*d])      # d:2d\n",
    "        attn.v_proj.bias.copy_(b[2*d:3*d])    # 2d:3d\n",
    "    \n",
    "    if add_bias_kv:\n",
    "        attn.bias_k.copy_(nn_attn.bias_k.squeeze(0))\n",
    "        attn.bias_v.copy_(nn_attn.bias_v.squeeze(0))\n",
    "\n",
    "    # 3) output projection\n",
    "    attn.out_proj.weight.copy_(nn_attn.out_proj.weight)\n",
    "    if bias:\n",
    "        attn.out_proj.bias.copy_(nn_attn.out_proj.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "336887f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = attn(q, k, v,attn_mask=mask)\n",
    "nn_out,_ = nn_attn(q,k,v, attn_mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "46ab0dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2999,  0.0182,  0.2453,  0.1384],\n",
       "         [ 0.2678,  0.1382,  0.0801,  0.1355],\n",
       "         [ 0.2231,  0.0361,  0.2631,  0.0378],\n",
       "         [ 0.2550,  0.0491,  0.2635,  0.0249],\n",
       "         [ 0.2416,  0.0456,  0.2600,  0.0315]],\n",
       "\n",
       "        [[ 0.1625,  0.0785,  0.1103,  0.0671],\n",
       "         [ 0.2661,  0.2824,  0.1659, -0.1440],\n",
       "         [ 0.2406,  0.3124,  0.1191, -0.1383],\n",
       "         [ 0.2286,  0.3090,  0.1082, -0.1263],\n",
       "         [ 0.2359,  0.3027,  0.1226, -0.1367]],\n",
       "\n",
       "        [[ 0.2562,  0.2110, -0.0463, -0.0349],\n",
       "         [ 0.3384,  0.2915,  0.0669, -0.0994],\n",
       "         [ 0.3229,  0.3656,  0.0345, -0.1476],\n",
       "         [ 0.3223,  0.3671,  0.0368, -0.1438],\n",
       "         [ 0.3368,  0.3819,  0.0356, -0.1506]]], device='mps:0',\n",
       "       grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "efbc4c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2999,  0.0182,  0.2453,  0.1384],\n",
       "         [ 0.2678,  0.1382,  0.0801,  0.1355],\n",
       "         [ 0.2231,  0.0361,  0.2631,  0.0378],\n",
       "         [ 0.2550,  0.0491,  0.2635,  0.0249],\n",
       "         [ 0.2416,  0.0456,  0.2600,  0.0315]],\n",
       "\n",
       "        [[ 0.1625,  0.0785,  0.1103,  0.0671],\n",
       "         [ 0.2661,  0.2824,  0.1659, -0.1440],\n",
       "         [ 0.2406,  0.3124,  0.1191, -0.1383],\n",
       "         [ 0.2286,  0.3090,  0.1082, -0.1263],\n",
       "         [ 0.2359,  0.3027,  0.1226, -0.1367]],\n",
       "\n",
       "        [[ 0.2562,  0.2110, -0.0463, -0.0349],\n",
       "         [ 0.3384,  0.2915,  0.0669, -0.0994],\n",
       "         [ 0.3229,  0.3656,  0.0345, -0.1476],\n",
       "         [ 0.3223,  0.3671,  0.0368, -0.1438],\n",
       "         [ 0.3368,  0.3819,  0.0356, -0.1506]]], device='mps:0',\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05ea950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
