{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "154968d9",
   "metadata": {},
   "source": [
    "# Multi-Head Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b5e26e",
   "metadata": {},
   "source": [
    "## Definition: Attention\n",
    "\n",
    "For a set of key-value pairs $\\{(k_i,v_i)\\}_{i=1}^N \\in \\mathbb{R}^{d_k\\times d_v}$ and another set of queries $\\{q_j\\}_{j=1}^M \\in \\mathbb{R}^{d_k}$, atention returns the \"expected\" value $o_j \\in \\mathbb{R}^{d_v}$ for each querry $q_j, \\ j=1,2,\\dots,M$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ee4d4",
   "metadata": {},
   "source": [
    "Inputs\n",
    "* a set of queries $Q = [q_1|q_2|\\dots|q_M]^T \\in \\mathbb{R}^{M \\times d_k}$\n",
    "* a set of keys $K = [k_1|k_2|\\dots|k_N]^T \\in \\mathbb{R}^{N \\times d_k}$\n",
    "* a set of values $V = [v_1|v_2|\\dots|v_N]^T \\in \\mathbb{R}^{N \\times d_v}$\n",
    "\n",
    "Output: $O= [o_1|o_2|\\dots|o_M]^T \\in\\mathbb{R}^{M \\times d_v}$\n",
    "$$ \\text{Attention}(Q,K,V) = O  = \\alpha V, \\quad \\text{ where } \\quad \\alpha = \\text{softmax}\\left(\\frac{QK^T+B}{\\sqrt{d_k}}\\right) \\in \\mathbb{R}^{M\\times N}$$\n",
    "$$ o_{i}  =  \\sum_{j=1}^N\\alpha_{i,j}v_j\\, \\quad \\text{ where } \\quad  \\alpha_{i,j}=\\frac{e^{\\frac{q_i^{T}k_j}{\\sqrt{d_k}}}}{\\sum_{l=1}^Ne^{\\frac{q_i^{T}k_l}{\\sqrt{d_k}}}}$$\n",
    "\n",
    "\n",
    "where $\\text{softmax}$ is applied per row, so each row of $\\alpha$ sums to one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c099fa",
   "metadata": {},
   "source": [
    "If we denote by $v(q_i)$ the random variable \"value of the querry $q_i$\", for $i=1,2,\\dots,M$, then the induced probability of $v(q_i)$ is \n",
    "$$p(v(q_i) = v_j)=\\alpha_{i,j}, \\quad j=1,2,\\dots,N.$$\n",
    "\n",
    "Notice that $\\sum_{j=1}^N\\alpha_{i,j}=1, \\ i =1,2,\\dots,M$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283e2ee1",
   "metadata": {},
   "source": [
    "**Note:** The value $p(v(q_i) = v_j)=\\alpha_{i,j}$ is ussually interpreted as how much attention (the outpu $o_i$ of) the querry $q_i$ pays to value $v_j$. So the \"attention\" of $o_i$ is partitioned along the values $v_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297bb21b",
   "metadata": {},
   "source": [
    "### Property: Key-Value Permutation Invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90689ed",
   "metadata": {},
   "source": [
    "If $\\pi_r(M)$ denotes an arbitrary permutation over the rows of a matrix $M$, then\n",
    "$$\\text{Attention}(Q,\\pi_r(K),\\pi_r(V))=\\text{Attention}(Q,K,V) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b0ff17",
   "metadata": {},
   "source": [
    "Proof:\n",
    "\n",
    "Notice that $\\pi_r(M)$ can be written as $\\pi_r(M) = R_{\\pi}M$ for some permutation matrix $R_{\\pi}$. A permutation matrix is a square binary matrix that has exactly one entry of 1 in each row and each column with all other entries 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7860acf8",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Attention}(Q,\\pi_r(K),\\pi_r(V)) & = \\text{Attention}(Q,R_{\\pi}K,R_{\\pi}V),\\\\\n",
    "& = \\text{softmax}\\left(\\frac{Q(R_{\\pi}K)^T}{\\sqrt{d_k}}\\right)R_{\\pi}V,\\\\\n",
    "& = \\text{softmax}\\left(\\frac{QK^TR_{\\pi}^{T}}{\\sqrt{d_k}}\\right)R_{\\pi}V,\\\\\n",
    "& = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}R_{\\pi}^{T}\\right)R_{\\pi}V,\\\\\n",
    "& = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)R_{\\pi}^{T}R_{\\pi}V,\\\\\n",
    "& = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V,\\\\\n",
    "& = \\text{Attention}(Q,K,V),\n",
    "\\end{align*}\n",
    "$$\n",
    "where in the fifth equality we used that postmultiply by a permutation matrix is a column permutation and softmax is permutation equivariant. Notice that for any permutation $\\pi$ we have \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{softmax}\\left(\\pi(x)\\right)& = \\text{softmax}\\left(x_{\\pi(1)},x_{\\pi(2)},\\dots,x_{\\pi(n)}\\right),\\\\\n",
    "&=\\frac{\\left(e^{x_{\\pi(1)}},e^{x_{\\pi(2)}},\\dots,e^{x_{\\pi(n)}}\\right)}{\\sum_{i=1}^n e^{x_{\\pi(i)}}},\\\\\n",
    "&=\\frac{\\left(e^{x_{\\pi(1)}},e^{x_{\\pi(2)}},\\dots,e^{x_{\\pi(n)}}\\right)}{\\sum_{i=1}^n e^{x_{i}}},\\\\\n",
    "&=\\frac{\\pi\\left(e^{x_{1}},e^{x_{2}},\\dots,e^{x_{n}}\\right)}{\\sum_{i=1}^n e^{x_{i}}},\\\\\n",
    "&=\\pi\\left(\\frac{\\left(e^{x_{1}},e^{x_{2}},\\dots,e^{x_{n}}\\right)}{\\sum_{i=1}^n e^{x_{i}}}\\right),\\\\\n",
    "&=\\pi\\left(\\text{softmax}(x)\\right),\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "and $\\text{softmax}$ is applied to each row. So for $A = [a_1|a_2|\\dots|a_n]^T = \\frac{QK^T}{\\sqrt{d_k}}$, $f=\\text{softmax}$, and a column permutation $\\pi_c(A) = [\\pi(a_1)|\\pi(a_2)|\\dots|\\pi(a_n)]^T = AC_{\\pi}$, we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(AC_{\\pi}) & = f([\\pi(a_1)|\\pi(a_2)|\\dots|\\pi(a_n)]^T),\\\\\n",
    " & = ([f(\\pi(a_1))|f(\\pi(a_2))|\\dots|f(\\pi(a_n))]^T),\\\\\n",
    "& = ([\\pi(f(a_1))|\\pi(f(a_2))|\\dots|\\pi(f(a_n))]^T),\\\\\n",
    "& = \\pi_c\\left([f(a_1)|f(a_2)|\\dots|f(a_n)]^T\\right),\\\\\n",
    "& = \\pi_c\\left(f(A)\\right),\\\\\n",
    "& =f(A)C_{\\pi}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae671a5f",
   "metadata": {},
   "source": [
    "### Property: Attention Permutation Equivariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49db8091",
   "metadata": {},
   "source": [
    "If $\\pi_r$ and $\\sigma_r$ denote arbitrary permutations over the rows of a matrix, then\n",
    "$$\\text{Attention}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))=\\pi_r\\left(\\text{Attention}(Q,K,V)\\right). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63084601",
   "metadata": {},
   "source": [
    "Proof:\n",
    "Consider  $\\pi_r(M) = R_{\\pi}M$ for some permutation matrix $R_{\\pi}$. From the previous result we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Attention}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V)) & = \\text{Attention}(\\pi_r(Q),K,V),\\\\\n",
    "&= \\text{Attention}(R_{\\pi}Q,K,V),\\\\\n",
    "& = \\text{softmax}\\left(\\frac{(R_{\\pi}Q)K^T}{\\sqrt{d_k}}\\right)V,\\\\\n",
    "& = \\text{softmax}\\left(R_{\\pi}\\frac{QK^T}{\\sqrt{d_k}}\\right)V,\\\\\n",
    "& = R_{\\pi}\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V,\\\\\n",
    "& = \\pi_r\\left(\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\\right),\\\\\n",
    "& = \\pi_r\\left(\\text{Attention}(Q,K,V) \\right),\n",
    "\\end{align*}\n",
    "$$\n",
    "where in the fifth equality we have used that the softmax funtion is applied individually for each row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97457b7d",
   "metadata": {},
   "source": [
    "## Masked Attention\n",
    "\n",
    "Masked attention intentionally breaks permutation equivariance so that order matters. In many tasks, like language modeling or temporal prediction, we donâ€™t want tokens to freely attend to all others. Masking restricts attention to valid positions (e.g., past tokens), enforcing causal or directional structure instead of treating the sequence as an unordered set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673e2a82",
   "metadata": {},
   "source": [
    "Inputs\n",
    "* a set of queries $Q = [q_1|q_2|\\dots|q_M]^T \\in \\mathbb{R}^{M \\times d_k}$\n",
    "* a set of keys $K = [k_1|k_2|\\dots|k_N]^T \\in \\mathbb{R}^{N \\times d_k}$\n",
    "* a set of values $V = [v_1|v_2|\\dots|v_N]^T \\in \\mathbb{R}^{N \\times d_v}$\n",
    "\n",
    "Output: $O= [o_1|o_2|\\dots|o_M]^T \\in\\mathbb{R}^{M \\times d_v}$\n",
    "$$ \\text{MaskedAttention}_{B}(Q,K,V) = O  = \\alpha V, \\quad \\text{ where } \\quad \\alpha = \\text{softmax}\\left(\\frac{QK^T+B}{\\sqrt{d_k}}\\right) \\in \\mathbb{R}^{M\\times N}$$\n",
    "where $B\\in\\mathbb{R}^{M\\times N}$ and $\\text{softmax}$ is applied to each row, so each row of $\\alpha$ sums to one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f548a734",
   "metadata": {},
   "source": [
    "**Note:** In general, the mask breaks the permutation equivariance properties of attention. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabd690b",
   "metadata": {},
   "source": [
    "### Property: The entries of $B$ cancel out the attention.\n",
    "\n",
    "If $b_{i,s} \\to -\\infty$ for $s \\neq i$, then the $i$-th output $o_i$ does not depend on $q_s$, $k_s$, or $v_s$. We have the formula:\n",
    "$$ \n",
    "\\begin{align*}\n",
    "o_i =\\frac{1}{\\sum_{l=1,l\\neq s}^Ne^{\\frac{q_i^{T}k_l+b_{i,l}}{\\sqrt{d_k}}}}\\sum_{j=1,j\\neq s}^Ne^{\\frac{q_i^{T}k_j+b_{i,j}}{\\sqrt{d_k}}} v_j.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da83900",
   "metadata": {},
   "source": [
    "**Proof:** If $b_{i,s}\\to -\\infty$ for some $(i,s)\\in M\\times N$ then\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\alpha_{i,j} &= \\text{softmax}\\left(\\frac{QK^T+B}{\\sqrt{d_k}}\\right)_{i,j} =\\frac{e^{\\frac{q_i^{T}k_j+b_{i,j}}{\\sqrt{d_k}}}}{\\sum_{l=1}^Ne^{\\frac{q_i^{T}k_l+b_{i,l}}{\\sqrt{d_k}}}},\\\\\n",
    "&=\\begin{cases}\n",
    "\\frac{e^{\\frac{q_i^{T}k_j+b_{i,j}}{\\sqrt{d_k}}}}{\\sum_{l=1,l\\neq s}^Ne^{\\frac{q_i^{T}k_l+b_{i,l}}{\\sqrt{d_k}}}},&\\quad j\\neq s\\\\\n",
    "0 ,&\\quad j = s.\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "Then we get\n",
    "$$ \n",
    "\\begin{align*}\n",
    "o_i &= \\sum_{j=1}^N\\alpha_{i,j} v_j=\\sum_{j=1,j\\neq s}^N\\alpha_{i,j} v_j,\\\\\n",
    "&=\\sum_{j=1,j\\neq s}^N\\frac{e^{\\frac{q_i^{T}k_j+b_{i,j}}{\\sqrt{d_k}}}}{\\sum_{l=1,l\\neq s}^Ne^{\\frac{q_i^{T}k_l+b_{i,l}}{\\sqrt{d_k}}}} v_j\n",
    "\\end{align*}\n",
    "$$\n",
    "Since $i\\neq s$ we conclude that $o_i$ does not depend on $q_s,k_s$ or $v_s$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92708176",
   "metadata": {},
   "source": [
    "## Definition: Attention (with weights)\n",
    "Inputs\n",
    "* a set of queries $Q = [q_1|q_2|\\dots|q_M]^T \\in \\mathbb{R}^{M \\times c_q}, \\quad M\\in\\mathbb{N}$\n",
    "* a set of keys $K = [k_1|k_2|\\dots|k_N]^T \\in \\mathbb{R}^{N \\times c_k}, \\quad N\\in\\mathbb{N}$\n",
    "* a set of values $V = [v_1|v_2|\\dots|v_N]^T \\in \\mathbb{R}^{N \\times c_v}, \\quad N\\in\\mathbb{N}$.\n",
    "\n",
    "Weights\n",
    "* a set of querry weights $W_q \\in \\mathbb{R}^{c_q\\times d_k}$ and bias $b_q\\in \\mathbb{R}^{d_k}$\n",
    "* a set of key weights $W_k \\in \\mathbb{R}^{c_k \\times d_k}$ and bias $b_k\\in \\mathbb{R}^{d_k}$\n",
    "* a set of value weights $W_v \\in \\mathbb{R}^{c_v \\times d_v}$ and bias $b_q\\in \\mathbb{R}^{d_v}$\n",
    "\n",
    "Output: \n",
    "* Output $O= [o_1|o_2|\\dots|o_N]^T \\in\\mathbb{R}^{M\\times d_v}$\n",
    "\\begin{align*}\n",
    "O =\\text{Attention}_{\\mathcal{W}}(Q,K,V)=\\text{Attention}(QW_q+B_q,KW_k+B_k,VW_v+B_v)\n",
    "\\end{align*}\n",
    "where\n",
    "\\begin{align*}\n",
    "B_q &= [b_q|b_q|\\dots|b_q]^T \\in \\mathbb{R}^{M \\times d_k},\\\\ \n",
    "B_k &= [b_k|b_k|\\dots|b_k]^T \\in \\mathbb{R}^{N \\times d_k},\\\\\n",
    "B_v &= [b_v|b_v|\\dots|b_v]^T \\in \\mathbb{R}^{N \\times d_v},\\\\\n",
    "\\mathcal{W} &= \\{W_q,b_q,W_k,b_k,W_v,b_v\\}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dbe492",
   "metadata": {},
   "source": [
    "**Note:** The same $\\operatorname{Attention}$ function can be applied to inputs of different sequence lengths. The model parameters are not tied to specific positions. In this sense, $\\operatorname{Attention}$ is **position-agnostic**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea74e686",
   "metadata": {},
   "source": [
    "### Property: Attention (with weights) Permutation Equivariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6f0ad6",
   "metadata": {},
   "source": [
    "If Attention with weights has no bias (linear projections instead of affine projections), the Attention is permutation equivariant. If $\\pi_r$ and $\\sigma_r$ denote arbitrary permutations over the rows of a matrix, then\n",
    "$$\\text{Attention}_{\\mathcal{W}}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))=\\pi_r\\left(\\text{Attention}_{\\mathcal{W}}(Q,K,V)\\right) $$\n",
    "where \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{W} &= \\{W_q,b_q,W_k,b_k,W_v,b_v\\},\\\\\n",
    "b_q &= 0_{\\mathbb{R}^{d_k}},\\\\\n",
    "b_k &= 0_{\\mathbb{R}^{d_k}},\\\\\n",
    "b_v &= 0_{\\mathbb{R}^{d_v}},\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28a8da0",
   "metadata": {},
   "source": [
    "Proof: Consider  $\\pi_r(M) = R_{\\pi}M$ and  $\\sigma_r(M) = R_{\\sigma}M$ for some permutation matrix $R_{\\pi}$ and $R_{\\sigma}$ respectively. From the previous result we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Attention}_{\\mathcal{W}}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))\n",
    "&=\\text{Attention}(\\pi_r(Q)W_q,\\sigma_r(K)W_k,\\sigma_r(V)W_v),\\\\\n",
    "&=\\text{Attention}((R_{\\pi}Q)W_q,(R_{\\sigma}K)W_k,(R_{\\sigma}V)W_v),\\\\\n",
    "&=\\text{Attention}(R_{\\pi}(QW_q),R_{\\sigma}(KW_k),R_{\\sigma}(VW_v)),\\\\\n",
    "&=R_{\\pi}\\text{Attention}(QW_q,KW_k,VW_v),\\\\\n",
    "&=R_{\\pi}\\text{Attention}_{\\mathcal{W}}(Q,K,V),\\\\\n",
    "&=\\pi_r\\left(\\text{Attention}_{\\mathcal{W}}(Q,K,V)\\right).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72b3d53",
   "metadata": {},
   "source": [
    "## Definition: Masked Attention with weights\n",
    "Inputs\n",
    "* a set of queries $Q = [q_1|q_2|\\dots|q_M]^T \\in \\mathbb{R}^{M \\times c_q}$\n",
    "* a set of keys $K = [k_1|k_2|\\dots|k_N]^T \\in \\mathbb{R}^{N \\times c_k}$\n",
    "* a set of values $V = [v_1|v_2|\\dots|v_N]^T \\in \\mathbb{R}^{N \\times c_v}$.\n",
    "\n",
    "Weights\n",
    "* a set of querry weights $W_q \\in \\mathbb{R}^{c_q\\times d_k}$ and bias $b_q\\in \\mathbb{R}^{d_k}$\n",
    "* a set of key weights $W_k \\in \\mathbb{R}^{c_k \\times d_k}$ and bias $b_k\\in \\mathbb{R}^{d_k}$\n",
    "* a set of value weights $W_v \\in \\mathbb{R}^{c_v \\times d_v}$ and bias $b_q\\in \\mathbb{R}^{d_v}$\n",
    "\n",
    "* a mask matrix $B\\in \\mathbb{R}^{M\\times N}$.\n",
    "\n",
    "Output: \n",
    "* Output $O= [o_1|o_2|\\dots|o_N]^T \\in\\mathbb{R}^{M\\times d_v}$\n",
    "\\begin{align*}\n",
    "O =\\text{MaskedAttention}_{\\mathcal{W}}(Q,K,V)=\\text{MaskedAttention}_B(QW_q+B_q,KW_k+B_k,VW_v+B_v)\n",
    "\\end{align*}\n",
    "where\n",
    "\\begin{align*}\n",
    "B_q &= [b_q|b_q|\\dots|b_q]^T \\in \\mathbb{R}^{M \\times d_k},\\\\ \n",
    "B_k &= [b_k|b_k|\\dots|b_k]^T \\in \\mathbb{R}^{N \\times d_k},\\\\\n",
    "B_v &= [b_v|b_v|\\dots|b_v]^T \\in \\mathbb{R}^{N \\times d_v},\\\\\n",
    "\\mathcal{W} &= \\{W_q,b_q,W_k,b_k,W_v,b_v,B\\}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d53e15",
   "metadata": {},
   "source": [
    "**Note:** The mask breaks the permutation-equivariance property of attention. In general, attention is not even position-agnostic, since the entries of $B$ depend on the input sequence lengths $M$ and $N$. Although this limitation can be mitigated by imposing specific structures on $B$, the masked version remains inherently order-dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f80602",
   "metadata": {},
   "source": [
    "### Property: The entries of $B$ cancel out the attention in the Attention (with weights).\n",
    "\n",
    "If $b_{i,s} \\to -\\infty$ for $s \\neq i$, then the $i$-th output $o_i$ does not depend on $q_s$, $k_s$, or $v_s$. We have\n",
    "\\begin{align*}\n",
    "o_i =\\frac{1}{\\sum_{l=1,l\\neq s}^Ne^{\\frac{\\hat{q}_i^{T}\\hat{k}_l+b_{i,l}}{\\sqrt{d_k}}}}\\sum_{j=1,j\\neq s}^Ne^{\\frac{\\hat{q}_i^{T}\\hat{k}_j+b_{i,j}}{\\sqrt{d_k}}} \\hat{v}_j.\n",
    "\\end{align*}\n",
    "where\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\hat{q}_i&=W_q^Tq_i+b_q,\\\\\n",
    "\\hat{k}_i&=W_k^Tk_i+b_k,\\\\\n",
    "\\hat{v}_i&=W_v^Tv_i+b_v,\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c97d494",
   "metadata": {},
   "source": [
    "**Proof:** If $b_{i,s}\\to -\\infty$ for some $(i,s)\\in M\\times N$, we can apply yhe result for the Attention function to obtain \n",
    "$$ \n",
    "\\begin{align*}\n",
    "o_i =\\frac{1}{\\sum_{l=1,l\\neq s}^Ne^{\\frac{\\hat{q}_i^{T}\\hat{k}_l+b_{i,l}}{\\sqrt{d_k}}}}\\sum_{j=1,j\\neq s}^Ne^{\\frac{\\hat{q}_i^{T}\\hat{k}_j+b_{i,j}}{\\sqrt{d_k}}} \\hat{v}_j.\n",
    "\\end{align*}\n",
    "$$\n",
    "where the vectors $\\hat{q}_i$, $\\hat{k}_i$ and $\\hat{v}_i$ are the $i$-th rows of $QW_q+B_q$,$KW_k+B_k$ and $VW_v+B_v$ repectively. So we have\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\hat{q}^T_i&=q_i^TW_q+b_q^T,\\\\\n",
    "\\hat{k}^T_i&=k_i^TW_k+b_k^T,\\\\\n",
    "\\hat{v}^T_i&=v_i^TW_v+b_v^T,\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Taking matrix tranpose in the previous equations we get the result. Since $i\\neq s$ we conclude that $o_i$ does not depend on $q_s,k_s$ or $v_s$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d632046",
   "metadata": {},
   "source": [
    "## Definition: Self-Attention\n",
    "Inputs\n",
    "* $X \\in \\mathbb{R}^{N \\times c}, \\quad N\\in \\mathbb{N}$ \n",
    "\n",
    "Weights\n",
    "* a set of querry weights $W_q \\in \\mathbb{R}^{c\\times d_k}$ and bias $b_q\\in \\mathbb{R}^{d_k}$\n",
    "* a set of key weights $W_k \\in \\mathbb{R}^{c \\times d_k}$ and bias $b_k\\in \\mathbb{R}^{d_k}$\n",
    "* a set of value weights $W_v \\in \\mathbb{R}^{c \\times d_v}$ and bias $b_q\\in \\mathbb{R}^{d_v}$\n",
    "\n",
    "Output: \n",
    "* Output $O= [o_1|o_2|\\dots|o_N]^T \\in\\mathbb{R}^{N\\times d_v}$\n",
    "\\begin{align*}\n",
    "O =\\text{SelfAttention}_{\\mathcal{W}}(X)=\\text{Attention}_{\\mathcal{W}}(X,X,X),\n",
    "\\end{align*}\n",
    "where\n",
    "\\begin{align*}\n",
    "\\mathcal{W} &= \\{W_q,b_q,W_k,b_k,W_v,b_v\\}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d49465",
   "metadata": {},
   "source": [
    "### Property: Self-Attention Permutation Equivariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741cee4b",
   "metadata": {},
   "source": [
    "If Self-Attention has no bias (linear projections instead of affine projections), the Self-Attention is permutation equivariant. If $\\pi_r$ denotes as arbitrary permutation over the rows of a matrix, then\n",
    "$$\\text{SelfAttention}_{\\mathcal{W}}(\\pi_r(X))=\\pi_r\\left(\\text{SelfAttention}_{\\mathcal{W}}(X)\\right) $$\n",
    "where \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{W} &= \\{W_q,b_q,W_k,b_k,W_v,b_v\\},\\\\\n",
    "b_q &= 0_{\\mathbb{R}^{d_k}},\\\\\n",
    "b_k &= 0_{\\mathbb{R}^{d_k}},\\\\\n",
    "b_v &= 0_{\\mathbb{R}^{d_v}},\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f719472",
   "metadata": {},
   "source": [
    "Proof: From the previous result we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{SelfAttention}_{\\mathcal{W}}(\\pi_r(X))\n",
    "&=\\text{Attention}_{\\mathcal{W}}(\\pi_r(X),\\pi_r(X),\\pi_r(X)),\\\\\n",
    "&=\\pi_r\\left(\\text{Attention}_{\\mathcal{W}}(X,X,X)\\right),\\\\\n",
    "&=\\pi_r\\left(\\text{SelfAttention}_{\\mathcal{W}}(X)\\right).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f3ef8d",
   "metadata": {},
   "source": [
    "## Definition: Masked Self-Attention\n",
    "Inputs\n",
    "* $X = [x_1|x_2|\\dots|x_N]^T \\in \\mathbb{R}^{N \\times c}$ \n",
    "\n",
    "Weights\n",
    "* a set of querry weights $W_q \\in \\mathbb{R}^{c\\times d_k}$ and bias $b_q\\in \\mathbb{R}^{d_k}$\n",
    "* a set of key weights $W_k \\in \\mathbb{R}^{c \\times d_k}$ and bias $b_k\\in \\mathbb{R}^{d_k}$\n",
    "* a set of value weights $W_v \\in \\mathbb{R}^{c \\times d_v}$ and bias $b_q\\in \\mathbb{R}^{d_v}$\n",
    "* a mask matrix $B\\in \\mathbb{R}^{N\\times N}.$ \n",
    "\n",
    "Output: \n",
    "* Output $O= [o_1|o_2|\\dots|o_N]^T \\in\\mathbb{R}^{N\\times d_v}$\n",
    "\\begin{align*}\n",
    "O =\\text{MaskedSelfAttention}_{\\mathcal{W}}(X)=\\text{MaskedAttention}_{\\mathcal{W}}(X,X,X),\n",
    "\\end{align*}\n",
    "where\n",
    "\\begin{align*}\n",
    "\\mathcal{W} &= \\{W_q,b_q,W_k,b_k,W_v,b_v,B\\}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5aed18",
   "metadata": {},
   "source": [
    "**Note:** In general, the mask breaks the permutation equivariance properties of Self-Attention and the Self-Attention is not even position agnostic anymore, notice that the weights in $B$ depend on input sequence legth $N$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec2c217",
   "metadata": {},
   "source": [
    "### Property: The entries of $B$ cancel out the attention in the Self-Attention.\n",
    "\n",
    "If $b_{i,s} \\to -\\infty$ for $s \\neq i$, then the $i$-th output $o_i$ does not depend on $x_s$. We have\n",
    "\\begin{align*}\n",
    "o_i =\\frac{1}{\\sum_{l=1,l\\neq s}^Ne^{\\frac{\\hat{q}_i^{T}\\hat{k}_l+b_{i,l}}{\\sqrt{d_k}}}}\\sum_{j=1,j\\neq s}^Ne^{\\frac{\\hat{q}_i^{T}\\hat{k}_j+b_{i,j}}{\\sqrt{d_k}}} \\hat{v}_j.\n",
    "\\end{align*}\n",
    "where\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\hat{q}_i&=W_q^Tx_i+b_q,\\\\\n",
    "\\hat{k}_i&=W_k^Tx_i+b_k,\\\\\n",
    "\\hat{v}_i&=W_v^Tx_i+b_v,\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d74a40",
   "metadata": {},
   "source": [
    "**Proof:** It is straigthforwrd from the same result for Attention with weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdcbb8f",
   "metadata": {},
   "source": [
    "## Definition: Multi-Head Attention\n",
    "\n",
    "Inputs\n",
    "* a set of queries $Q = [q_1|q_2|\\dots|q_M]^T \\in \\mathbb{R}^{M \\times c_q}, \\quad M\\in\\mathbb{N}$\n",
    "* a set of keys $K = [k_1|k_2|\\dots|k_N]^T \\in \\mathbb{R}^{N \\times c_k}, \\quad N\\in\\mathbb{N}$\n",
    "* a set of values $V = [v_1|v_2|\\dots|v_N]^T \\in \\mathbb{R}^{N \\times c_v}, \\quad N\\in\\mathbb{N}$\n",
    "\n",
    "Weights\n",
    "* $W_{q,i} \\in \\mathbb{R}^{c_q \\times d_k}$ and $b_{q,i}\\in \\mathbb{R}^{d_k}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{k,i} \\in \\mathbb{R}^{c_k \\times d_k}$ and $b_{k,i}\\in \\mathbb{R}^{d_k}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{v,i} \\in \\mathbb{R}^{c_v \\times d_v}$ and $b_{v,i}\\in \\mathbb{R}^{d_v}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{o} \\in \\mathbb{R}^{d_vh \\times d_o }$ and $b_{o}\\in \\mathbb{R}^{d_0}$.\n",
    "\n",
    "Output: \n",
    "* Output $O= [o_1|o_2|\\dots|o_M]^T \\in\\mathbb{R}^{M\\times d_o}$\n",
    "\\begin{align*}\n",
    "O  &= \\text{MultiHeadAttention}_{\\{\\mathcal{B}_{i}\\}_{i=0}^h}(Q,K,V),\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\text{Attention}_{\\mathcal{B}_{1}}(Q,K,V)|\n",
    "\\text{Attention}_{\\mathcal{B}_{2}}(Q,K,V)|\n",
    "\\dots|\n",
    "\\text{Attention}_{\\mathcal{B}_{h}}(Q,K,V)\n",
    "\\end{pmatrix}W_o + B_o\n",
    "\\end{align*}\n",
    "where\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "B_o  &= [b_o|b_o|\\dots|b_o]^T \\in \\mathbb{R}^{M \\times d_o},\\\\ \n",
    "\\mathcal{W}_0 &= \\{W_{o},b_{o}\\},\\\\\n",
    "\\mathcal{W}_i &= \\{W_{q,i},b_{q,i},W_{k,i},b_{k,i},W_{v,i},b_{v,i}\\}, \\quad i = 1,2,\\dots,h.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a11aa8",
   "metadata": {},
   "source": [
    "Usually we have:\n",
    " * $d_h:=d_k=d_v$ (head dimensions)\n",
    " * $d := c_q = c_k = c_v = d_h\\cdot h = d_o$ (model dimension)  \n",
    "**Note:** In this case, internally we have:\n",
    "    * $W_q=[W_{q,1}|W_{q,2}|\\dots|W_{q,h}]\\in\\mathbb{R}^{d\\times d}$  \n",
    "    * $W_k=[W_{k,1}|W_{k,2}|\\dots|W_{k,h}]\\in\\mathbb{R}^{d\\times d}$  \n",
    "    * $W_v=[W_{v,1}|W_{v,2}|\\dots|W_{v,h}]\\in\\mathbb{R}^{d\\times d}$  \n",
    "\n",
    "**Note:** The same $\\operatorname{MultiHeadAttention}$ function can be applied to inputs of different sequence lengths. The model parameters are not tied to specific positions. In this sense, $\\operatorname{MultiHeadAttention}$ is **position-agnostic**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6759e72",
   "metadata": {},
   "source": [
    "### Property: Multi-Head Attention Permutation Equivariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fb3c96",
   "metadata": {},
   "source": [
    "If Multi-Head Attention has no bias (linear projections instead of affine projections), the Multi-Head Attention is permutation equivariant. If $\\pi_r$ and $\\sigma_r$ denote arbitrary permutations over the rows of a matrix, then\n",
    "$$\\text{MultiHeadAttention}_{\\{\\mathcal{B}_{i}\\}_{i=0}^h}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))=\\pi_r\\left(\\text{MultiHeadAttention}_{\\{\\mathcal{B}_{i}\\}_{i=0}^h}(Q,K,V)\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e562f03e",
   "metadata": {},
   "source": [
    "Proof:  Consider  $\\pi_r(M) = R_{\\pi}M$ and $\\sigma_r(M) = R_{\\sigma}M$ for permutation matrices $R_{\\pi}$ and $R_{\\sigma}$ respectively. From the Attention equivariance we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{MultiHeadAttention}_{\\{\\mathcal{B}_{i}\\}_{i=0}^h}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\text{Attention}_{\\mathcal{B}_{1}}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))|\n",
    "\\text{Attention}_{\\mathcal{B}_{2}}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))|\n",
    "\\dots|\n",
    "\\text{Attention}_{\\mathcal{B}_{h}}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))\n",
    "\\end{pmatrix}W_o,\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\pi_r\\left(\\text{Attention}_{\\mathcal{B}_{1}}(Q,K,V)\\right)|\n",
    "\\pi_r\\left(\\text{Attention}_{\\mathcal{B}_{2}}(Q,K,V)\\right)|\n",
    "\\dots|\n",
    "\\pi_r\\left(\\text{Attention}_{\\mathcal{B}_{h}}(Q,K,V)\\right)\n",
    "\\end{pmatrix}W_o,\\\\\n",
    "&=\\begin{pmatrix}\n",
    "R_{\\pi}\\text{Attention}_{\\mathcal{B}_{1}}(Q,K,V)|\n",
    "R_{\\pi}\\text{Attention}_{\\mathcal{B}_{2}}(Q,K,V)|\n",
    "\\dots|\n",
    "R_{\\pi}\\text{Attention}_{\\mathcal{B}_{h}}(Q,K,V)\n",
    "\\end{pmatrix}W_o,\\\\\n",
    "&=R_{\\pi}\\begin{pmatrix}\n",
    "\\text{Attention}_{\\mathcal{B}_{1}}(Q,K,V)|\n",
    "\\text{Attention}_{\\mathcal{B}_{2}}(Q,K,V)|\n",
    "\\dots|\n",
    "\\text{Attention}_{\\mathcal{B}_{h}}(Q,K,V)\n",
    "\\end{pmatrix}W_o,\\\\\n",
    "&=R_{\\pi}\\text{MultiHeadAttention}_{\\{\\mathcal{B}_{i}\\}_{i=0}^h}(Q,K,V),\\\\\n",
    "&=\\pi_r\\left(\\text{MultiHeadAttention}_{\\{\\mathcal{B}_{i}\\}_{i=0}^h}(Q,K,V)\\right).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021f71a",
   "metadata": {},
   "source": [
    "## Definition: Multi-Head Attention (torch implementation)\n",
    "\n",
    "Torch implementation corresponds to the particular (usual) case\n",
    " * $d_h:=d_k=d_v$ (head dimensions)\n",
    " * $d := c_q = d_h\\cdot h = d_o$ (model dimension)  \n",
    "\n",
    "Denote $d_h = d|h$ ($h$ must divide $d$). Then we have the simplified defintion\n",
    "\n",
    "Inputs\n",
    "* a set of queries $Q = [q_1|q_2|\\dots|q_M]^T \\in \\mathbb{R}^{M \\times d}$\n",
    "* a set of keys $K = [k_1|k_2|\\dots|k_N]^T \\in \\mathbb{R}^{N \\times c_k}$\n",
    "* a set of values $V = [v_1|v_2|\\dots|v_N]^T \\in \\mathbb{R}^{N \\times c_v}$\n",
    "\n",
    "Weights\n",
    "* $W_{q,i} \\in \\mathbb{R}^{d \\times d_h}$ and $b_{q,i}\\in \\mathbb{R}^{d_h}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{k,i} \\in \\mathbb{R}^{c_k \\times d_h}$ and $b_{k,i}\\in \\mathbb{R}^{d_h}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{v,i} \\in \\mathbb{R}^{c_v \\times d_h}$ and $b_{v,i}\\in \\mathbb{R}^{d_h}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{o} \\in \\mathbb{R}^{d \\times d }$ and $b_{o}\\in \\mathbb{R}^{d}$.\n",
    "\n",
    "\n",
    "Output: \n",
    "* Output $O= [o_1|o_2|\\dots|o_M]^T \\in\\mathbb{R}^{M\\times d}$\n",
    "\\begin{align*}\n",
    "O  &= \\text{MultiHeadAttention}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(Q,K,V),\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\text{Attention}_{\\mathcal{W}_1}(Q,K,V)|\n",
    "\\text{Attention}_{\\mathcal{W}_2}(Q,K,V)|\n",
    "\\dots|\n",
    "\\text{Attention}_{\\mathcal{W}_h}(Q,K,V)\n",
    "\\end{pmatrix}W_o + B_o\n",
    "\\end{align*}\n",
    "where:\n",
    "    * $W_q=[W_{q,1}|W_{q,2}|\\dots|W_{q,h}]\\in\\mathbb{R}^{d\\times d}$  \n",
    "    * $W_k=[W_{k,1}|W_{k,2}|\\dots|W_{k,h}]\\in\\mathbb{R}^{d\\times d}$  \n",
    "    * $W_v=[W_{v,1}|W_{v,2}|\\dots|W_{v,h}]\\in\\mathbb{R}^{d\\times d}$  \n",
    "    * $b_q=[b^T_{q,1}|b^T_{q,2}|\\dots|b^T_{q,h}]^T\\in\\mathbb{R}^{d}$  \n",
    "    * $b_k=[b^T_{k,1}|b^T_{k,2}|\\dots|b^T_{k,h}]^T\\in\\mathbb{R}^{d}$ \n",
    "    * $b_v=[b^T_{v,1}|b^T_{v,2}|\\dots|b^T_{v,h}]^T\\in\\mathbb{R}^{d}$  \n",
    "    * $B_o= [b_o|b_o|\\dots|b_o]^T \\in \\mathbb{R}^{M \\times d}$\n",
    "    * $\\mathcal{W}_0 = \\{W_{o},b_{o}\\}$\n",
    "    * $\\mathcal{W}_i = \\{W_{q,i},b_{q,i},W_{k,i},b_{k,i},W_{v,i},b_{v,i}\\}, \\quad i = 1,2,\\dots,h$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf061ec4",
   "metadata": {},
   "source": [
    "## Definition: Masked Multi-Head Attention\n",
    "\n",
    "Inputs\n",
    "* a set of queries $Q = [q_1|q_2|\\dots|q_M]^T \\in \\mathbb{R}^{M \\times c_q}$\n",
    "* a set of keys $K = [k_1|k_2|\\dots|k_N]^T \\in \\mathbb{R}^{N \\times c_k}$\n",
    "* a set of values $V = [v_1|v_2|\\dots|v_N]^T \\in \\mathbb{R}^{N \\times c_v}$\n",
    "\n",
    "Weights\n",
    "* $W_{q,i} \\in \\mathbb{R}^{c_q \\times d_k}$ and $b_{q,i}\\in \\mathbb{R}^{d_k}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{k,i} \\in \\mathbb{R}^{c_k \\times d_k}$ and $b_{k,i}\\in \\mathbb{R}^{d_k}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{v,i} \\in \\mathbb{R}^{c_v \\times d_v}$ and $b_{v,i}\\in \\mathbb{R}^{d_v}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $B_{i} \\in \\mathbb{R}^{M \\times N}, \\quad i=1,2,\\dots,h.$\n",
    "* $W_{o} \\in \\mathbb{R}^{d_vh \\times d_o }$ and $b_{o}\\in \\mathbb{R}^{d_0}$.\n",
    "\n",
    "Output: \n",
    "* Output $O= [o_1|o_2|\\dots|o_M]^T \\in\\mathbb{R}^{M\\times d_o}$\n",
    "\\begin{align*}\n",
    "O  &= \\text{MaskedMultiHeadAttention}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(Q,K,V),\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\text{MaskAttn}_{\\mathcal{W}_{1}}(Q,K,V)|\n",
    "\\text{MaskAttn}_{\\mathcal{W}_{2}}(Q,K,V)|\n",
    "\\dots|\n",
    "\\text{MaskAttn}_{\\mathcal{W}_{h}}(Q,K,V)\n",
    "\\end{pmatrix}W_o + B_o\n",
    "\\end{align*}\n",
    "where $\\operatorname{MaskAttn}=\\operatorname{MaskedAttention}$ and\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "B_o  &= [b_o|b_o|\\dots|b_o]^T \\in \\mathbb{R}^{M \\times d_o},\\\\ \n",
    "\\mathcal{W}_0 &= \\{W_{o},b_{o}\\},\\\\\n",
    "\\mathcal{W}_i &= \\{W_{q,i},b_{q,i},W_{k,i},b_{k,i},W_{v,i},b_{v,i},B_i\\}, \\quad i = 1,2,\\dots,h.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77786d61",
   "metadata": {},
   "source": [
    "### Property: If the same entry of all masks are $-\\infty$, the corresponding attention in the Multi-Head Attention cancels out.\n",
    "\n",
    "Denote the $(i,s) \\in M\\times N$ entry of the $l$-th mask matrix by $b^{(l)}_{i,s}$. If $b_{l,i,s} \\to -\\infty$ for fixed values $s \\neq i$ and for all values $l=1,2,\\dots,h$, then the $i$-th output $o_i$ of the masked Multi-Head Attention does not depend on $q_s$,$k_s$ or $v_s$. Particulary, we have:\n",
    "$$\n",
    "\\begin{align*}\n",
    "o_i &=W_o^T\\begin{pmatrix}\n",
    "o_{i}^{(1)}\\\\\n",
    "o_{i}^{(2)}\\\\\n",
    "\\vdots\\\\\n",
    "o_{i}^{(h)}\n",
    "\\end{pmatrix}\n",
    "+ B^T_o\\\\\n",
    "o_i^{(\\kappa)} &=\\frac{1}{\\sum_{l=1,l\\neq s}^Ne^{\\frac{(\\hat{q}_i^{(\\kappa)})^{T}\\hat{k}_l^{(\\kappa)}+b_{i,l}}{\\sqrt{d_k}}}}\\sum_{j=1,j\\neq s}^Ne^{\\frac{(\\hat{q}_i^{(\\kappa)})^{T}\\hat{k}_j^{(\\kappa)}+b_{i,j}}{\\sqrt{d_k}}} \\hat{v}_j^{(\\kappa)}, \\quad \\kappa = 1,2,\\dots,h.\n",
    "\\end{align*}\n",
    "$$\n",
    "where\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\hat{q}^{(\\kappa)}_l&=W_{q,l}^Tq_l+b_q, \\quad \\kappa = 1,2,\\dots,h, \\ l = 1,2,\\dots, N\\\\\n",
    "\\hat{k}^{(\\kappa)}_l&=W_{k,l}^Tk_l+b_k, \\quad \\kappa = 1,2,\\dots,h, \\ l = 1,2,\\dots, N\\\\\n",
    "\\hat{v}^{(\\kappa)}_l&=W_{v,l}^Tv_l+b_v, \\quad \\kappa = 1,2,\\dots,h, \\ l = 1,2,\\dots, N\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b7a8bf",
   "metadata": {},
   "source": [
    "**Proof:** It is straigthforwrd from the same result for Masked Attention with weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc23134",
   "metadata": {},
   "source": [
    "## Definition: Multi-Head Self-Attention\n",
    "\n",
    "This is usually denoted as $\\text{MultiHeadAttention}(X)$ with only one argument.\n",
    "\n",
    "Inputs\n",
    "* $X \\in \\mathbb{R}^{N \\times c}, \\quad N \\in \\mathbb{N}.$ \n",
    "\n",
    "Weights\n",
    "* $W_{q,i} \\in \\mathbb{R}^{c \\times d_k}$ and $b_{q,i}\\in \\mathbb{R}^{d_k}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{k,i} \\in \\mathbb{R}^{c \\times d_k}$ and $b_{k,i}\\in \\mathbb{R}^{d_k}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{v,i} \\in \\mathbb{R}^{c \\times d_v}$ and $b_{v,i}\\in \\mathbb{R}^{d_v}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{o} \\in \\mathbb{R}^{d_vh \\times d_o }$ and $b_{o}\\in \\mathbb{R}^{d_0}$.\n",
    "\n",
    "Output: \n",
    "* Output $O= [o_1|o_2|\\dots|o_M]^T \\in\\mathbb{R}^{M\\times d_o}$\n",
    "\\begin{align*}\n",
    "O = \\text{MultiHeadSelfAttention}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(X)= \\text{MultiHeadAttention}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(X,X,X).\n",
    "\\end{align*}\n",
    "where\n",
    "\\begin{align*}\n",
    "\\mathcal{W}_0 &= \\{W_{o},b_{o}\\},\\\\\n",
    "\\mathcal{W}_i &= \\{W_{q,i},b_{q,i},W_{k,i},b_{k,i},W_{v,i},b_{v,i}\\}, \\quad i = 1,2,\\dots,h.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369a7e28",
   "metadata": {},
   "source": [
    "**Note:** The same $\\operatorname{MultiHeadSelfAttention}$ function can be applied to inputs of different sequence lengths. The model parameters are not tied to specific positions. In this sense, $\\operatorname{MultiHeadHeadAttention}$ is **position-agnostic**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b398a204",
   "metadata": {},
   "source": [
    "### Property: Multi-Head Self-Attention Permutation Equivariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0764344d",
   "metadata": {},
   "source": [
    "If Multi-Head-Self-Attention has no bias (linear projections instead of affine projections), the Multi-Head-Self-Attention is permutation equivariant. If $\\pi_r(M)$ denotes an arbitrary permutation over the rows of a matrix $M$, then\n",
    "$$\\text{MultiHeadSelfAttention}_{\\{\\mathcal{B}_{i}\\}_{i=0}^h}(\\pi_r(X))=\\pi_r\\left(\\text{MultiHeadSelfAttention}_{\\{\\mathcal{B}_{i}\\}_{i=0}^h}(X)\\right),$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce029db0",
   "metadata": {},
   "source": [
    "Proof: From the previous result we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{MultiHeadSelfAttention}_{\\{\\mathcal{B}_{i}\\}_{i=0}^h}(\\pi_r(X))\n",
    "&=\\text{MultiHeadAttention}_{\\{\\mathcal{B}_{i}\\}_{i=0}^h}(\\pi_r(X),\\pi_r(X),\\pi_r(X)),\\\\\n",
    "&=\\pi_r\\left(\\text{MultiHeadAttention}_{\\{\\mathcal{B}_{i}\\}_{i=0}^h}(X,X,X)\\right),\\\\\n",
    "&=\\pi_r\\left(\\text{MultiHeadSelfAttention}_{\\{\\mathcal{B}_{i}\\}_{i=0}^h}(X)\\right).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac4e810",
   "metadata": {},
   "source": [
    "## Definition: Masked Multi-Head Self-Attention\n",
    "\n",
    "This is usually denoted as $\\text{MultiHeadAttention}(X)$ with only one argument.\n",
    "\n",
    "Inputs\n",
    "* $X \\in \\mathbb{R}^{N \\times c}, \\quad N \\in \\mathbb{N}.$ \n",
    "\n",
    "Weights\n",
    "* $W_{q,i} \\in \\mathbb{R}^{c \\times d_k}$ and $b_{q,i}\\in \\mathbb{R}^{d_k}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{k,i} \\in \\mathbb{R}^{c \\times d_k}$ and $b_{k,i}\\in \\mathbb{R}^{d_k}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{v,i} \\in \\mathbb{R}^{c \\times d_v}$ and $b_{v,i}\\in \\mathbb{R}^{d_v}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $B_{i} \\in \\mathbb{R}^{M \\times N}, \\quad i=1,2,\\dots,h.$\n",
    "* $W_{o} \\in \\mathbb{R}^{d_vh \\times d_o }$ and $b_{o}\\in \\mathbb{R}^{d_0}$.\n",
    "\n",
    "Output: \n",
    "* Output $O= [o_1|o_2|\\dots|o_M]^T \\in\\mathbb{R}^{M\\times d_o}$\n",
    "\\begin{align*}\n",
    "O = \\text{MaskedMultiHeadSelfAttention}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(X)= \\text{MaskedMultiHeadAttention}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(X,X,X).\n",
    "\\end{align*}\n",
    "where\n",
    "\\begin{align*}\n",
    "\\mathcal{W}_0 &= \\{W_{o},b_{o}\\},\\\\\n",
    "\\mathcal{W}_i &= \\{W_{q,i},b_{q,i},W_{k,i},b_{k,i},W_{v,i},b_{v,i},B_i\\}, \\quad i = 1,2,\\dots,h.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10047e06",
   "metadata": {},
   "source": [
    "### Property: If the same entry of all masks are $-\\infty$, the corresponding attention in the Multi-Head Attention cancels out.\n",
    "\n",
    "Denote the $(i,s) \\in M\\times N$ entry of the $l$-th mask matrix by $b^{(l)}_{i,s}$. If $b_{l,i,s} \\to -\\infty$ for fixed values $s \\neq i$ and for all values $l=1,2,\\dots,h$, then the $i$-th output $o_i$ of the masked Multi-Head Attention does not depend on $q_s$,$k_s$ or $v_s$. Particulary, we have:\n",
    "$$\n",
    "\\begin{align*}\n",
    "o_i &=W_o^T\\begin{pmatrix}\n",
    "o_{i}^{(1)}\\\\\n",
    "o_{i}^{(2)}\\\\\n",
    "\\vdots\\\\\n",
    "o_{i}^{(h)}\n",
    "\\end{pmatrix}\n",
    "+ B^T_o\\\\\n",
    "o_i^{(\\kappa)} &=\\frac{1}{\\sum_{l=1,l\\neq s}^Ne^{\\frac{(\\hat{q}_i^{(\\kappa)})^{T}\\hat{k}_l^{(\\kappa)}+b_{i,l}}{\\sqrt{d_k}}}}\\sum_{j=1,j\\neq s}^Ne^{\\frac{(\\hat{q}_i^{(\\kappa)})^{T}\\hat{k}_j^{(\\kappa)}+b_{i,j}}{\\sqrt{d_k}}} \\hat{v}_j^{(\\kappa)}, \\quad \\kappa = 1,2,\\dots,h.\n",
    "\\end{align*}\n",
    "$$\n",
    "where\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\hat{q}^{(\\kappa)}_l&=W_{q,l}^Tx_l+b_q, \\quad \\kappa = 1,2,\\dots,h, \\ l = 1,2,\\dots, N\\\\\n",
    "\\hat{k}^{(\\kappa)}_l&=W_{k,l}^Tx_l+b_k, \\quad \\kappa = 1,2,\\dots,h, \\ l = 1,2,\\dots, N\\\\\n",
    "\\hat{v}^{(\\kappa)}_l&=W_{v,l}^Tx_l+b_v, \\quad \\kappa = 1,2,\\dots,h, \\ l = 1,2,\\dots, N\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e1de1c",
   "metadata": {},
   "source": [
    "**Proof:** It is straigthforwrd from the same result for Multi-Head Attention with weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c73680d",
   "metadata": {},
   "source": [
    "## Definition: Multi-Head Attention\n",
    "\n",
    "Inputs\n",
    "* a set of queries $Q = [q_1|q_2|\\dots|q_M]^T \\in \\mathbb{R}^{M \\times c_q}, \\quad M\\in\\mathbb{N}$\n",
    "* a set of keys $K = [k_1|k_2|\\dots|k_N]^T \\in \\mathbb{R}^{N \\times c_k}, \\quad N\\in\\mathbb{N}$\n",
    "* a set of values $V = [v_1|v_2|\\dots|v_N]^T \\in \\mathbb{R}^{N \\times c_v}, \\quad N\\in\\mathbb{N}$\n",
    "\n",
    "Weights\n",
    "* $W_{q,i} \\in \\mathbb{R}^{c_q \\times d_k}$ and $b_{q,i}\\in \\mathbb{R}^{d_k}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{k,i} \\in \\mathbb{R}^{c_k \\times d_k}$ and $b_{k,i}\\in \\mathbb{R}^{d_k}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{v,i} \\in \\mathbb{R}^{c_v \\times d_v}$ and $b_{v,i}\\in \\mathbb{R}^{d_v}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{o} \\in \\mathbb{R}^{d_vh \\times d_o }$ and $b_{o}\\in \\mathbb{R}^{d_0}$.\n",
    "\n",
    "Output: \n",
    "* Output $O= [o_1|o_2|\\dots|o_M]^T \\in\\mathbb{R}^{M\\times d_o}$\n",
    "\\begin{align*}\n",
    "O  &= \\text{MultiHeadAttention}_{\\{\\mathcal{B}_{i}\\}_{i=0}^h}(Q,K,V),\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\text{Attention}_{\\mathcal{B}_{1}}(Q,K,V)|\n",
    "\\text{Attention}_{\\mathcal{B}_{2}}(Q,K,V)|\n",
    "\\dots|\n",
    "\\text{Attention}_{\\mathcal{B}_{h}}(Q,K,V)\n",
    "\\end{pmatrix}W_o + B_o\n",
    "\\end{align*}\n",
    "where\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "B_o  &= [b_o|b_o|\\dots|b_o]^T \\in \\mathbb{R}^{M \\times d_o},\\\\ \n",
    "\\mathcal{W}_0 &= \\{W_{o},b_{o}\\},\\\\\n",
    "\\mathcal{W}_i &= \\{W_{q,i},b_{q,i},W_{k,i},b_{k,i},W_{v,i},b_{v,i}\\}, \\quad i = 1,2,\\dots,h.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af82ec56",
   "metadata": {},
   "source": [
    "Usually we have:\n",
    " * $d_h:=d_k=d_v$ (head dimensions)\n",
    " * $d := c_q = c_k = c_v = d_h\\cdot h = d_o$ (model dimension)  \n",
    "**Note:** In this case, internally we have:\n",
    "    * $W_q=[W_{q,1}|W_{q,2}|\\dots|W_{q,h}]\\in\\mathbb{R}^{d\\times d}$  \n",
    "    * $W_k=[W_{k,1}|W_{k,2}|\\dots|W_{k,h}]\\in\\mathbb{R}^{d\\times d}$  \n",
    "    * $W_v=[W_{v,1}|W_{v,2}|\\dots|W_{v,h}]\\in\\mathbb{R}^{d\\times d}$  \n",
    "\n",
    "**Note:** The same $\\operatorname{MultiHeadAttention}$ function can be applied to inputs of different sequence lengths. The model parameters are not tied to specific positions. In this sense, $\\operatorname{MultiHeadAttention}$ is **position-agnostic**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90843db7",
   "metadata": {},
   "source": [
    "### Property: Multi-Head Attention Permutation Equivariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a3e282",
   "metadata": {},
   "source": [
    "If Multi-Head Attention has no bias (linear projections instead of affine projections), the Multi-Head Attention is permutation equivariant. If $\\pi_r$ and $\\sigma_r$ denote arbitrary permutations over the rows of a matrix, then\n",
    "$$\\text{MultiHeadAttention}_{\\{\\mathcal{B}_{i}\\}_{i=0}^h}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))=\\pi_r\\left(\\text{MultiHeadAttention}_{\\{\\mathcal{B}_{i}\\}_{i=0}^h}(Q,K,V)\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34d559d",
   "metadata": {},
   "source": [
    "Proof:  Consider  $\\pi_r(M) = R_{\\pi}M$ and $\\sigma_r(M) = R_{\\sigma}M$ for permutation matrices $R_{\\pi}$ and $R_{\\sigma}$ respectively. From the Attention equivariance we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{MultiHeadAttention}_{\\{\\mathcal{B}_{i}\\}_{i=0}^h}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\text{Attention}_{\\mathcal{B}_{1}}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))|\n",
    "\\text{Attention}_{\\mathcal{B}_{2}}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))|\n",
    "\\dots|\n",
    "\\text{Attention}_{\\mathcal{B}_{h}}(\\pi_r(Q),\\sigma_r(K),\\sigma_r(V))\n",
    "\\end{pmatrix}W_o,\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\pi_r\\left(\\text{Attention}_{\\mathcal{B}_{1}}(Q,K,V)\\right)|\n",
    "\\pi_r\\left(\\text{Attention}_{\\mathcal{B}_{2}}(Q,K,V)\\right)|\n",
    "\\dots|\n",
    "\\pi_r\\left(\\text{Attention}_{\\mathcal{B}_{h}}(Q,K,V)\\right)\n",
    "\\end{pmatrix}W_o,\\\\\n",
    "&=\\begin{pmatrix}\n",
    "R_{\\pi}\\text{Attention}_{\\mathcal{B}_{1}}(Q,K,V)|\n",
    "R_{\\pi}\\text{Attention}_{\\mathcal{B}_{2}}(Q,K,V)|\n",
    "\\dots|\n",
    "R_{\\pi}\\text{Attention}_{\\mathcal{B}_{h}}(Q,K,V)\n",
    "\\end{pmatrix}W_o,\\\\\n",
    "&=R_{\\pi}\\begin{pmatrix}\n",
    "\\text{Attention}_{\\mathcal{B}_{1}}(Q,K,V)|\n",
    "\\text{Attention}_{\\mathcal{B}_{2}}(Q,K,V)|\n",
    "\\dots|\n",
    "\\text{Attention}_{\\mathcal{B}_{h}}(Q,K,V)\n",
    "\\end{pmatrix}W_o,\\\\\n",
    "&=R_{\\pi}\\text{MultiHeadAttention}_{\\{\\mathcal{B}_{i}\\}_{i=0}^h}(Q,K,V),\\\\\n",
    "&=\\pi_r\\left(\\text{MultiHeadAttention}_{\\{\\mathcal{B}_{i}\\}_{i=0}^h}(Q,K,V)\\right).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9497a0",
   "metadata": {},
   "source": [
    "## Definition: Multi-Head Attention (torch implementation)\n",
    "\n",
    "Torch implementation corresponds to the particular (usual) case\n",
    " * $d_h:=d_k=d_v$ (head dimensions)\n",
    " * $d := c_q = d_h\\cdot h = d_o$ (model dimension)  \n",
    "\n",
    "Denote $d_h = d|h$ ($h$ must divide $d$). Then we have the simplified defintion\n",
    "\n",
    "Inputs\n",
    "* a set of queries $Q = [q_1|q_2|\\dots|q_M]^T \\in \\mathbb{R}^{M \\times d}$\n",
    "* a set of keys $K = [k_1|k_2|\\dots|k_N]^T \\in \\mathbb{R}^{N \\times c_k}$\n",
    "* a set of values $V = [v_1|v_2|\\dots|v_N]^T \\in \\mathbb{R}^{N \\times c_v}$\n",
    "\n",
    "Weights\n",
    "* $W_{q,i} \\in \\mathbb{R}^{d \\times d_h}$ and $b_{q,i}\\in \\mathbb{R}^{d_h}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{k,i} \\in \\mathbb{R}^{c_k \\times d_h}$ and $b_{k,i}\\in \\mathbb{R}^{d_h}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{v,i} \\in \\mathbb{R}^{c_v \\times d_h}$ and $b_{v,i}\\in \\mathbb{R}^{d_h}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{o} \\in \\mathbb{R}^{d \\times d }$ and $b_{o}\\in \\mathbb{R}^{d}$.\n",
    "\n",
    "\n",
    "Output: \n",
    "* Output $O= [o_1|o_2|\\dots|o_M]^T \\in\\mathbb{R}^{M\\times d}$\n",
    "\\begin{align*}\n",
    "O  &= \\text{MultiHeadAttention}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(Q,K,V),\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\text{Attention}_{\\mathcal{W}_1}(Q,K,V)|\n",
    "\\text{Attention}_{\\mathcal{W}_2}(Q,K,V)|\n",
    "\\dots|\n",
    "\\text{Attention}_{\\mathcal{W}_h}(Q,K,V)\n",
    "\\end{pmatrix}W_o + B_o\n",
    "\\end{align*}\n",
    "where:\n",
    "    * $W_q=[W_{q,1}|W_{q,2}|\\dots|W_{q,h}]\\in\\mathbb{R}^{d\\times d}$  \n",
    "    * $W_k=[W_{k,1}|W_{k,2}|\\dots|W_{k,h}]\\in\\mathbb{R}^{d\\times d}$  \n",
    "    * $W_v=[W_{v,1}|W_{v,2}|\\dots|W_{v,h}]\\in\\mathbb{R}^{d\\times d}$  \n",
    "    * $b_q=[b^T_{q,1}|b^T_{q,2}|\\dots|b^T_{q,h}]^T\\in\\mathbb{R}^{d}$  \n",
    "    * $b_k=[b^T_{k,1}|b^T_{k,2}|\\dots|b^T_{k,h}]^T\\in\\mathbb{R}^{d}$ \n",
    "    * $b_v=[b^T_{v,1}|b^T_{v,2}|\\dots|b^T_{v,h}]^T\\in\\mathbb{R}^{d}$  \n",
    "    * $B_o= [b_o|b_o|\\dots|b_o]^T \\in \\mathbb{R}^{M \\times d}$\n",
    "    * $\\mathcal{W}_0 = \\{W_{o},b_{o}\\}$\n",
    "    * $\\mathcal{W}_i = \\{W_{q,i},b_{q,i},W_{k,i},b_{k,i},W_{v,i},b_{v,i}\\}, \\quad i = 1,2,\\dots,h$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63664937",
   "metadata": {},
   "source": [
    "## Definition: Masked Multi-Head Attention\n",
    "\n",
    "Inputs\n",
    "* a set of queries $Q = [q_1|q_2|\\dots|q_M]^T \\in \\mathbb{R}^{M \\times c_q}$\n",
    "* a set of keys $K = [k_1|k_2|\\dots|k_N]^T \\in \\mathbb{R}^{N \\times c_k}$\n",
    "* a set of values $V = [v_1|v_2|\\dots|v_N]^T \\in \\mathbb{R}^{N \\times c_v}$\n",
    "\n",
    "Weights\n",
    "* $W_{q,i} \\in \\mathbb{R}^{c_q \\times d_k}$ and $b_{q,i}\\in \\mathbb{R}^{d_k}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{k,i} \\in \\mathbb{R}^{c_k \\times d_k}$ and $b_{k,i}\\in \\mathbb{R}^{d_k}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{v,i} \\in \\mathbb{R}^{c_v \\times d_v}$ and $b_{v,i}\\in \\mathbb{R}^{d_v}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $B_{i} \\in \\mathbb{R}^{M \\times N}, \\quad i=1,2,\\dots,h.$\n",
    "* $W_{o} \\in \\mathbb{R}^{d_vh \\times d_o }$ and $b_{o}\\in \\mathbb{R}^{d_0}$.\n",
    "\n",
    "Output: \n",
    "* Output $O= [o_1|o_2|\\dots|o_M]^T \\in\\mathbb{R}^{M\\times d_o}$\n",
    "\\begin{align*}\n",
    "O  &= \\text{MaskedMultiHeadAttention}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(Q,K,V),\\\\\n",
    "&=\\begin{pmatrix}\n",
    "\\text{MaskAttn}_{\\mathcal{W}_{1}}(Q,K,V)|\n",
    "\\text{MaskAttn}_{\\mathcal{W}_{2}}(Q,K,V)|\n",
    "\\dots|\n",
    "\\text{MaskAttn}_{\\mathcal{W}_{h}}(Q,K,V)\n",
    "\\end{pmatrix}W_o + B_o\n",
    "\\end{align*}\n",
    "where $\\operatorname{MaskAttn}=\\operatorname{MaskedAttention}$ and\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "B_o  &= [b_o|b_o|\\dots|b_o]^T \\in \\mathbb{R}^{M \\times d_o},\\\\ \n",
    "\\mathcal{W}_0 &= \\{W_{o},b_{o}\\},\\\\\n",
    "\\mathcal{W}_i &= \\{W_{q,i},b_{q,i},W_{k,i},b_{k,i},W_{v,i},b_{v,i},B_i\\}, \\quad i = 1,2,\\dots,h.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3ba3ad",
   "metadata": {},
   "source": [
    "### Property: If the same entry of all masks are $-\\infty$, the corresponding attention in the Multi-Head Attention cancels out.\n",
    "\n",
    "Denote the $(i,s) \\in M\\times N$ entry of the $l$-th mask matrix by $b^{(l)}_{i,s}$. If $b_{l,i,s} \\to -\\infty$ for fixed values $s \\neq i$ and for all values $l=1,2,\\dots,h$, then the $i$-th output $o_i$ of the masked Multi-Head Attention does not depend on $q_s$,$k_s$ or $v_s$. Particulary, we have:\n",
    "$$\n",
    "\\begin{align*}\n",
    "o_i &=W_o^T\\begin{pmatrix}\n",
    "o_{i}^{(1)}\\\\\n",
    "o_{i}^{(2)}\\\\\n",
    "\\vdots\\\\\n",
    "o_{i}^{(h)}\n",
    "\\end{pmatrix}\n",
    "+ B^T_o\\\\\n",
    "o_i^{(\\kappa)} &=\\frac{1}{\\sum_{l=1,l\\neq s}^Ne^{\\frac{(\\hat{q}_i^{(\\kappa)})^{T}\\hat{k}_l^{(\\kappa)}+b_{i,l}}{\\sqrt{d_k}}}}\\sum_{j=1,j\\neq s}^Ne^{\\frac{(\\hat{q}_i^{(\\kappa)})^{T}\\hat{k}_j^{(\\kappa)}+b_{i,j}}{\\sqrt{d_k}}} \\hat{v}_j^{(\\kappa)}, \\quad \\kappa = 1,2,\\dots,h.\n",
    "\\end{align*}\n",
    "$$\n",
    "where\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\hat{q}^{(\\kappa)}_l&=W_{q,l}^Tq_l+b_q, \\quad \\kappa = 1,2,\\dots,h, \\ l = 1,2,\\dots, N\\\\\n",
    "\\hat{k}^{(\\kappa)}_l&=W_{k,l}^Tk_l+b_k, \\quad \\kappa = 1,2,\\dots,h, \\ l = 1,2,\\dots, N\\\\\n",
    "\\hat{v}^{(\\kappa)}_l&=W_{v,l}^Tv_l+b_v, \\quad \\kappa = 1,2,\\dots,h, \\ l = 1,2,\\dots, N\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159719b2",
   "metadata": {},
   "source": [
    "**Proof:** It is straigthforwrd from the same result for Masked Attention with weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47f166f",
   "metadata": {},
   "source": [
    "## Definition: Multi-Head Self-Attention\n",
    "\n",
    "This is usually denoted as $\\text{MultiHeadAttention}(X)$ with only one argument.\n",
    "\n",
    "Inputs\n",
    "* $X \\in \\mathbb{R}^{N \\times c}, \\quad N \\in \\mathbb{N}.$ \n",
    "\n",
    "Weights\n",
    "* $W_{q,i} \\in \\mathbb{R}^{c \\times d_k}$ and $b_{q,i}\\in \\mathbb{R}^{d_k}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{k,i} \\in \\mathbb{R}^{c \\times d_k}$ and $b_{k,i}\\in \\mathbb{R}^{d_k}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{v,i} \\in \\mathbb{R}^{c \\times d_v}$ and $b_{v,i}\\in \\mathbb{R}^{d_v}$, $\\quad i=1,2,\\dots,h.$\n",
    "* $W_{o} \\in \\mathbb{R}^{d_vh \\times d_o }$ and $b_{o}\\in \\mathbb{R}^{d_0}$.\n",
    "\n",
    "Output: \n",
    "* Output $O= [o_1|o_2|\\dots|o_M]^T \\in\\mathbb{R}^{M\\times d_o}$\n",
    "\\begin{align*}\n",
    "O = \\text{MultiHeadSelfAttention}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(X)= \\text{MultiHeadAttention}_{\\{\\mathcal{W}_{i}\\}_{i=0}^h}(X,X,X).\n",
    "\\end{align*}\n",
    "where\n",
    "\\begin{align*}\n",
    "\\mathcal{W}_0 &= \\{W_{o},b_{o}\\},\\\\\n",
    "\\mathcal{W}_i &= \\{W_{q,i},b_{q,i},W_{k,i},b_{k,i},W_{v,i},b_{v,i}\\}, \\quad i = 1,2,\\dots,h.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c53342",
   "metadata": {},
   "source": [
    "**Note:** The same $\\operatorname{MultiHeadSelfAttention}$ function can be applied to inputs of different sequence lengths. The model parameters are not tied to specific positions. In this sense, $\\operatorname{MultiHeadHeadAttention}$ is **position-agnostic**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a6d386",
   "metadata": {},
   "source": [
    "## Code: Multi-Head Attention\n",
    "\n",
    "The code for the remaining models are easily deduced from $\\text{MultiHeadAttention}$:\n",
    "* $\\text{Attention}_{\\mathcal{W}}(Q,K,V) = \\text{MultiHeadAttention}_{\\{\\mathcal{W},\\operatorname{Id},0\\}}(Q,K,V)$ (one head)\n",
    "* $\\text{SelfAttention}_{\\mathcal{W}}(X) = \\text{Attention}_{\\mathcal{W}}(X,X,X)$\n",
    "* $\\text{MultiHeadSelfAttention}_{\\{\\mathcal{B}_{i}\\}_{i=1}^h\\cup \\{W_o,b_o\\}}(X) = \\text{MultiHeadAttention}_{\\{\\mathcal{B}_{i}\\}_{i=1}^h\\cup \\{W_o,b_o\\}}(X,X,X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4ab3980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "52e46913",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention module with (general) parameters\n",
    "    cq, ck, cv: input dimensions for Q, K, V\n",
    "    dk, dv: dimensions for each head's Q, K and V\n",
    "    do: output dimension\n",
    "    h: number of heads.\n",
    "    \n",
    "    The initialization of the weights differs from PyTorchâ€™s `nn.MultiheadAttention`.\n",
    "    Here we use standard `nn.Linear` initialization (Xavier uniform for weights and\n",
    "    zeros for biases) for clarity and simplicity. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, cq, ck, cv, dk, dv, do, h, bias=True, add_bias_kv=False, device=None, dtype=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert dk % h == 0, \"dk must be divisible by h\"\n",
    "        self.cq = cq\n",
    "        self.ck = ck\n",
    "        self.cv = cv\n",
    "        self.dk = dk\n",
    "        self.dv = dv\n",
    "        self.do = do\n",
    "        self.h = h\n",
    "        self.add_bias_kv = add_bias_kv\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        # Q -> QW_q+B_q\n",
    "        self.q_proj = torch.nn.Linear(cq, dk * h, bias, self.device, self.dtype)\n",
    "        # K -> KW_k+B_k\n",
    "        self.k_proj = torch.nn.Linear(ck, dk * h, bias, self.device, self.dtype)\n",
    "        # V -> VW_v+B_v\n",
    "        self.v_proj = torch.nn.Linear(cv, dv * h, bias, self.device, self.dtype)\n",
    "\n",
    "        self.out_proj = torch.nn.Linear(dv * h, do, bias, self.device, self.dtype)\n",
    "        if self.add_bias_kv:\n",
    "            self.bias_k = torch.nn.Parameter(\n",
    "                torch.zeros(1, 1, dk * h, device=self.device, dtype=self.dtype)\n",
    "            )\n",
    "            self.bias_v = torch.nn.Parameter(\n",
    "                torch.zeros(1, 1, dv * h, device=self.device, dtype=self.dtype)\n",
    "            )\n",
    "\n",
    "    def forward(self, Q, K, V,attn_mask=None):\n",
    "        \"\"\"Forward pass of the MHA module.\"\"\"\n",
    "        # Linear projections\n",
    "        proj_q = self.q_proj(Q)  # Q=QW_q+B_q\n",
    "        proj_k = self.k_proj(K)  # K=KW_k+B_k\n",
    "        proj_v = self.v_proj(V)  # V=VW_v+B_v\n",
    "        if self.add_bias_kv:\n",
    "            # append bias to the key and value sequences\n",
    "            batch_size = proj_k.shape[0]\n",
    "            proj_k = torch.cat([proj_k, self.bias_k.repeat(batch_size, 1, 1)], dim=1)\n",
    "            proj_v = torch.cat([proj_v, self.bias_v.repeat(batch_size, 1, 1)], dim=1)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        r_q = einops.rearrange(proj_q, \"b m (h dk) -> b h m dk\", h=self.h)\n",
    "        r_k = einops.rearrange(proj_k, \"b n (h dk) -> b h n dk\", h=self.h)\n",
    "        r_v = einops.rearrange(proj_v, \"b n (h dv) -> b h n dv\", h=self.h)\n",
    "\n",
    "        # QK^T\n",
    "        scores = torch.einsum(\"bhmd, bhnd -> bhmn\", r_q, r_k)\n",
    "        if attn_mask is not None:\n",
    "            scores += attn_mask.unsqueeze(0).unsqueeze(0)  # Broadcasting the mask over heads\n",
    "        \n",
    "        # softmax(QK^T/sqrt(dk))\n",
    "        attn = torch.nn.functional.softmax(scores / (self.dk**0.5), dim=-1)\n",
    "\n",
    "        # softmax(QK^T/sqrt(dk))V\n",
    "        o = torch.einsum(\"bhmn, bhnv -> bhmv\", attn, r_v)\n",
    "\n",
    "        # Reshape back\n",
    "        r_o = einops.rearrange(o, \"b h m dv -> b m (h dv)\")\n",
    "\n",
    "        # Final linear projection\n",
    "        proj_o = self.out_proj(r_o)\n",
    "        return proj_o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f5b424f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(12).reshape(3,4).unsqueeze(0).shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fac18d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dim = 3\n",
    "M = 5  # sequence length of q\n",
    "N = 3  # sequence length of k,v\n",
    "d = 4 # embedding/model dimension\n",
    "ck = 8  # key dimension \n",
    "cv = 16  # value dimension\n",
    "h = 2  # number of heads\n",
    "\n",
    "mask = torch.full((M, N), float('-inf'),device=device)\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "  # Lower triangular mask\n",
    "\n",
    "bias = True\n",
    "add_bias_kv = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "59ebb393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf],\n",
       "        [0., 0., -inf],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]], device='mps:0')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e18b6f",
   "metadata": {},
   "source": [
    "#### Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0fed376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_attn = torch.nn.MultiheadAttention(embed_dim=d,kdim=ck,vdim=cv,\n",
    "                                        num_heads=h,batch_first=True,\n",
    "                                        bias=bias,add_bias_kv=add_bias_kv).to(device)\n",
    "\n",
    "assert d % h == 0, \"d must be divisible by h\"\n",
    "dh = d // h\n",
    "attn = MultiHeadAttention(cq = d, ck = ck, cv=cv, dk = dh ,dv=dh,do=d,h=h,bias=bias,add_bias_kv=add_bias_kv).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c3405b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch MHA Weights:\n",
      "q_proj_weight - torch.Size([4, 4])\n",
      "k_proj_weight - torch.Size([4, 8])\n",
      "v_proj_weight - torch.Size([4, 16])\n",
      "in_proj_bias - torch.Size([12])\n",
      "out_proj.weight - torch.Size([4, 4])\n",
      "out_proj.bias - torch.Size([4])\n",
      "\n",
      "Our MHA Weights:\n",
      "q_proj.weight - torch.Size([4, 4])\n",
      "q_proj.bias - torch.Size([4])\n",
      "k_proj.weight - torch.Size([4, 8])\n",
      "k_proj.bias - torch.Size([4])\n",
      "v_proj.weight - torch.Size([4, 16])\n",
      "v_proj.bias - torch.Size([4])\n",
      "out_proj.weight - torch.Size([4, 4])\n",
      "out_proj.bias - torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch MHA Weights:\")\n",
    "for name, w in nn_attn.named_parameters():\n",
    "    print(f\"{name} - {w.shape}\")\n",
    "\n",
    "print(\"\\nOur MHA Weights:\")\n",
    "for name, w in attn.named_parameters():\n",
    "    print(f\"{name} - {w.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cee4a8",
   "metadata": {},
   "source": [
    "#### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "72f74493",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand(batch_dim,M,d,device=device)\n",
    "k = torch.rand(batch_dim,N,ck,device=device) \n",
    "v = torch.rand(batch_dim,N,cv,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c520f7",
   "metadata": {},
   "source": [
    "#### Load weights\n",
    "\n",
    "Weights are created differently, so lets load the nn wieghts on our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "53d3e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # 1) copy weights (shapes already match)\n",
    "    attn.q_proj.weight.copy_(nn_attn.q_proj_weight)   # (d, d)\n",
    "    attn.k_proj.weight.copy_(nn_attn.k_proj_weight)   # (d, ck)\n",
    "    attn.v_proj.weight.copy_(nn_attn.v_proj_weight)   # (d, cv)\n",
    "\n",
    "    # 2) split the packed bias: (3d,) -> (d,) + (d,) + (d,)\n",
    "    b = nn_attn.in_proj_bias      # shape (48,)\n",
    "    if bias:\n",
    "        attn.q_proj.bias.copy_(b[0:d])        # 0:d\n",
    "        attn.k_proj.bias.copy_(b[d:2*d])      # d:2d\n",
    "        attn.v_proj.bias.copy_(b[2*d:3*d])    # 2d:3d\n",
    "    \n",
    "    if add_bias_kv:\n",
    "        attn.bias_k.copy_(nn_attn.bias_k.squeeze(0))\n",
    "        attn.bias_v.copy_(nn_attn.bias_v.squeeze(0))\n",
    "\n",
    "    # 3) output projection\n",
    "    attn.out_proj.weight.copy_(nn_attn.out_proj.weight)\n",
    "    if bias:\n",
    "        attn.out_proj.bias.copy_(nn_attn.out_proj.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "336887f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = attn(q, k, v,attn_mask=mask)\n",
    "nn_out,_ = nn_attn(q,k,v, attn_mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "46ab0dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2615, -0.0980, -0.0476,  0.0864],\n",
       "         [-0.5672, -0.2472, -0.1618,  0.1811],\n",
       "         [-0.4092, -0.2078, -0.0457,  0.1335],\n",
       "         [-0.4180, -0.2088, -0.0589,  0.1355],\n",
       "         [-0.4114, -0.2066, -0.0552,  0.1334]],\n",
       "\n",
       "        [[-0.5213, -0.0770, -0.1666,  0.1922],\n",
       "         [-0.5738, -0.1202, -0.2060,  0.2089],\n",
       "         [-0.6630, -0.1685, -0.2321,  0.2355],\n",
       "         [-0.6603, -0.1693, -0.2241,  0.2352],\n",
       "         [-0.6556, -0.1652, -0.2271,  0.2332]],\n",
       "\n",
       "        [[-0.9014, -0.4767, -0.2997,  0.2832],\n",
       "         [-0.5683, -0.3029, -0.1355,  0.1863],\n",
       "         [-0.6160, -0.2944, -0.1184,  0.2057],\n",
       "         [-0.6061, -0.2948, -0.0987,  0.2034],\n",
       "         [-0.5989, -0.2933, -0.0916,  0.2013]]], device='mps:0',\n",
       "       grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "efbc4c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2615, -0.0980, -0.0476,  0.0864],\n",
       "         [-0.5672, -0.2472, -0.1618,  0.1811],\n",
       "         [-0.4092, -0.2078, -0.0457,  0.1335],\n",
       "         [-0.4180, -0.2088, -0.0589,  0.1355],\n",
       "         [-0.4114, -0.2066, -0.0552,  0.1334]],\n",
       "\n",
       "        [[-0.5213, -0.0770, -0.1666,  0.1922],\n",
       "         [-0.5738, -0.1202, -0.2060,  0.2089],\n",
       "         [-0.6630, -0.1685, -0.2321,  0.2355],\n",
       "         [-0.6603, -0.1693, -0.2241,  0.2352],\n",
       "         [-0.6556, -0.1652, -0.2271,  0.2332]],\n",
       "\n",
       "        [[-0.9014, -0.4767, -0.2997,  0.2832],\n",
       "         [-0.5683, -0.3029, -0.1355,  0.1863],\n",
       "         [-0.6160, -0.2944, -0.1184,  0.2057],\n",
       "         [-0.6061, -0.2948, -0.0987,  0.2034],\n",
       "         [-0.5989, -0.2933, -0.0916,  0.2013]]], device='mps:0',\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05ea950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
